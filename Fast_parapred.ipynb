{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast-parapred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iOJ6UVS9Ei6AmAeb3j9udQN0QnZGko-U",
      "authorship_tag": "ABX9TyPlF5OutaxHAP8oViz3Khjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawarad/ADS/blob/master/Fast_parapred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZYNN14LyRQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5249927b-5369-4849-ad16-b676ee72eed8"
      },
      "source": [
        "!pip install Biopython"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/66/134dbd5f885fc71493c61b6cf04c9ea08082da28da5ed07709b02857cbd0/biopython-1.77-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Biopython) (1.18.5)\n",
            "Installing collected packages: Biopython\n",
            "Successfully installed Biopython-1.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj7tJE24_HQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bio.PDB import *\n",
        "from Bio.PDB.Model import Model\n",
        "from Bio.PDB.Structure import Structure\n",
        "import numpy as np\n",
        "import Bio.PDB\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ju-tG17_KN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residue_in_contact_with(res, ab_search, dist):\n",
        "    return any(len(ab_search.search(a.coord, dist)) > 0\n",
        "               for a in res.get_unpacked_list())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2GLshO6_Mau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aaMap(_aa):\n",
        "    aa_df = pd.DataFrame([['Alanine','Ala','A'],\n",
        "                        ['Arginine','Arg','R'],\n",
        "                        ['Asparagine','Asn','N'],\n",
        "                        ['Aspartate','Asp','D'],\n",
        "                        ['Cysteine','Cys','C'],\n",
        "                        ['Glutamate','Glu','E'],\n",
        "                        ['Glutamine','Gln','Q'],\n",
        "                        ['Glycine','Gly','G'],\n",
        "                        ['Histidine','His','H'],\n",
        "                        ['Histidine_D','HID','H'],\n",
        "                        ['Histidine_E','HIE','H'],\n",
        "                        ['Histidine_P','HIP','H'],\n",
        "                        ['Isoleucine','Ile','I'],\n",
        "                        ['Leucine','Leu','L'],\n",
        "                        ['Lysine','Lys','K'],\n",
        "                        ['Methionine','Met','M'],\n",
        "                        ['Phenylalanine','Phe','F'],\n",
        "                        ['Proline','Pro','P'],\n",
        "                        ['Serine','Ser','S'],\n",
        "                        ['Threonine','Thr','T'],\n",
        "                        ['Tryptophan','Trp','W'],\n",
        "                        ['Tyrosine','Tyr','Y'],\n",
        "                        ['Valine','Val','V']],columns=['Full','3','1'])\n",
        "    aa_df['3'] = aa_df['3'].str.upper()\n",
        "    _aa = _aa.upper()\n",
        "    if len(_aa)==1:\n",
        "        toret = aa_df.loc[aa_df['1']==_aa,'3'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        toret = aa_df.loc[aa_df['3']==_aa,'1'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    return toret\n",
        "\n",
        "def maxi_len(all_variables):\n",
        "    max_length = 0\n",
        "    for i in range(len(all_variables)):\n",
        "        length = len(all_variables[i])\n",
        "        max_length = max(max_length,length)\n",
        "    #     max_length = length\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def ext_epitope(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in epitope:\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in ext_epi:\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "\n",
        "def ext_epitope_sorted(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in sorted(epitope):\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in sorted(ext_epi):\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "aa_s = \"CSTPAGNDEQHRKMILVFYWX\"\n",
        "def one_to_number(res_str):\n",
        "    return [aa_s.index(r) for r in res_str]\n",
        "\n",
        "def aa_features():\n",
        "    # Meiler's features\n",
        "    prop1 = [[1.77, 0.13, 2.43,  1.54,  6.35, 0.17, 0.41],\n",
        "             [1.31, 0.06, 1.60, -0.04,  5.70, 0.20, 0.28],\n",
        "             [3.03, 0.11, 2.60,  0.26,  5.60, 0.21, 0.36],\n",
        "             [2.67, 0.00, 2.72,  0.72,  6.80, 0.13, 0.34],\n",
        "             [1.28, 0.05, 1.00,  0.31,  6.11, 0.42, 0.23],\n",
        "             [0.00, 0.00, 0.00,  0.00,  6.07, 0.13, 0.15],\n",
        "             [1.60, 0.13, 2.95, -0.60,  6.52, 0.21, 0.22],\n",
        "             [1.60, 0.11, 2.78, -0.77,  2.95, 0.25, 0.20],\n",
        "             [1.56, 0.15, 3.78, -0.64,  3.09, 0.42, 0.21],\n",
        "             [1.56, 0.18, 3.95, -0.22,  5.65, 0.36, 0.25],\n",
        "             [2.99, 0.23, 4.66,  0.13,  7.69, 0.27, 0.30],\n",
        "             [2.34, 0.29, 6.13, -1.01, 10.74, 0.36, 0.25],\n",
        "             [1.89, 0.22, 4.77, -0.99,  9.99, 0.32, 0.27],\n",
        "             [2.35, 0.22, 4.43,  1.23,  5.71, 0.38, 0.32],\n",
        "             [4.19, 0.19, 4.00,  1.80,  6.04, 0.30, 0.45],\n",
        "             [2.59, 0.19, 4.00,  1.70,  6.04, 0.39, 0.31],\n",
        "             [3.67, 0.14, 3.00,  1.22,  6.02, 0.27, 0.49],\n",
        "             [2.94, 0.29, 5.89,  1.79,  5.67, 0.30, 0.38],\n",
        "             [2.94, 0.30, 6.47,  0.96,  5.66, 0.25, 0.41],\n",
        "             [3.21, 0.41, 8.08,  2.25,  5.94, 0.32, 0.42],\n",
        "             [0.00, 0.00, 0.00,  0.00,  0.00, 0.00, 0.00]]\n",
        "    return np.array(prop1)\n",
        "\n",
        "NUM_FEATURES = len(aa_s) + 7\n",
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" Converts a class vector to binary class matrix. \"\"\"\n",
        "    new_y = torch.LongTensor(y)\n",
        "    n = new_y.size()[0]\n",
        "    categorical = torch.zeros(n, num_classes)\n",
        "    arangedTensor = torch.arange(0, n)\n",
        "    intaranged = arangedTensor.long()\n",
        "    categorical[intaranged, new_y] = 1\n",
        "    return categorical\n",
        "\n",
        "def seq_to_one_hot(res_seq_one):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[new_ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    return torch.cat((onehot, feats), axis=1)\n",
        "\n",
        "def cdrseq_to_one_hot(res_seq_one,i):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    if i%6 == 0:\n",
        "        ext_onehot = [1, 0, 0, 0, 0, 0]\n",
        "    if i%6 == 1:\n",
        "        ext_onehot = [0, 1, 0, 0, 0, 0]\n",
        "    if i%6 == 2:\n",
        "        ext_onehot = [0, 0, 1, 0, 0, 0]\n",
        "    if i%6 == 3:\n",
        "        ext_onehot = [0, 0, 0, 1, 0, 0]\n",
        "    if i%6 == 4:\n",
        "        ext_onehot = [0, 0, 0, 0, 1, 0]\n",
        "    if i%6 == 5:\n",
        "        ext_onehot = [0, 0, 0, 0, 0, 1]\n",
        "    \n",
        "    chain_encoding = torch.Tensor(ext_onehot)\n",
        "    chain_encoding = chain_encoding.expand(onehot.shape[0], 6)\n",
        "    concatenated = torch.cat((onehot, feats,chain_encoding), 1)\n",
        "\n",
        "    return concatenated\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC-1BLTt_PG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import index_select"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEthaZ8U_WHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 32"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCyodD5Q_axj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AtrousSelf(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AtrousSelf, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(NUM_FEATURES, 64, 3, padding=1)  # a trous convolution\n",
        "        self.conv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)\n",
        "        self.conv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # batch normalisation after first a trous convolution\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.elu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, 1, 1)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.aconv1 = nn.Conv1d(256, 1, 1)\n",
        "        self.aconv2 = nn.Conv1d(256, 1, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        \"\"\"\n",
        "        :param m: Layer type\n",
        "        :return: void. Performs parameter initialisation\n",
        "        \"\"\"\n",
        "        if isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, input, unpacked_masks):\n",
        "        \"\"\"\n",
        "        :param input: antibody amino acid sequences\n",
        "        :param unpacked_masks:\n",
        "        :return: antibody binding probabilities\n",
        "        \"\"\"\n",
        "        x=input\n",
        "\n",
        "        unpacked_masks = torch.transpose(unpacked_masks, 1, 2)\n",
        "\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        #print(\"x after conv1\", x.data.shape)\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "        x = self.bn1(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        #print(\"x after conv2\", x.data.shape)\n",
        "\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "        x = self.bn2(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        #print(\"x after conv3\", x.data.shape)\n",
        "\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "        x = self.bn3(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        old = x\n",
        "\n",
        "        #print(\"x\", x.data.shape)\n",
        "\n",
        "        w_1 = self.aconv1(x)\n",
        "        #print(\"w_1\", w_1.data.shape)\n",
        "\n",
        "        w_2 = self.aconv2(x)\n",
        "        #print(\"w_2\", w_2.data.shape)\n",
        "\n",
        "\n",
        "        w = self.lrelu(w_1 + torch.transpose(w_2, 1, 2))\n",
        "\n",
        "        #print(\"w\", w.data.shape)\n",
        "\n",
        "        bias_mat = 1e9 * (unpacked_masks - 1.0)\n",
        "        w = self.softmax(w + bias_mat)\n",
        "\n",
        "        x = torch.bmm(w, torch.transpose(x, 1, 2))\n",
        "        #print(\"x after bmm\", x.data.shape)\n",
        "\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "\n",
        "        #x = x + old\n",
        "        x = torch.cat((x, old), dim=1)\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "\n",
        "        x = self.bn4(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, unpacked_masks)\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        #print(\"x after fc\", x.data.shape)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtwKcjvK_kYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.set_printoptions(threshold=50000)\n",
        "import torch.optim as optim\n",
        "from torch import squeeze\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch import index_select\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZnFP7v7_oGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off):\n",
        "    \"\"\"\n",
        "    For every CDR get the corresponding seq and check if dist is less than cutoff\n",
        "    and substitute 1 orelse 0 \n",
        "    \n",
        "    E.g. - CDR - TCRASGNIHNYLAWY\n",
        "           Seq.no - ['22','23','24','25','26','27','28','29','30','31','32','33','34','35','36']\n",
        "           \n",
        "           Output - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    \"\"\"\n",
        "    pdb_name = cdr_loop['PDB_Name'].to_numpy()\n",
        "    seq_no = cdr_loop['seq_no'].to_numpy()\n",
        "    chain_type = cdr_loop['Type'].to_numpy()\n",
        "\n",
        "    all_var = []\n",
        "    for i in range(len(pdb_name)):\n",
        "        if chain_type[i] == 'H1' or chain_type[i] == 'H2' or chain_type[i] == 'H3':\n",
        "            dist = pd.read_pickle(dist_matx_path_heavy+pdb_name[i]+\"heavy\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "\n",
        "        elif chain_type[i] == 'L1' or chain_type[i] == 'L2' or chain_type[i] == 'L3':\n",
        "            dist = pd.read_pickle(dist_matx_path_light+pdb_name[i]+\"light\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "    \n",
        "    return all_var"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZxSpWrpAG6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_with_lengths(matrix, lengths):\n",
        "    seqs = []\n",
        "    for i, example in enumerate(matrix):\n",
        "        seq = example[:lengths[i]]\n",
        "        seqs.append(seq)\n",
        "    return np.concatenate(seqs)\n",
        "\n",
        "def permute_training_data(cdrs, masks, lengths, lbls):\n",
        "    \"\"\"\n",
        "    Shuffle training data\n",
        "    :param cdrs:\n",
        "    :param masks:\n",
        "    :param lengths:\n",
        "    :param lbls:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    index = torch.randperm(cdrs.shape[0])\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    lbls = torch.index_select(lbls, 0, index)\n",
        "    masks = torch.index_select(masks, 0, index)\n",
        "    lengths = [lengths[i] for i in index]\n",
        "\n",
        "    return cdrs, masks, lengths, lbls\n",
        "\n",
        "def sort_batch(cdrs, masks, lengths, lbls):\n",
        "    \"\"\"\n",
        "    Sort antibody amino acid sequences by length -- for RNN\n",
        "    :param cdrs:\n",
        "    :param masks:\n",
        "    :param lengths:\n",
        "    :param lbls:\n",
        "    :return: sorted input\n",
        "    \"\"\"\n",
        "    order = np.argsort(lengths)\n",
        "    order = order.tolist()\n",
        "    order.reverse()\n",
        "    lengths.sort(reverse=True)\n",
        "    index = Variable(torch.LongTensor(order))\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    lbls = torch.index_select(lbls, 0, index)\n",
        "    masks = torch.index_select(masks, 0, index)\n",
        "    return cdrs, masks, lengths, lbls"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpS6xzr5Aju9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def atrous_self_run(cdrs_train, lbls_train, masks_train, lengths_train, weights_template, weights_template_number,\n",
        "               cdrs_test, lbls_test, masks_test, lengths_test):\n",
        "    \"\"\"\n",
        "    :param cdrs_train: antibody amino acid training sequences\n",
        "    :param lbls_train:\n",
        "    :param masks_train:\n",
        "    :param lengths_train:\n",
        "    :param weights_template:\n",
        "    :param weights_template_number:\n",
        "    :param cdrs_test:\n",
        "    :param lbls_test:\n",
        "    :param masks_test:\n",
        "    :param lengths_test:\n",
        "    :return: binding probabilities for testing antibody amino acids\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"atrous self run\", file=print_file)\n",
        "    print(\"atrous self run\")\n",
        "    model = AtrousSelf()\n",
        "\n",
        "    ignored_params = list(map(id, [model.conv1.weight, model.conv2.weight, model.conv3.weight,\n",
        "                                   model.aconv1.weight, model.aconv2.weight]))\n",
        "    base_params = filter(lambda p: id(p) not in ignored_params,\n",
        "                         model.parameters())\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': base_params},\n",
        "        {'params': model.conv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.conv2.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.conv3.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.aconv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.aconv2.weight, 'weight_decay': 0.01}\n",
        "    ], lr=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "\n",
        "    total_input = cdrs_train\n",
        "    total_lbls = lbls_train\n",
        "    total_masks = masks_train\n",
        "    total_lengths = lengths_train\n",
        "\n",
        "    if use_cuda:\n",
        "        print(\"using cuda\")\n",
        "        model.cuda()\n",
        "        total_input = total_input.cuda()\n",
        "        total_lbls = total_lbls.cuda()\n",
        "        total_masks = total_masks.cuda()\n",
        "        cdrs_test = cdrs_test.cuda()\n",
        "        lbls_test = lbls_test.cuda()\n",
        "        masks_test = masks_test.cuda()\n",
        "\n",
        "    times = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train(True)\n",
        "        scheduler.step()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        batches_done=0\n",
        "\n",
        "        total_input, total_masks, total_lengths, total_lbls = \\\n",
        "            permute_training_data(total_input, total_masks, total_lengths, total_lbls)\n",
        "\n",
        "\n",
        "        total_time = 0\n",
        "\n",
        "\n",
        "        for j in range(0, cdrs_train.shape[0], batch_size):\n",
        "            batches_done +=1\n",
        "            interval = [x for x in range(j, min(cdrs_train.shape[0], j + batch_size))]\n",
        "            interval = torch.LongTensor(interval)\n",
        "            if use_cuda:\n",
        "                interval = interval.cuda()\n",
        "\n",
        "            input = Variable(index_select(total_input, 0, interval), requires_grad=True)\n",
        "            masks = Variable(index_select(total_masks, 0, interval))\n",
        "            lengths = total_lengths[j:j + batch_size]\n",
        "            lbls = Variable(index_select(total_lbls, 0, interval))\n",
        "\n",
        "            output = model(input, masks)\n",
        "\n",
        "            loss_weights = (lbls * 1.5 + 1) * masks\n",
        "            max_val = (-output).clamp(min=0)\n",
        "            loss = loss_weights * (output - output * lbls + max_val + ((-max_val).exp() + (-output - max_val).exp()).log())\n",
        "            masks_added = masks.sum()\n",
        "            loss = loss.sum() / masks_added\n",
        "\n",
        "            #print(\"Epoch %d - Batch %d has loss %d \" % (epoch, j, loss.data), file=monitoring_file)\n",
        "            epoch_loss +=loss\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_time += time.time() - start\n",
        "\n",
        "        print(\"Epoch %d - loss is %f : \" % (epoch, epoch_loss.data/batches_done))\n",
        "        # print(\"--- %s seconds ---\" % (total_time))\n",
        "        times.append(total_time)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        cdrs_test2, masks_test2, lengths_test2, lbls_test2 = sort_batch(cdrs_test, masks_test, list(lengths_test),\n",
        "                                                                    lbls_test)\n",
        "\n",
        "        probs_test2 = model(cdrs_test2, masks_test2)\n",
        "\n",
        "        # K.mean(K.equal(lbls_test, K.round(y_pred)), axis=-1)\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        probs_test2 = sigmoid(probs_test2)\n",
        "\n",
        "        probs_test2 = probs_test2.data.cpu().numpy().astype('float32')\n",
        "        lbls_test2 = lbls_test2.data.cpu().numpy().astype('int32')\n",
        "\n",
        "        probs_test2 = flatten_with_lengths(probs_test2, lengths_test2)\n",
        "        lbls_test2 = flatten_with_lengths(lbls_test2, lengths_test2)\n",
        "\n",
        "        print(\"Roc\", roc_auc_score(lbls_test2, probs_test2))\n",
        "\n",
        "    torch.save(model.state_dict(), weights_template.format(weights_template_number))\n",
        "\n",
        "    times_mean = np.mean(times)\n",
        "    times_std = 2 * np.std(times)\n",
        "\n",
        "    print(\"Time mean\", times_mean)\n",
        "    print(\"Time std\", times_std)\n",
        "\n",
        "    # print(\"test\", file=track_f)\n",
        "    print(\"test\")\n",
        "    model.eval()\n",
        "\n",
        "    probs_test = model(cdrs_test, masks_test)\n",
        "\n",
        "    # K.mean(K.equal(lbls_test, K.round(y_pred)), axis=-1)\n",
        "\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    probs_test = sigmoid(probs_test)\n",
        "\n",
        "    probs_test1 = probs_test.data.cpu().numpy().astype('float32')\n",
        "    lbls_test1 = lbls_test.data.cpu().numpy().astype('int32')\n",
        "\n",
        "    probs_test1 = flatten_with_lengths(probs_test1, list(lengths_test))\n",
        "    lbls_test1 = flatten_with_lengths(lbls_test1, list(lengths_test))\n",
        "\n",
        "    print(\"Roc\", roc_auc_score(lbls_test1, probs_test1))\n",
        "\n",
        "    return probs_test, lbls_test, probs_test1, lbls_test1  # get them in kfold, append, concatenate do roc on them"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRkfGI4eApnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kfold_cv_eval(cdrs, lbls, masks, lengths, ag, ag_masks, ag_lengths, output_file=\"crossval-data.p\",\n",
        "                  weights_template=\"weights-fold-{}.h5\", seed=0):\n",
        "    \"\"\"\n",
        "    Performs 10-fold cross-vallidation\n",
        "    :param dataset: contains antibody amino acids, ground truth values, antigen atoms\n",
        "    :param output_file: where to print weights\n",
        "    :param weights_template:\n",
        "    :param seed: cv\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # cdrs, lbls, masks, lengths, ag, ag_masks, ag_lengths, dist_mat = \\\n",
        "    #     dataset[\"cdrs\"], dataset[\"lbls\"], dataset[\"masks\"], dataset[\"lengths\"],\\\n",
        "    #     dataset[\"ag\"], dataset[\"ag_masks\"], dataset[\"ag_lengths\"], dataset[\"dist_mat\"]\n",
        "\n",
        "\n",
        "    print(\"cdrs\", cdrs.shape)\n",
        "    print(\"ag\", ag.shape)\n",
        "    #print(\"lbls\", lbls, file=data_file)\n",
        "    #print(\"masks\", masks, file=data_file)\n",
        "    #print(\"lengths\", lengths, file=data_file)\n",
        "\n",
        "    kf = KFold(n_splits=NUM_SPLIT, random_state=seed, shuffle=True)\n",
        "\n",
        "    all_lbls2 = []\n",
        "    all_probs2 = []\n",
        "    all_masks = []\n",
        "\n",
        "    all_probs1 = []\n",
        "    all_lbls1 = []\n",
        "\n",
        "    for i, (train_idx, test_idx) in enumerate(kf.split(cdrs)):\n",
        "        print(\"Fold: \", i + 1)\n",
        "        #print(train_idx, )\n",
        "        #print(test_idx)\n",
        "\n",
        "        lengths_train = [lengths[i] for i in train_idx]\n",
        "        lengths_test = [lengths[i] for i in test_idx]\n",
        "\n",
        "        ag_lengths_train = [ag_lengths[i] for i in train_idx]\n",
        "        ag_lengths_test = [ag_lengths[i] for i in test_idx]\n",
        "\n",
        "        #print(\"train_idx\", train_idx)\n",
        "\n",
        "        print(\"len(train_idx\",len(train_idx))\n",
        "\n",
        "        train_idx = torch.from_numpy(train_idx)\n",
        "        test_idx = torch.from_numpy(test_idx)\n",
        "\n",
        "        cdrs_train = index_select(cdrs, 0, train_idx)\n",
        "        lbls_train = index_select(lbls, 0, train_idx)\n",
        "        mask_train = index_select(masks, 0, train_idx)\n",
        "        ag_train = index_select(ag, 0, train_idx)\n",
        "        ag_masks_train = index_select(ag_masks, 0, train_idx)\n",
        "        # dist_mat_train = index_select(dist_mat, 0, train_idx)\n",
        "\n",
        "        cdrs_test = Variable(index_select(cdrs, 0, test_idx))\n",
        "        lbls_test = Variable(index_select(lbls, 0, test_idx))\n",
        "        mask_test = Variable(index_select(masks, 0, test_idx))\n",
        "        ag_test = Variable(index_select(ag, 0, test_idx))\n",
        "        ag_masks_test = Variable(index_select(ag_masks, 0, test_idx))\n",
        "        # dist_mat_test = Variable(index_select(dist_mat, 0, test_idx))\n",
        "\n",
        "        code = 5\n",
        "        if code ==1:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                simple_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                    cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "        if code == 2:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                attention_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                              cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 3:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                atrous_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                     cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 4:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                antigen_run(cdrs_train, lbls_train, mask_train, lengths_train,\n",
        "                            ag_train, ag_masks_train, ag_lengths_train, weights_template, i,\n",
        "                           cdrs_test, lbls_test, mask_test, lengths_test,\n",
        "                            ag_test, ag_masks_test, ag_lengths_test)\n",
        "\n",
        "        if code == 5:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                atrous_self_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                           cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 6:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                rnn_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                    cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 7:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                xself_run(cdrs_train, lbls_train, mask_train, lengths_train,\n",
        "                            ag_train, ag_masks_train, ag_lengths_train, dist_mat_train, weights_template, i,\n",
        "                            cdrs_test, lbls_test, mask_test, lengths_test,\n",
        "                            ag_test, ag_masks_test, ag_lengths_test, dist_mat_test)\n",
        "\n",
        "        # print(\"test\", file=track_f)\n",
        "        print(\"test\")\n",
        "\n",
        "        lbls_test2 = np.squeeze(lbls_test2)\n",
        "        all_lbls2 = np.concatenate((all_lbls2, lbls_test2))\n",
        "        all_lbls1.append(lbls_test1)\n",
        "\n",
        "        probs_test_pad = torch.zeros(probs_test1.data.shape[0], MAX_CDR_LENGTH, probs_test1.data.shape[2])\n",
        "        probs_test_pad[:probs_test1.data.shape[0], :probs_test1.data.shape[1], :] = probs_test1.data\n",
        "        probs_test_pad = Variable(probs_test_pad)\n",
        "\n",
        "        probs_test2 = np.squeeze(probs_test2)\n",
        "        #print(probs_test)\n",
        "        all_probs2 = np.concatenate((all_probs2, probs_test2))\n",
        "        #print(all_probs)\n",
        "        #print(type(all_probs))\n",
        "\n",
        "        all_probs1.append(probs_test_pad)\n",
        "\n",
        "        all_masks.append(mask_test)\n",
        "\n",
        "    lbl_mat1 = torch.cat(all_lbls1)\n",
        "    prob_mat1 = torch.cat(all_probs1)\n",
        "    #print(\"end\", all_probs)\n",
        "    mask_mat = torch.cat(all_masks)\n",
        "\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump((lbl_mat1, prob_mat1, mask_mat, all_lbls2, all_probs2), f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sF4yu63A3fo",
        "colab_type": "text"
      },
      "source": [
        " **Building the input Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm6j3vSsAvIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_files = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/summary_file_cleaned.pkl\")\n",
        "summary_files = summary_files.drop(summary_files[summary_files.pdb.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "\n",
        "pdb_names = summary_files['pdb'].to_numpy()\n",
        "Hchain_id = summary_files['Hchain'].to_numpy()\n",
        "Lchain_id = summary_files['Lchain'].to_numpy()\n",
        "Achain_id = summary_files['antigen_chain_list'].to_numpy()\n",
        " \n",
        "epi_res = []\n",
        "epi_res_num = []\n",
        "ext_epi_res_num = []\n",
        "ext_epi_res = []\n",
        "A_pdb = []\n",
        "# for i in range(len(pdb_names)):\n",
        "#     epi_residues,epi_res_number,ext_epi_res_number, ext_epi_residues = ext_epitope_sorted(Hchain_id[i],Lchain_id[i],Achain_id[i],pdb_names[i])\n",
        "#     A_pdb.append(pdb_names[i])\n",
        "#     epi_res.append(epi_residues)\n",
        "#     epi_res_num.append(epi_res_number)\n",
        "#     ext_epi_res_num.append(ext_epi_res_number)\n",
        "#     ext_epi_res.append(ext_epi_residues)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_qgNu4hA9UX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fa9ca632-1cc8-47fe-8d26-01f4036a1b86"
      },
      "source": [
        " Sorted_epi = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")\n",
        " Sorted_epi.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pdb</th>\n",
              "      <th>sorted_epi_residues</th>\n",
              "      <th>sorted_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_residues</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4ydk</td>\n",
              "      <td>[Q, L, T, G, G, S, T, E, N, A, K, T, Q, P, S, ...</td>\n",
              "      <td>[105 , 122 , 123 , 124 , 198 , 199 , 257 , 275...</td>\n",
              "      <td>[52 , 54 , 65 , 66 , 69 , 93 , 94 , 95 , 96 , ...</td>\n",
              "      <td>[L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4ydj</td>\n",
              "      <td>[W, K, E, E, N, N, A, K, T, I, R, P, S, G, G, ...</td>\n",
              "      <td>[96 , 97 , 102 , 275 , 279 , 280 , 281 , 282 ,...</td>\n",
              "      <td>[47 , 48 , 49 , 50 , 51 , 52 , 69 , 92 , 93 , ...</td>\n",
              "      <td>[D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1u8i</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1u8l</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5y9j</td>\n",
              "      <td>[K, G, S, Y, T, Y, A, M, G, H, L, Q, K, V, G, ...</td>\n",
              "      <td>[160 , 161 , 162 , 163 , 205 , 206 , 207 , 208...</td>\n",
              "      <td>[156 , 157 , 158 , 159 , 160 , 161 , 162 , 163...</td>\n",
              "      <td>[P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pdb  ...                            sorted_ext_epi_residues\n",
              "0  4ydk  ...  [L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...\n",
              "1  4ydj  ...  [D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...\n",
              "2  1u8i  ...                              [E, L, D, K, W, A, N]\n",
              "3  1u8l  ...                              [D, L, D, R, W, A, S]\n",
              "4  5y9j  ...  [P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufstSsMoBF0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epi_res = Sorted_epi['sorted_epi_residues'].to_numpy()\n",
        "epi_res_num = Sorted_epi['sorted_epi_res_number'].to_numpy()\n",
        "ext_epi_res_num = Sorted_epi['sorted_ext_epi_res_number'].to_numpy()\n",
        "ext_epi_res = Sorted_epi['sorted_ext_epi_residues'].to_numpy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zAXFrGABIdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e3a04cf-6cbd-4fdc-a97b-6bb436e476b5"
      },
      "source": [
        "##Joining all the residue abbrevations for making a sequence and repeating each seq 6 times/pdb\n",
        "\n",
        "Anti_str = []\n",
        "for i in range(len(ext_epi_res)):\n",
        "#     print(i)\n",
        "    str1 = ''.join(ext_epi_res[i])\n",
        "#     print(repeat(str1, 6))\n",
        "#     print(str1,i,len(A_mat[i]))\n",
        "    Anti_str.append(str1)\n",
        "\n",
        "import itertools\n",
        "Anti_seq = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in Anti_str))\n",
        "\n",
        "MAX_EXT_AG_LENGTH = maxi_len(Anti_seq)\n",
        "MAX_EXT_AG_LENGTH"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdMuXdIVBKfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fa084b2-d837-4c2c-e54d-b7ea73eea325"
      },
      "source": [
        "## Generating Labels for Extended Epitope Antigen Matrix\n",
        "\n",
        "labels = []\n",
        "for i in range(len(ext_epi_res_num)):\n",
        "    label = []\n",
        "    for j in ext_epi_res_num[i]:\n",
        "#     print(i)\n",
        "        if j in epi_res_num[i]:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    labels.append(label)\n",
        "\n",
        "import itertools\n",
        "Anti_repeat_labels = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in labels))\n",
        "\n",
        "# Antigen_lbls = np.stack(labels)\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "label_mats = []\n",
        "for i in range(len(Anti_repeat_labels)):\n",
        "    label_mat = torch.tensor(Anti_repeat_labels[i])\n",
        "    label_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, 1))\n",
        "    label_mat_pad[:label_mat.shape[0], 0] = label_mat\n",
        "    label_mats.append(label_mat_pad)\n",
        "\n",
        "ext_epi_lbls = torch.stack(label_mats)\n",
        "ext_epi_lbls.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3R_QoHPBNFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e92a4ef3-46c5-4199-d6d9-653eb488e033"
      },
      "source": [
        "NUM_FEATURES = 28\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "# cdr_mats = []\n",
        "ext_epi_mats = []\n",
        "\n",
        "ext_epi_lengths = []\n",
        "for i in range(len(Anti_seq)):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "#     print(i)\n",
        "    ext_epi_mat = seq_to_one_hot(Anti_seq[i])\n",
        "    ext_epi_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, NUM_FEATURES))\n",
        "    ext_epi_mat_pad[:ext_epi_mat.shape[0], :] = ext_epi_mat\n",
        "    ext_epi_mats.append(ext_epi_mat_pad)\n",
        "    ext_epi_lengths.append(ext_epi_mat.shape[0])\n",
        "\n",
        "ext_epi = torch.stack(ext_epi_mats)\n",
        "ext_epi.shape\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuVtIglhBPpf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f183ba2a-d76a-4f46-e2b0-adabd7168068"
      },
      "source": [
        "ext_epi_masks = []\n",
        "for i in range(len(Anti_seq)):\n",
        "    ext_epi_mask = torch.zeros((MAX_EXT_AG_LENGTH, 1), dtype=int)\n",
        "    ext_epi_mask[:len(Anti_seq[i]), 0] = 1\n",
        "    ext_epi_masks.append(ext_epi_mask)\n",
        "ext_epi_masks = torch.stack(ext_epi_masks)\n",
        "ext_epi_masks.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8wQq73DBR7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aed86b2b-67f6-4007-d23c-1ff50c10f3eb"
      },
      "source": [
        "NUM_FEATURES = 28+6\n",
        "Max_len_CDR = 38\n",
        "cdr_mats = []\n",
        "\n",
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "cdrs_seq = torch.stack(cdr_mats)\n",
        "cdrs_seq.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 34])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfKmYVl5BU8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95a1c6b2-b9a1-48ea-b16d-3441f6661103"
      },
      "source": [
        "all_cdrs_lengths = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "    all_cdrs_lengths.append(cdr_mat.shape[0])\n",
        "max(all_cdrs_lengths)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK535xaGBXjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02bfc904-c53f-49b4-b4bf-a47c03132415"
      },
      "source": [
        "Max_len_CDR = 38\n",
        "\n",
        "cdr_masks = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "    cdr_mask = torch.zeros((Max_len_CDR, 1), dtype=int)\n",
        "    cdr_mask[:len(cdr_loop['CDR'][i]), 0] = 1\n",
        "    cdr_masks.append(cdr_mask)\n",
        "cdr_mask = torch.stack(cdr_masks)\n",
        "cdr_mask.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5XORTXTBaSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\n",
        "                                                          \"5e08\",\"5gkr\",\"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dist_matx_path_light = \"/content/drive/My Drive/Peritia_Fast-Parapred/Light_chain_dist_clean/\"\n",
        "dist_matx_path_heavy = \"/content/drive/My Drive/Peritia_Fast-Parapred/Heavy_chain_dist_clean/\"\n",
        "cut_off = 5\n",
        "# all_variables = get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off)\n",
        "with open('/content/drive/My Drive/Peritia_Fast-Parapred/all_variables.pkl', 'rb') as f:\n",
        "    all_variables = pickle.load(f)\n",
        "# all_variables\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrTFPqfHBcmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "638ecb60-4a82-415a-f19f-29e02d3d90d8"
      },
      "source": [
        "cont_mats = []\n",
        "cdr_lengths = []\n",
        "Max_len_CDR = 38\n",
        "\"\"\"\n",
        "Pad the variables to Max CDR len with 0\n",
        "Here max len of CDR is 38\n",
        "\n",
        "E.g - Input = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    Output = [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[1.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.]\n",
        "\n",
        "\"\"\"\n",
        "for i in range(len(all_variables)):\n",
        "    cont_mat = torch.tensor(all_variables[i])\n",
        "    cont_mat_pad = torch.zeros((Max_len_CDR, 1))\n",
        "    cont_mat_pad[:cont_mat.shape[0], 0] = cont_mat\n",
        "    cont_mats.append(cont_mat_pad)\n",
        "    cdr_lengths.append(cont_mat.shape[0])\n",
        "\n",
        "\n",
        "cdr_lbls = torch.stack(cont_mats)\n",
        "\n",
        "cdr_lbls.shape\n",
        "# cdr_lengths"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqB4kZWeBfjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "AG_NUM_FEATURES = 28\n",
        "\n",
        "NUM_FEATURES = 34\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 32"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBF22HeBBiAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_ITERATIONS = 10\n",
        "NUM_SPLIT = 10\n",
        "output_file=\"crossval-data.p\",\n",
        "# weights_template=\"weights-fold-{}.h5\"\n",
        "import os \n",
        "def run_cv(output_folder=\"cv-only_ab-seq\",\n",
        "           num_iters=NUM_ITERATIONS):\n",
        "#     cache_file = dataset.split(\"/\")[-1] + \".p\"\n",
        "#     dataset = open_dataset(dataset_cache=cache_file)\n",
        "\n",
        "    dir =  output_folder + \"/weights\"\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(output_folder + \"/weights\")\n",
        "    for i in range(num_iters):\n",
        "        #i=0\n",
        "        print(\"Crossvalidation run\", i+1)\n",
        "        output_file = \"{}/run-{}.p\".format(output_folder, i)\n",
        "        weights_template = output_folder + \"/weights/run-\" + \\\n",
        "                           str(i) + \"-fold-{}.pth.tar\"\n",
        "        kfold_cv_eval(cdrs_seq, cdr_lbls, cdr_mask, all_cdrs_lengths, ext_epi, ext_epi_masks, ext_epi_lengths, output_file,\n",
        "                  weights_template, seed=0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCUxNv_gBmpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97f110b2-9905-406d-d70a-d97e27587c59"
      },
      "source": [
        "import time\n",
        "import os\n",
        "start_time = time.time()\n",
        "run_cv(output_folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-only_ab-seq\",\n",
        "           num_iters=NUM_ITERATIONS)\n",
        "end_time = time.time() - start_time\n",
        "hours, rem = divmod(end_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crossvalidation run 1\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.999564 : \n",
            "Roc 0.7857975192924458\n",
            "Epoch 1 - loss is 0.795327 : \n",
            "Roc 0.7597767036750398\n",
            "Epoch 2 - loss is 0.814754 : \n",
            "Roc 0.809584037989318\n",
            "Epoch 3 - loss is 0.801573 : \n",
            "Roc 0.798448184296602\n",
            "Epoch 4 - loss is 0.784410 : \n",
            "Roc 0.7966553921709137\n",
            "Epoch 5 - loss is 0.771625 : \n",
            "Roc 0.8587825614762582\n",
            "Epoch 6 - loss is 0.767178 : \n",
            "Roc 0.8238985072440227\n",
            "Epoch 7 - loss is 0.768881 : \n",
            "Roc 0.8377930692107535\n",
            "Epoch 8 - loss is 0.765176 : \n",
            "Roc 0.8301629785689034\n",
            "Epoch 9 - loss is 0.713829 : \n",
            "Roc 0.8676908861307778\n",
            "Epoch 10 - loss is 0.694774 : \n",
            "Roc 0.8779388533774122\n",
            "Epoch 11 - loss is 0.682939 : \n",
            "Roc 0.865611752865601\n",
            "Epoch 12 - loss is 0.677310 : \n",
            "Roc 0.8802940764308896\n",
            "Epoch 13 - loss is 0.668684 : \n",
            "Roc 0.8881450189978438\n",
            "Epoch 14 - loss is 0.667091 : \n",
            "Roc 0.887791297618024\n",
            "Epoch 15 - loss is 0.661821 : \n",
            "Roc 0.8851972081291025\n",
            "Epoch 16 - loss is 0.667963 : \n",
            "Roc 0.8889017279597939\n",
            "Epoch 17 - loss is 0.653037 : \n",
            "Roc 0.8943533567103952\n",
            "Epoch 18 - loss is 0.655542 : \n",
            "Roc 0.8945056340629907\n",
            "Epoch 19 - loss is 0.645549 : \n",
            "Roc 0.9023275145469658\n",
            "Time mean 0.6559970736503601\n",
            "Time std 0.03812915743430292\n",
            "test\n",
            "Roc 0.9023275145469658\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.970274 : \n",
            "Roc 0.7791640458524813\n",
            "Epoch 1 - loss is 0.803238 : \n",
            "Roc 0.8074058592847423\n",
            "Epoch 2 - loss is 0.785385 : \n",
            "Roc 0.7852020808194913\n",
            "Epoch 3 - loss is 0.778322 : \n",
            "Roc 0.7841203932504942\n",
            "Epoch 4 - loss is 0.769521 : \n",
            "Roc 0.8028149550075736\n",
            "Epoch 5 - loss is 0.764313 : \n",
            "Roc 0.7704949006443994\n",
            "Epoch 6 - loss is 0.764938 : \n",
            "Roc 0.8140847234345716\n",
            "Epoch 7 - loss is 0.752013 : \n",
            "Roc 0.8106866238992579\n",
            "Epoch 8 - loss is 0.762198 : \n",
            "Roc 0.8140127179019795\n",
            "Epoch 9 - loss is 0.711384 : \n",
            "Roc 0.8441986807142308\n",
            "Epoch 10 - loss is 0.688822 : \n",
            "Roc 0.8501019309517086\n",
            "Epoch 11 - loss is 0.674427 : \n",
            "Roc 0.8478670517124078\n",
            "Epoch 12 - loss is 0.669260 : \n",
            "Roc 0.8551401119355087\n",
            "Epoch 13 - loss is 0.661518 : \n",
            "Roc 0.8569749492952684\n",
            "Epoch 14 - loss is 0.660742 : \n",
            "Roc 0.8530764213370645\n",
            "Epoch 15 - loss is 0.655257 : \n",
            "Roc 0.8618937896331288\n",
            "Epoch 16 - loss is 0.643018 : \n",
            "Roc 0.8641018924417858\n",
            "Epoch 17 - loss is 0.635906 : \n",
            "Roc 0.8626074266385972\n",
            "Epoch 18 - loss is 0.641835 : \n",
            "Roc 0.8632047315858387\n",
            "Epoch 19 - loss is 0.634779 : \n",
            "Roc 0.8659050393443044\n",
            "Time mean 0.6885171055793762\n",
            "Time std 0.10529436654141079\n",
            "test\n",
            "Roc 0.8659050393443044\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.941189 : \n",
            "Roc 0.8043777467567772\n",
            "Epoch 1 - loss is 0.781112 : \n",
            "Roc 0.8018702581528098\n",
            "Epoch 2 - loss is 0.798463 : \n",
            "Roc 0.8191938255780198\n",
            "Epoch 3 - loss is 0.787023 : \n",
            "Roc 0.7520085019862446\n",
            "Epoch 4 - loss is 0.784470 : \n",
            "Roc 0.7549014068645188\n",
            "Epoch 5 - loss is 0.785875 : \n",
            "Roc 0.8260611217706169\n",
            "Epoch 6 - loss is 0.759412 : \n",
            "Roc 0.8292787579323273\n",
            "Epoch 7 - loss is 0.756892 : \n",
            "Roc 0.8324423673310057\n",
            "Epoch 8 - loss is 0.773741 : \n",
            "Roc 0.8246258932012647\n",
            "Epoch 9 - loss is 0.717867 : \n",
            "Roc 0.8514122291480475\n",
            "Epoch 10 - loss is 0.697235 : \n",
            "Roc 0.8527054472293591\n",
            "Epoch 11 - loss is 0.691585 : \n",
            "Roc 0.8599151665679348\n",
            "Epoch 12 - loss is 0.677819 : \n",
            "Roc 0.862791140523991\n",
            "Epoch 13 - loss is 0.669712 : \n",
            "Roc 0.8511335347542371\n",
            "Epoch 14 - loss is 0.665028 : \n",
            "Roc 0.8679069423249086\n",
            "Epoch 15 - loss is 0.660400 : \n",
            "Roc 0.8643304276827045\n",
            "Epoch 16 - loss is 0.658382 : \n",
            "Roc 0.8689237488333834\n",
            "Epoch 17 - loss is 0.652082 : \n",
            "Roc 0.8720538768577882\n",
            "Epoch 18 - loss is 0.645224 : \n",
            "Roc 0.8728275629326183\n",
            "Epoch 19 - loss is 0.645780 : \n",
            "Roc 0.8728064468386163\n",
            "Time mean 0.6810060739517212\n",
            "Time std 0.10160054807771793\n",
            "test\n",
            "Roc 0.8728064468386163\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977145 : \n",
            "Roc 0.7842449639498765\n",
            "Epoch 1 - loss is 0.802689 : \n",
            "Roc 0.7795603347225072\n",
            "Epoch 2 - loss is 0.785451 : \n",
            "Roc 0.7957995965342195\n",
            "Epoch 3 - loss is 0.776525 : \n",
            "Roc 0.8121351649673462\n",
            "Epoch 4 - loss is 0.776884 : \n",
            "Roc 0.808351887098765\n",
            "Epoch 5 - loss is 0.765776 : \n",
            "Roc 0.8135072395063127\n",
            "Epoch 6 - loss is 0.760025 : \n",
            "Roc 0.8000380705970774\n",
            "Epoch 7 - loss is 0.749360 : \n",
            "Roc 0.7742848938543796\n",
            "Epoch 8 - loss is 0.753933 : \n",
            "Roc 0.8056839971414622\n",
            "Epoch 9 - loss is 0.717377 : \n",
            "Roc 0.8511513358318022\n",
            "Epoch 10 - loss is 0.688156 : \n",
            "Roc 0.8558296756471608\n",
            "Epoch 11 - loss is 0.679354 : \n",
            "Roc 0.8581787710331696\n",
            "Epoch 12 - loss is 0.668983 : \n",
            "Roc 0.8600041745972221\n",
            "Epoch 13 - loss is 0.668547 : \n",
            "Roc 0.8634888036673561\n",
            "Epoch 14 - loss is 0.660874 : \n",
            "Roc 0.8722094665084879\n",
            "Epoch 15 - loss is 0.655546 : \n",
            "Roc 0.871487854752319\n",
            "Epoch 16 - loss is 0.655280 : \n",
            "Roc 0.872535336668294\n",
            "Epoch 17 - loss is 0.646000 : \n",
            "Roc 0.8786374421275823\n",
            "Epoch 18 - loss is 0.649737 : \n",
            "Roc 0.8677410358189875\n",
            "Epoch 19 - loss is 0.645076 : \n",
            "Roc 0.8791293920732968\n",
            "Time mean 0.6740429639816284\n",
            "Time std 0.10626611227619585\n",
            "test\n",
            "Roc 0.8791293920732968\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.958583 : \n",
            "Roc 0.8346693183174717\n",
            "Epoch 1 - loss is 0.784428 : \n",
            "Roc 0.8244778903081693\n",
            "Epoch 2 - loss is 0.777296 : \n",
            "Roc 0.815775622513711\n",
            "Epoch 3 - loss is 0.775191 : \n",
            "Roc 0.8267394717916285\n",
            "Epoch 4 - loss is 0.768661 : \n",
            "Roc 0.7541604952092689\n",
            "Epoch 5 - loss is 0.779078 : \n",
            "Roc 0.7956871012974762\n",
            "Epoch 6 - loss is 0.772920 : \n",
            "Roc 0.8193351882384868\n",
            "Epoch 7 - loss is 0.771674 : \n",
            "Roc 0.7530069919413027\n",
            "Epoch 8 - loss is 0.788622 : \n",
            "Roc 0.8252406404365806\n",
            "Epoch 9 - loss is 0.722276 : \n",
            "Roc 0.8577628288651689\n",
            "Epoch 10 - loss is 0.701147 : \n",
            "Roc 0.8615531343188518\n",
            "Epoch 11 - loss is 0.685121 : \n",
            "Roc 0.8639976859599077\n",
            "Epoch 12 - loss is 0.693461 : \n",
            "Roc 0.8588352508566774\n",
            "Epoch 13 - loss is 0.678807 : \n",
            "Roc 0.8715122595866887\n",
            "Epoch 14 - loss is 0.662753 : \n",
            "Roc 0.8678993560583637\n",
            "Epoch 15 - loss is 0.663299 : \n",
            "Roc 0.8705957480978626\n",
            "Epoch 16 - loss is 0.657103 : \n",
            "Roc 0.8816663706711634\n",
            "Epoch 17 - loss is 0.651585 : \n",
            "Roc 0.874067785119378\n",
            "Epoch 18 - loss is 0.653284 : \n",
            "Roc 0.8772010391688994\n",
            "Epoch 19 - loss is 0.645900 : \n",
            "Roc 0.8778745998932072\n",
            "Time mean 0.6934796214103699\n",
            "Time std 0.11601945445552538\n",
            "test\n",
            "Roc 0.8778745998932072\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.991811 : \n",
            "Roc 0.8068422171695211\n",
            "Epoch 1 - loss is 0.786573 : \n",
            "Roc 0.8285448029223086\n",
            "Epoch 2 - loss is 0.790416 : \n",
            "Roc 0.8342599314628975\n",
            "Epoch 3 - loss is 0.790716 : \n",
            "Roc 0.8150468439028221\n",
            "Epoch 4 - loss is 0.771830 : \n",
            "Roc 0.817064167814379\n",
            "Epoch 5 - loss is 0.781258 : \n",
            "Roc 0.8178246270014731\n",
            "Epoch 6 - loss is 0.765140 : \n",
            "Roc 0.765270767527435\n",
            "Epoch 7 - loss is 0.778598 : \n",
            "Roc 0.8226406131178252\n",
            "Epoch 8 - loss is 0.762792 : \n",
            "Roc 0.8423376553721558\n",
            "Epoch 9 - loss is 0.717233 : \n",
            "Roc 0.8599532997607523\n",
            "Epoch 10 - loss is 0.694457 : \n",
            "Roc 0.8600919829978948\n",
            "Epoch 11 - loss is 0.680209 : \n",
            "Roc 0.8616049519876334\n",
            "Epoch 12 - loss is 0.671891 : \n",
            "Roc 0.8691092666836508\n",
            "Epoch 13 - loss is 0.669326 : \n",
            "Roc 0.8490337187986661\n",
            "Epoch 14 - loss is 0.659665 : \n",
            "Roc 0.8697262538477895\n",
            "Epoch 15 - loss is 0.665850 : \n",
            "Roc 0.8692176847944556\n",
            "Epoch 16 - loss is 0.659218 : \n",
            "Roc 0.8794999741405566\n",
            "Epoch 17 - loss is 0.659712 : \n",
            "Roc 0.8804203787738025\n",
            "Epoch 18 - loss is 0.656165 : \n",
            "Roc 0.8835241824105982\n",
            "Epoch 19 - loss is 0.651642 : \n",
            "Roc 0.8823454706706024\n",
            "Time mean 0.6767410039901733\n",
            "Time std 0.07088645072693445\n",
            "test\n",
            "Roc 0.8823454706706024\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.939380 : \n",
            "Roc 0.830853667927067\n",
            "Epoch 1 - loss is 0.787825 : \n",
            "Roc 0.8069443600124824\n",
            "Epoch 2 - loss is 0.804335 : \n",
            "Roc 0.8087249287731968\n",
            "Epoch 3 - loss is 0.785409 : \n",
            "Roc 0.8013649608708514\n",
            "Epoch 4 - loss is 0.791460 : \n",
            "Roc 0.7703866713678383\n",
            "Epoch 5 - loss is 0.767544 : \n",
            "Roc 0.8080099251459997\n",
            "Epoch 6 - loss is 0.782296 : \n",
            "Roc 0.8236507434740848\n",
            "Epoch 7 - loss is 0.765779 : \n",
            "Roc 0.8109823017098485\n",
            "Epoch 8 - loss is 0.761341 : \n",
            "Roc 0.8107352875617536\n",
            "Epoch 9 - loss is 0.716231 : \n",
            "Roc 0.857843256452966\n",
            "Epoch 10 - loss is 0.691071 : \n",
            "Roc 0.8603776337706233\n",
            "Epoch 11 - loss is 0.680064 : \n",
            "Roc 0.860030111814936\n",
            "Epoch 12 - loss is 0.683692 : \n",
            "Roc 0.8657981657325114\n",
            "Epoch 13 - loss is 0.673819 : \n",
            "Roc 0.863978488086988\n",
            "Epoch 14 - loss is 0.672295 : \n",
            "Roc 0.8664653470964186\n",
            "Epoch 15 - loss is 0.662448 : \n",
            "Roc 0.8707903236918787\n",
            "Epoch 16 - loss is 0.656610 : \n",
            "Roc 0.8731650232829579\n",
            "Epoch 17 - loss is 0.650140 : \n",
            "Roc 0.869221408973564\n",
            "Epoch 18 - loss is 0.648463 : \n",
            "Roc 0.8792556748410314\n",
            "Epoch 19 - loss is 0.641869 : \n",
            "Roc 0.8744256262487488\n",
            "Time mean 0.6913975596427917\n",
            "Time std 0.09937775173282047\n",
            "test\n",
            "Roc 0.8744256262487488\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.959972 : \n",
            "Roc 0.8147514789903498\n",
            "Epoch 1 - loss is 0.814279 : \n",
            "Roc 0.808830611499172\n",
            "Epoch 2 - loss is 0.790469 : \n",
            "Roc 0.810668727748422\n",
            "Epoch 3 - loss is 0.791194 : \n",
            "Roc 0.7677060842992118\n",
            "Epoch 4 - loss is 0.776568 : \n",
            "Roc 0.7959678207494703\n",
            "Epoch 5 - loss is 0.772078 : \n",
            "Roc 0.8326596942602262\n",
            "Epoch 6 - loss is 0.763138 : \n",
            "Roc 0.7837196651455166\n",
            "Epoch 7 - loss is 0.762388 : \n",
            "Roc 0.8146839711710213\n",
            "Epoch 8 - loss is 0.767229 : \n",
            "Roc 0.8260447129406603\n",
            "Epoch 9 - loss is 0.716114 : \n",
            "Roc 0.8572811108019184\n",
            "Epoch 10 - loss is 0.696682 : \n",
            "Roc 0.8595717649330632\n",
            "Epoch 11 - loss is 0.693736 : \n",
            "Roc 0.856907560993511\n",
            "Epoch 12 - loss is 0.684497 : \n",
            "Roc 0.8639256267924407\n",
            "Epoch 13 - loss is 0.677922 : \n",
            "Roc 0.8672608859302322\n",
            "Epoch 14 - loss is 0.674927 : \n",
            "Roc 0.8781750752496318\n",
            "Epoch 15 - loss is 0.660270 : \n",
            "Roc 0.877570841017908\n",
            "Epoch 16 - loss is 0.659458 : \n",
            "Roc 0.8752622305917963\n",
            "Epoch 17 - loss is 0.653340 : \n",
            "Roc 0.8820373467386069\n",
            "Epoch 18 - loss is 0.647408 : \n",
            "Roc 0.8829200507250614\n",
            "Epoch 19 - loss is 0.647695 : \n",
            "Roc 0.8745272980819537\n",
            "Time mean 0.687349510192871\n",
            "Time std 0.10531407677361995\n",
            "test\n",
            "Roc 0.8745272980819537\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.038463 : \n",
            "Roc 0.732847621128871\n",
            "Epoch 1 - loss is 0.790336 : \n",
            "Roc 0.8273814076548451\n",
            "Epoch 2 - loss is 0.800535 : \n",
            "Roc 0.8120559128371627\n",
            "Epoch 3 - loss is 0.796328 : \n",
            "Roc 0.7998478084415584\n",
            "Epoch 4 - loss is 0.789387 : \n",
            "Roc 0.7746745442057943\n",
            "Epoch 5 - loss is 0.787444 : \n",
            "Roc 0.8343154891983016\n",
            "Epoch 6 - loss is 0.760890 : \n",
            "Roc 0.8283632383241758\n",
            "Epoch 7 - loss is 0.763571 : \n",
            "Roc 0.8286311344905096\n",
            "Epoch 8 - loss is 0.770161 : \n",
            "Roc 0.8282838255494505\n",
            "Epoch 9 - loss is 0.722328 : \n",
            "Roc 0.8598741102647351\n",
            "Epoch 10 - loss is 0.688916 : \n",
            "Roc 0.8651112559315685\n",
            "Epoch 11 - loss is 0.683614 : \n",
            "Roc 0.8698075362137863\n",
            "Epoch 12 - loss is 0.680220 : \n",
            "Roc 0.867310423951049\n",
            "Epoch 13 - loss is 0.677473 : \n",
            "Roc 0.8641381079857642\n",
            "Epoch 14 - loss is 0.662669 : \n",
            "Roc 0.8780321241258741\n",
            "Epoch 15 - loss is 0.658189 : \n",
            "Roc 0.8774619911338661\n",
            "Epoch 16 - loss is 0.656242 : \n",
            "Roc 0.8754088684752747\n",
            "Epoch 17 - loss is 0.651059 : \n",
            "Roc 0.8800377357017982\n",
            "Epoch 18 - loss is 0.648248 : \n",
            "Roc 0.8844319742757243\n",
            "Epoch 19 - loss is 0.651721 : \n",
            "Roc 0.8821701735764236\n",
            "Time mean 0.6873385429382324\n",
            "Time std 0.10555430879823434\n",
            "test\n",
            "Roc 0.8821701735764236\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.953145 : \n",
            "Roc 0.787327190187288\n",
            "Epoch 1 - loss is 0.801345 : \n",
            "Roc 0.7771633521408369\n",
            "Epoch 2 - loss is 0.786718 : \n",
            "Roc 0.8173671493662201\n",
            "Epoch 3 - loss is 0.770559 : \n",
            "Roc 0.8019841978664288\n",
            "Epoch 4 - loss is 0.775371 : \n",
            "Roc 0.7853704520254603\n",
            "Epoch 5 - loss is 0.761243 : \n",
            "Roc 0.8267334276831296\n",
            "Epoch 6 - loss is 0.750957 : \n",
            "Roc 0.7850138297814757\n",
            "Epoch 7 - loss is 0.754967 : \n",
            "Roc 0.8330935033128603\n",
            "Epoch 8 - loss is 0.757325 : \n",
            "Roc 0.8327853883560027\n",
            "Epoch 9 - loss is 0.710537 : \n",
            "Roc 0.8605111543859957\n",
            "Epoch 10 - loss is 0.687348 : \n",
            "Roc 0.8674335398119916\n",
            "Epoch 11 - loss is 0.679324 : \n",
            "Roc 0.8690455223193894\n",
            "Epoch 12 - loss is 0.677363 : \n",
            "Roc 0.8694710540571036\n",
            "Epoch 13 - loss is 0.673580 : \n",
            "Roc 0.874336668053184\n",
            "Epoch 14 - loss is 0.663511 : \n",
            "Roc 0.8742464195168341\n",
            "Epoch 15 - loss is 0.659627 : \n",
            "Roc 0.8776252747531853\n",
            "Epoch 16 - loss is 0.663594 : \n",
            "Roc 0.874196246743454\n",
            "Epoch 17 - loss is 0.650479 : \n",
            "Roc 0.8784689476333649\n",
            "Epoch 18 - loss is 0.655390 : \n",
            "Roc 0.87661036906759\n",
            "Epoch 19 - loss is 0.645470 : \n",
            "Roc 0.8803997666653759\n",
            "Time mean 0.6800780177116394\n",
            "Time std 0.08134346088759728\n",
            "test\n",
            "Roc 0.8803997666653759\n",
            "test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Crossvalidation run 2\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.019820 : \n",
            "Roc 0.7258557788160507\n",
            "Epoch 1 - loss is 0.820002 : \n",
            "Roc 0.8312450435214646\n",
            "Epoch 2 - loss is 0.798336 : \n",
            "Roc 0.8316296184825294\n",
            "Epoch 3 - loss is 0.791160 : \n",
            "Roc 0.8516105967520535\n",
            "Epoch 4 - loss is 0.773690 : \n",
            "Roc 0.8495869999331174\n",
            "Epoch 5 - loss is 0.760252 : \n",
            "Roc 0.8531715889076799\n",
            "Epoch 6 - loss is 0.758082 : \n",
            "Roc 0.825046957160101\n",
            "Epoch 7 - loss is 0.761257 : \n",
            "Roc 0.8324913291483933\n",
            "Epoch 8 - loss is 0.767166 : \n",
            "Roc 0.8123909176611473\n",
            "Epoch 9 - loss is 0.717214 : \n",
            "Roc 0.8718808024638277\n",
            "Epoch 10 - loss is 0.693978 : \n",
            "Roc 0.8775898097986197\n",
            "Epoch 11 - loss is 0.684500 : \n",
            "Roc 0.8821452913055803\n",
            "Epoch 12 - loss is 0.677089 : \n",
            "Roc 0.8854565772669221\n",
            "Epoch 13 - loss is 0.667901 : \n",
            "Roc 0.8902458492975733\n",
            "Epoch 14 - loss is 0.658486 : \n",
            "Roc 0.8956406477420753\n",
            "Epoch 15 - loss is 0.660782 : \n",
            "Roc 0.8900690881353448\n",
            "Epoch 16 - loss is 0.651421 : \n",
            "Roc 0.8927589232538069\n",
            "Epoch 17 - loss is 0.647767 : \n",
            "Roc 0.893689108964498\n",
            "Epoch 18 - loss is 0.647740 : \n",
            "Roc 0.8958150202399493\n",
            "Epoch 19 - loss is 0.637194 : \n",
            "Roc 0.9028934289436052\n",
            "Time mean 0.6624743938446045\n",
            "Time std 0.045381033060181104\n",
            "test\n",
            "Roc 0.9028934289436052\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.020226 : \n",
            "Roc 0.8080510007766168\n",
            "Epoch 1 - loss is 0.799850 : \n",
            "Roc 0.7663122617198532\n",
            "Epoch 2 - loss is 0.805294 : \n",
            "Roc 0.8063789280121179\n",
            "Epoch 3 - loss is 0.785998 : \n",
            "Roc 0.8119838266347462\n",
            "Epoch 4 - loss is 0.783219 : \n",
            "Roc 0.8060465793561141\n",
            "Epoch 5 - loss is 0.781800 : \n",
            "Roc 0.7846470966727427\n",
            "Epoch 6 - loss is 0.764350 : \n",
            "Roc 0.7725841642384019\n",
            "Epoch 7 - loss is 0.756613 : \n",
            "Roc 0.8134264444430181\n",
            "Epoch 8 - loss is 0.758864 : \n",
            "Roc 0.7803087131909323\n",
            "Epoch 9 - loss is 0.712665 : \n",
            "Roc 0.8452909986264794\n",
            "Epoch 10 - loss is 0.686745 : \n",
            "Roc 0.8523667957818797\n",
            "Epoch 11 - loss is 0.677003 : \n",
            "Roc 0.8515703223280532\n",
            "Epoch 12 - loss is 0.664260 : \n",
            "Roc 0.8569801641806372\n",
            "Epoch 13 - loss is 0.657228 : \n",
            "Roc 0.8611681182768094\n",
            "Epoch 14 - loss is 0.650216 : \n",
            "Roc 0.8524624688711457\n",
            "Epoch 15 - loss is 0.650184 : \n",
            "Roc 0.8569464679982542\n",
            "Epoch 16 - loss is 0.653233 : \n",
            "Roc 0.8665394502066699\n",
            "Epoch 17 - loss is 0.639714 : \n",
            "Roc 0.8703782075556469\n",
            "Epoch 18 - loss is 0.634521 : \n",
            "Roc 0.868054976123848\n",
            "Epoch 19 - loss is 0.628285 : \n",
            "Roc 0.8601638516982875\n",
            "Time mean 0.6638919353485108\n",
            "Time std 0.05179132418221432\n",
            "test\n",
            "Roc 0.8601638516982875\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.002159 : \n",
            "Roc 0.8086357785770187\n",
            "Epoch 1 - loss is 0.790343 : \n",
            "Roc 0.8297024114579349\n",
            "Epoch 2 - loss is 0.777537 : \n",
            "Roc 0.7986130530942306\n",
            "Epoch 3 - loss is 0.791245 : \n",
            "Roc 0.8190581879651961\n",
            "Epoch 4 - loss is 0.777731 : \n",
            "Roc 0.7675811457250182\n",
            "Epoch 5 - loss is 0.767900 : \n",
            "Roc 0.8303557738439699\n",
            "Epoch 6 - loss is 0.758798 : \n",
            "Roc 0.8227455145421405\n",
            "Epoch 7 - loss is 0.767387 : \n",
            "Roc 0.7982057597855975\n",
            "Epoch 8 - loss is 0.765599 : \n",
            "Roc 0.7921401142247521\n",
            "Epoch 9 - loss is 0.723810 : \n",
            "Roc 0.8459614233693143\n",
            "Epoch 10 - loss is 0.692974 : \n",
            "Roc 0.8528372801405609\n",
            "Epoch 11 - loss is 0.685936 : \n",
            "Roc 0.8534962544614882\n",
            "Epoch 12 - loss is 0.676351 : \n",
            "Roc 0.8570722935159893\n",
            "Epoch 13 - loss is 0.663908 : \n",
            "Roc 0.863366601644316\n",
            "Epoch 14 - loss is 0.669488 : \n",
            "Roc 0.865490100737085\n",
            "Epoch 15 - loss is 0.652317 : \n",
            "Roc 0.8712341538933321\n",
            "Epoch 16 - loss is 0.655573 : \n",
            "Roc 0.8714580605837858\n",
            "Epoch 17 - loss is 0.650315 : \n",
            "Roc 0.8711507358102701\n",
            "Epoch 18 - loss is 0.644333 : \n",
            "Roc 0.8779017032507751\n",
            "Epoch 19 - loss is 0.643021 : \n",
            "Roc 0.8742444338166446\n",
            "Time mean 0.6800215601921081\n",
            "Time std 0.07516780530986303\n",
            "test\n",
            "Roc 0.8742444338166446\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.017070 : \n",
            "Roc 0.800153245454524\n",
            "Epoch 1 - loss is 0.785122 : \n",
            "Roc 0.7677946137475541\n",
            "Epoch 2 - loss is 0.799464 : \n",
            "Roc 0.7520048483504838\n",
            "Epoch 3 - loss is 0.780412 : \n",
            "Roc 0.8137362134123284\n",
            "Epoch 4 - loss is 0.785706 : \n",
            "Roc 0.7986076224528864\n",
            "Epoch 5 - loss is 0.766609 : \n",
            "Roc 0.7996660125678176\n",
            "Epoch 6 - loss is 0.765585 : \n",
            "Roc 0.7716317447064771\n",
            "Epoch 7 - loss is 0.766195 : \n",
            "Roc 0.8113037833171775\n",
            "Epoch 8 - loss is 0.756253 : \n",
            "Roc 0.8020301435794145\n",
            "Epoch 9 - loss is 0.714182 : \n",
            "Roc 0.8488841993460584\n",
            "Epoch 10 - loss is 0.688072 : \n",
            "Roc 0.8555234598998882\n",
            "Epoch 11 - loss is 0.681006 : \n",
            "Roc 0.8583721704524996\n",
            "Epoch 12 - loss is 0.670076 : \n",
            "Roc 0.863635229040731\n",
            "Epoch 13 - loss is 0.675922 : \n",
            "Roc 0.8521336633702287\n",
            "Epoch 14 - loss is 0.665357 : \n",
            "Roc 0.8693008099976494\n",
            "Epoch 15 - loss is 0.648102 : \n",
            "Roc 0.8674430749249792\n",
            "Epoch 16 - loss is 0.654923 : \n",
            "Roc 0.8642714423744418\n",
            "Epoch 17 - loss is 0.639165 : \n",
            "Roc 0.8693758898535274\n",
            "Epoch 18 - loss is 0.644475 : \n",
            "Roc 0.8717894517126461\n",
            "Epoch 19 - loss is 0.649188 : \n",
            "Roc 0.8726786173356635\n",
            "Time mean 0.7025069475173951\n",
            "Time std 0.12865298644431902\n",
            "test\n",
            "Roc 0.8726786173356635\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977378 : \n",
            "Roc 0.7515897303041368\n",
            "Epoch 1 - loss is 0.809551 : \n",
            "Roc 0.7998170689177175\n",
            "Epoch 2 - loss is 0.796053 : \n",
            "Roc 0.718793330859866\n",
            "Epoch 3 - loss is 0.781980 : \n",
            "Roc 0.8005109488052821\n",
            "Epoch 4 - loss is 0.780948 : \n",
            "Roc 0.6896422674545031\n",
            "Epoch 5 - loss is 0.760551 : \n",
            "Roc 0.8249094771508079\n",
            "Epoch 6 - loss is 0.754878 : \n",
            "Roc 0.8225120112630686\n",
            "Epoch 7 - loss is 0.752280 : \n",
            "Roc 0.7671524509697614\n",
            "Epoch 8 - loss is 0.762035 : \n",
            "Roc 0.8371816656282403\n",
            "Epoch 9 - loss is 0.711379 : \n",
            "Roc 0.8591095595606449\n",
            "Epoch 10 - loss is 0.689477 : \n",
            "Roc 0.8657396634560436\n",
            "Epoch 11 - loss is 0.682050 : \n",
            "Roc 0.8665614173085274\n",
            "Epoch 12 - loss is 0.674845 : \n",
            "Roc 0.8670586506786112\n",
            "Epoch 13 - loss is 0.669832 : \n",
            "Roc 0.868616388069447\n",
            "Epoch 14 - loss is 0.661239 : \n",
            "Roc 0.8660313606747213\n",
            "Epoch 15 - loss is 0.662094 : \n",
            "Roc 0.8783949853868095\n",
            "Epoch 16 - loss is 0.654647 : \n",
            "Roc 0.8797698503083335\n",
            "Epoch 17 - loss is 0.649521 : \n",
            "Roc 0.8805300605413375\n",
            "Epoch 18 - loss is 0.642774 : \n",
            "Roc 0.8839917426046551\n",
            "Epoch 19 - loss is 0.641914 : \n",
            "Roc 0.8805044662106968\n",
            "Time mean 0.6768706321716309\n",
            "Time std 0.10216100465243425\n",
            "test\n",
            "Roc 0.8805044662106968\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.988912 : \n",
            "Roc 0.8223021417365669\n",
            "Epoch 1 - loss is 0.803115 : \n",
            "Roc 0.7441572024296385\n",
            "Epoch 2 - loss is 0.796478 : \n",
            "Roc 0.7696423543203468\n",
            "Epoch 3 - loss is 0.788855 : \n",
            "Roc 0.8070209346560597\n",
            "Epoch 4 - loss is 0.785085 : \n",
            "Roc 0.8287443995149918\n",
            "Epoch 5 - loss is 0.767813 : \n",
            "Roc 0.8275301549842641\n",
            "Epoch 6 - loss is 0.767188 : \n",
            "Roc 0.8227147435222093\n",
            "Epoch 7 - loss is 0.763825 : \n",
            "Roc 0.8094272037513432\n",
            "Epoch 8 - loss is 0.758154 : \n",
            "Roc 0.8286003528377388\n",
            "Epoch 9 - loss is 0.717069 : \n",
            "Roc 0.8579222799217704\n",
            "Epoch 10 - loss is 0.690811 : \n",
            "Roc 0.8571192963166574\n",
            "Epoch 11 - loss is 0.673391 : \n",
            "Roc 0.8667066370655852\n",
            "Epoch 12 - loss is 0.675087 : \n",
            "Roc 0.8677988633337995\n",
            "Epoch 13 - loss is 0.672285 : \n",
            "Roc 0.8725184990795954\n",
            "Epoch 14 - loss is 0.662728 : \n",
            "Roc 0.8765846571133582\n",
            "Epoch 15 - loss is 0.655923 : \n",
            "Roc 0.8761621904289412\n",
            "Epoch 16 - loss is 0.654802 : \n",
            "Roc 0.876557169482792\n",
            "Epoch 17 - loss is 0.643964 : \n",
            "Roc 0.8783048847530806\n",
            "Epoch 18 - loss is 0.645512 : \n",
            "Roc 0.876143609939987\n",
            "Epoch 19 - loss is 0.643632 : \n",
            "Roc 0.877165536832467\n",
            "Time mean 0.6925731420516967\n",
            "Time std 0.11620441013215817\n",
            "test\n",
            "Roc 0.877165536832467\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.000475 : \n",
            "Roc 0.7854648688737856\n",
            "Epoch 1 - loss is 0.814175 : \n",
            "Roc 0.8046995505517798\n",
            "Epoch 2 - loss is 0.803173 : \n",
            "Roc 0.8066224716004653\n",
            "Epoch 3 - loss is 0.781155 : \n",
            "Roc 0.7938161355558528\n",
            "Epoch 4 - loss is 0.769165 : \n",
            "Roc 0.8195305312729233\n",
            "Epoch 5 - loss is 0.776935 : \n",
            "Roc 0.795768134972259\n",
            "Epoch 6 - loss is 0.785164 : \n",
            "Roc 0.7995787182822892\n",
            "Epoch 7 - loss is 0.757555 : \n",
            "Roc 0.8221098939399466\n",
            "Epoch 8 - loss is 0.758362 : \n",
            "Roc 0.7435906008988964\n",
            "Epoch 9 - loss is 0.724483 : \n",
            "Roc 0.8552072568258174\n",
            "Epoch 10 - loss is 0.691368 : \n",
            "Roc 0.8669752823742538\n",
            "Epoch 11 - loss is 0.681778 : \n",
            "Roc 0.8717881068462838\n",
            "Epoch 12 - loss is 0.675175 : \n",
            "Roc 0.8664393082793306\n",
            "Epoch 13 - loss is 0.671159 : \n",
            "Roc 0.8745019189696328\n",
            "Epoch 14 - loss is 0.664732 : \n",
            "Roc 0.8730716077601753\n",
            "Epoch 15 - loss is 0.657954 : \n",
            "Roc 0.8762430748904749\n",
            "Epoch 16 - loss is 0.648455 : \n",
            "Roc 0.8689988125488861\n",
            "Epoch 17 - loss is 0.643149 : \n",
            "Roc 0.8786609361005402\n",
            "Epoch 18 - loss is 0.639508 : \n",
            "Roc 0.8780840293904282\n",
            "Epoch 19 - loss is 0.638435 : \n",
            "Roc 0.8821706038979197\n",
            "Time mean 0.6680820226669312\n",
            "Time std 0.056578299841567634\n",
            "test\n",
            "Roc 0.8821706038979197\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.983637 : \n",
            "Roc 0.8310450014188416\n",
            "Epoch 1 - loss is 0.795297 : \n",
            "Roc 0.8047991779745534\n",
            "Epoch 2 - loss is 0.765935 : \n",
            "Roc 0.8171595849525071\n",
            "Epoch 3 - loss is 0.782368 : \n",
            "Roc 0.789766913554275\n",
            "Epoch 4 - loss is 0.793180 : \n",
            "Roc 0.8279109901552405\n",
            "Epoch 5 - loss is 0.786461 : \n",
            "Roc 0.8292827333445061\n",
            "Epoch 6 - loss is 0.763591 : \n",
            "Roc 0.8284467353493313\n",
            "Epoch 7 - loss is 0.758477 : \n",
            "Roc 0.8109516129855217\n",
            "Epoch 8 - loss is 0.750279 : \n",
            "Roc 0.8147218461975632\n",
            "Epoch 9 - loss is 0.705596 : \n",
            "Roc 0.8616472378900981\n",
            "Epoch 10 - loss is 0.688272 : \n",
            "Roc 0.866027003912706\n",
            "Epoch 11 - loss is 0.674244 : \n",
            "Roc 0.8672062320706885\n",
            "Epoch 12 - loss is 0.671254 : \n",
            "Roc 0.874640530674258\n",
            "Epoch 13 - loss is 0.678775 : \n",
            "Roc 0.8676507239624893\n",
            "Epoch 14 - loss is 0.672397 : \n",
            "Roc 0.8797938892078939\n",
            "Epoch 15 - loss is 0.661417 : \n",
            "Roc 0.8796829134176892\n",
            "Epoch 16 - loss is 0.655388 : \n",
            "Roc 0.8846778051951977\n",
            "Epoch 17 - loss is 0.648507 : \n",
            "Roc 0.8777460866070373\n",
            "Epoch 18 - loss is 0.644373 : \n",
            "Roc 0.8859826292961173\n",
            "Epoch 19 - loss is 0.639823 : \n",
            "Roc 0.8857763772085752\n",
            "Time mean 0.6609259247779846\n",
            "Time std 0.0428168175530888\n",
            "test\n",
            "Roc 0.8857763772085752\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.997534 : \n",
            "Roc 0.8201603084415584\n",
            "Epoch 1 - loss is 0.807424 : \n",
            "Roc 0.8240969967532467\n",
            "Epoch 2 - loss is 0.822485 : \n",
            "Roc 0.8259806599650349\n",
            "Epoch 3 - loss is 0.796661 : \n",
            "Roc 0.825757250561938\n",
            "Epoch 4 - loss is 0.779527 : \n",
            "Roc 0.8139438686313687\n",
            "Epoch 5 - loss is 0.780820 : \n",
            "Roc 0.8218449519230769\n",
            "Epoch 6 - loss is 0.769501 : \n",
            "Roc 0.8160095763611389\n",
            "Epoch 7 - loss is 0.760697 : \n",
            "Roc 0.809472363573926\n",
            "Epoch 8 - loss is 0.768037 : \n",
            "Roc 0.8033556287462539\n",
            "Epoch 9 - loss is 0.722762 : \n",
            "Roc 0.8662483024787712\n",
            "Epoch 10 - loss is 0.691700 : \n",
            "Roc 0.8665624414647852\n",
            "Epoch 11 - loss is 0.682009 : \n",
            "Roc 0.8678283630432067\n",
            "Epoch 12 - loss is 0.669618 : \n",
            "Roc 0.874234554507992\n",
            "Epoch 13 - loss is 0.664573 : \n",
            "Roc 0.8733766233766234\n",
            "Epoch 14 - loss is 0.659968 : \n",
            "Roc 0.8783017763486514\n",
            "Epoch 15 - loss is 0.653299 : \n",
            "Roc 0.8689857017982018\n",
            "Epoch 16 - loss is 0.647401 : \n",
            "Roc 0.8739397321428571\n",
            "Epoch 17 - loss is 0.646153 : \n",
            "Roc 0.8807583041958043\n",
            "Epoch 18 - loss is 0.638536 : \n",
            "Roc 0.8804277753496502\n",
            "Epoch 19 - loss is 0.631530 : \n",
            "Roc 0.8814375078046952\n",
            "Time mean 0.6763975620269775\n",
            "Time std 0.09272707115860532\n",
            "test\n",
            "Roc 0.8814375078046952\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.994773 : \n",
            "Roc 0.8093199361619119\n",
            "Epoch 1 - loss is 0.809222 : \n",
            "Roc 0.8115908266682657\n",
            "Epoch 2 - loss is 0.789526 : \n",
            "Roc 0.8195501854727129\n",
            "Epoch 3 - loss is 0.781658 : \n",
            "Roc 0.8185548492505937\n",
            "Epoch 4 - loss is 0.789083 : \n",
            "Roc 0.7958598926344287\n",
            "Epoch 5 - loss is 0.790231 : \n",
            "Roc 0.8246265875727244\n",
            "Epoch 6 - loss is 0.767036 : \n",
            "Roc 0.8007331054115395\n",
            "Epoch 7 - loss is 0.769623 : \n",
            "Roc 0.8077855028581826\n",
            "Epoch 8 - loss is 0.766130 : \n",
            "Roc 0.8150976120674472\n",
            "Epoch 9 - loss is 0.721067 : \n",
            "Roc 0.8577368706595867\n",
            "Epoch 10 - loss is 0.695104 : \n",
            "Roc 0.8602754880811558\n",
            "Epoch 11 - loss is 0.689634 : \n",
            "Roc 0.8618828905013571\n",
            "Epoch 12 - loss is 0.679458 : \n",
            "Roc 0.8661334196055213\n",
            "Epoch 13 - loss is 0.673441 : \n",
            "Roc 0.8718387509519295\n",
            "Epoch 14 - loss is 0.670909 : \n",
            "Roc 0.8791269187963032\n",
            "Epoch 15 - loss is 0.663264 : \n",
            "Roc 0.8727660104234458\n",
            "Epoch 16 - loss is 0.664197 : \n",
            "Roc 0.8776435951019715\n",
            "Epoch 17 - loss is 0.653848 : \n",
            "Roc 0.879889919686089\n",
            "Epoch 18 - loss is 0.645795 : \n",
            "Roc 0.8826635788551531\n",
            "Epoch 19 - loss is 0.643832 : \n",
            "Roc 0.8827368602502977\n",
            "Time mean 0.6674532413482666\n",
            "Time std 0.07232894129952779\n",
            "test\n",
            "Roc 0.8827368602502977\n",
            "test\n",
            "Crossvalidation run 3\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.984123 : \n",
            "Roc 0.8057376912125815\n",
            "Epoch 1 - loss is 0.800989 : \n",
            "Roc 0.7418254929407007\n",
            "Epoch 2 - loss is 0.794537 : \n",
            "Roc 0.8379404697069587\n",
            "Epoch 3 - loss is 0.770578 : \n",
            "Roc 0.8201461145985611\n",
            "Epoch 4 - loss is 0.783165 : \n",
            "Roc 0.8202735100307977\n",
            "Epoch 5 - loss is 0.795901 : \n",
            "Roc 0.7876159895599444\n",
            "Epoch 6 - loss is 0.791817 : \n",
            "Roc 0.825458106012109\n",
            "Epoch 7 - loss is 0.771681 : \n",
            "Roc 0.823760561877554\n",
            "Epoch 8 - loss is 0.770923 : \n",
            "Roc 0.8192129430574268\n",
            "Epoch 9 - loss is 0.721910 : \n",
            "Roc 0.8717368854364728\n",
            "Epoch 10 - loss is 0.700130 : \n",
            "Roc 0.8818634289117563\n",
            "Epoch 11 - loss is 0.696765 : \n",
            "Roc 0.8812528265861529\n",
            "Epoch 12 - loss is 0.691301 : \n",
            "Roc 0.880854317749687\n",
            "Epoch 13 - loss is 0.677372 : \n",
            "Roc 0.8858349815117379\n",
            "Epoch 14 - loss is 0.668389 : \n",
            "Roc 0.8836485574059743\n",
            "Epoch 15 - loss is 0.668933 : \n",
            "Roc 0.8925747970431521\n",
            "Epoch 16 - loss is 0.659742 : \n",
            "Roc 0.895746744250485\n",
            "Epoch 17 - loss is 0.652164 : \n",
            "Roc 0.8910165916626059\n",
            "Epoch 18 - loss is 0.648948 : \n",
            "Roc 0.8967716803139023\n",
            "Epoch 19 - loss is 0.649576 : \n",
            "Roc 0.8611226563221575\n",
            "Time mean 0.6702782154083252\n",
            "Time std 0.09434949211427929\n",
            "test\n",
            "Roc 0.8611226563221575\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.000914 : \n",
            "Roc 0.802778450809992\n",
            "Epoch 1 - loss is 0.819538 : \n",
            "Roc 0.6739191347783112\n",
            "Epoch 2 - loss is 0.799495 : \n",
            "Roc 0.7631439179995378\n",
            "Epoch 3 - loss is 0.786216 : \n",
            "Roc 0.8081630205257888\n",
            "Epoch 4 - loss is 0.779251 : \n",
            "Roc 0.8013895864355216\n",
            "Epoch 5 - loss is 0.752171 : \n",
            "Roc 0.8150460674950579\n",
            "Epoch 6 - loss is 0.746320 : \n",
            "Roc 0.8287397547559755\n",
            "Epoch 7 - loss is 0.750299 : \n",
            "Roc 0.7898034148673975\n",
            "Epoch 8 - loss is 0.752909 : \n",
            "Roc 0.8186054273317759\n",
            "Epoch 9 - loss is 0.706008 : \n",
            "Roc 0.8439166757592873\n",
            "Epoch 10 - loss is 0.685425 : \n",
            "Roc 0.849900155002439\n",
            "Epoch 11 - loss is 0.673054 : \n",
            "Roc 0.85072751662345\n",
            "Epoch 12 - loss is 0.672884 : \n",
            "Roc 0.8553583348309414\n",
            "Epoch 13 - loss is 0.668085 : \n",
            "Roc 0.8602075765063798\n",
            "Epoch 14 - loss is 0.658991 : \n",
            "Roc 0.8612345077790045\n",
            "Epoch 15 - loss is 0.649729 : \n",
            "Roc 0.8647824349824138\n",
            "Epoch 16 - loss is 0.654380 : \n",
            "Roc 0.8614992634977279\n",
            "Epoch 17 - loss is 0.652250 : \n",
            "Roc 0.8557139498985906\n",
            "Epoch 18 - loss is 0.645288 : \n",
            "Roc 0.8634905474185515\n",
            "Epoch 19 - loss is 0.644689 : \n",
            "Roc 0.8625675127082746\n",
            "Time mean 0.6841707825660706\n",
            "Time std 0.10290344503981577\n",
            "test\n",
            "Roc 0.8625675127082746\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.961384 : \n",
            "Roc 0.7843901272558551\n",
            "Epoch 1 - loss is 0.811310 : \n",
            "Roc 0.5412951851120504\n",
            "Epoch 2 - loss is 0.796692 : \n",
            "Roc 0.8125980899636918\n",
            "Epoch 3 - loss is 0.794466 : \n",
            "Roc 0.7667557157081292\n",
            "Epoch 4 - loss is 0.786664 : \n",
            "Roc 0.8116394002724927\n",
            "Epoch 5 - loss is 0.761013 : \n",
            "Roc 0.7898037420762332\n",
            "Epoch 6 - loss is 0.774523 : \n",
            "Roc 0.8377240541226415\n",
            "Epoch 7 - loss is 0.750957 : \n",
            "Roc 0.8135506921322955\n",
            "Epoch 8 - loss is 0.764277 : \n",
            "Roc 0.807177626680394\n",
            "Epoch 9 - loss is 0.714572 : \n",
            "Roc 0.8543163578961066\n",
            "Epoch 10 - loss is 0.688836 : \n",
            "Roc 0.8617383795851885\n",
            "Epoch 11 - loss is 0.675453 : \n",
            "Roc 0.8650128960361477\n",
            "Epoch 12 - loss is 0.667897 : \n",
            "Roc 0.8669133444963281\n",
            "Epoch 13 - loss is 0.662362 : \n",
            "Roc 0.8703554580537353\n",
            "Epoch 14 - loss is 0.662900 : \n",
            "Roc 0.8751781075946791\n",
            "Epoch 15 - loss is 0.654499 : \n",
            "Roc 0.8741968750463699\n",
            "Epoch 16 - loss is 0.648998 : \n",
            "Roc 0.8723955390634422\n",
            "Epoch 17 - loss is 0.647622 : \n",
            "Roc 0.8813215593036938\n",
            "Epoch 18 - loss is 0.643528 : \n",
            "Roc 0.8797663875157088\n",
            "Epoch 19 - loss is 0.642520 : \n",
            "Roc 0.8804641697931803\n",
            "Time mean 0.6752328157424927\n",
            "Time std 0.11235099928338381\n",
            "test\n",
            "Roc 0.8804641697931803\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.024707 : \n",
            "Roc 0.7485242484349192\n",
            "Epoch 1 - loss is 0.818822 : \n",
            "Roc 0.7784787524007866\n",
            "Epoch 2 - loss is 0.799039 : \n",
            "Roc 0.8090393001771256\n",
            "Epoch 3 - loss is 0.790696 : \n",
            "Roc 0.7962693369937711\n",
            "Epoch 4 - loss is 0.762024 : \n",
            "Roc 0.7968410838543718\n",
            "Epoch 5 - loss is 0.767484 : \n",
            "Roc 0.8272850809643869\n",
            "Epoch 6 - loss is 0.753188 : \n",
            "Roc 0.7917926514510066\n",
            "Epoch 7 - loss is 0.764314 : \n",
            "Roc 0.8092709274288333\n",
            "Epoch 8 - loss is 0.751404 : \n",
            "Roc 0.8203334135233364\n",
            "Epoch 9 - loss is 0.705577 : \n",
            "Roc 0.854518038426735\n",
            "Epoch 10 - loss is 0.684978 : \n",
            "Roc 0.8572750611448774\n",
            "Epoch 11 - loss is 0.674579 : \n",
            "Roc 0.8545537111854835\n",
            "Epoch 12 - loss is 0.669543 : \n",
            "Roc 0.8670319046147774\n",
            "Epoch 13 - loss is 0.662603 : \n",
            "Roc 0.8697060839840909\n",
            "Epoch 14 - loss is 0.654026 : \n",
            "Roc 0.8653375959823235\n",
            "Epoch 15 - loss is 0.652081 : \n",
            "Roc 0.8620766341268213\n",
            "Epoch 16 - loss is 0.651626 : \n",
            "Roc 0.8686616485020586\n",
            "Epoch 17 - loss is 0.652070 : \n",
            "Roc 0.8639269005227288\n",
            "Epoch 18 - loss is 0.634935 : \n",
            "Roc 0.8765637050611684\n",
            "Epoch 19 - loss is 0.635011 : \n",
            "Roc 0.874204782468893\n",
            "Time mean 0.6769419193267823\n",
            "Time std 0.07287607355423718\n",
            "test\n",
            "Roc 0.874204782468893\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.955860 : \n",
            "Roc 0.8170119654472627\n",
            "Epoch 1 - loss is 0.786242 : \n",
            "Roc 0.8113724207606362\n",
            "Epoch 2 - loss is 0.791163 : \n",
            "Roc 0.8289314015279621\n",
            "Epoch 3 - loss is 0.773309 : \n",
            "Roc 0.7984273553768902\n",
            "Epoch 4 - loss is 0.776122 : \n",
            "Roc 0.818111056340546\n",
            "Epoch 5 - loss is 0.775179 : \n",
            "Roc 0.7333892305699468\n",
            "Epoch 6 - loss is 0.769993 : \n",
            "Roc 0.7867491116715812\n",
            "Epoch 7 - loss is 0.769609 : \n",
            "Roc 0.8192328109159235\n",
            "Epoch 8 - loss is 0.770890 : \n",
            "Roc 0.8356340878190328\n",
            "Epoch 9 - loss is 0.709650 : \n",
            "Roc 0.8546516523680324\n",
            "Epoch 10 - loss is 0.693754 : \n",
            "Roc 0.8548345248373441\n",
            "Epoch 11 - loss is 0.689049 : \n",
            "Roc 0.8635636569101469\n",
            "Epoch 12 - loss is 0.679125 : \n",
            "Roc 0.8675295082117749\n",
            "Epoch 13 - loss is 0.668147 : \n",
            "Roc 0.8644521318612101\n",
            "Epoch 14 - loss is 0.667991 : \n",
            "Roc 0.8722480477484705\n",
            "Epoch 15 - loss is 0.665623 : \n",
            "Roc 0.872803601024633\n",
            "Epoch 16 - loss is 0.657883 : \n",
            "Roc 0.8759447678252302\n",
            "Epoch 17 - loss is 0.646208 : \n",
            "Roc 0.8786997728356625\n",
            "Epoch 18 - loss is 0.638023 : \n",
            "Roc 0.8820889701915919\n",
            "Epoch 19 - loss is 0.636105 : \n",
            "Roc 0.8738309887168078\n",
            "Time mean 0.6846928000450134\n",
            "Time std 0.11099686168996074\n",
            "test\n",
            "Roc 0.8738309887168078\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.962424 : \n",
            "Roc 0.8258008286514971\n",
            "Epoch 1 - loss is 0.802383 : \n",
            "Roc 0.8237437578176928\n",
            "Epoch 2 - loss is 0.783221 : \n",
            "Roc 0.8155072377708776\n",
            "Epoch 3 - loss is 0.774152 : \n",
            "Roc 0.8185071247555324\n",
            "Epoch 4 - loss is 0.772282 : \n",
            "Roc 0.7938833796568547\n",
            "Epoch 5 - loss is 0.773955 : \n",
            "Roc 0.8217537299852695\n",
            "Epoch 6 - loss is 0.757638 : \n",
            "Roc 0.8291373672788012\n",
            "Epoch 7 - loss is 0.768006 : \n",
            "Roc 0.8258009244272133\n",
            "Epoch 8 - loss is 0.773842 : \n",
            "Roc 0.8287276387646464\n",
            "Epoch 9 - loss is 0.716438 : \n",
            "Roc 0.8528103468421789\n",
            "Epoch 10 - loss is 0.698003 : \n",
            "Roc 0.8605990196397684\n",
            "Epoch 11 - loss is 0.685947 : \n",
            "Roc 0.8600575037400416\n",
            "Epoch 12 - loss is 0.678330 : \n",
            "Roc 0.8665363478420774\n",
            "Epoch 13 - loss is 0.673829 : \n",
            "Roc 0.8641024953405115\n",
            "Epoch 14 - loss is 0.667141 : \n",
            "Roc 0.8710692209411688\n",
            "Epoch 15 - loss is 0.659778 : \n",
            "Roc 0.8733193756189506\n",
            "Epoch 16 - loss is 0.656956 : \n",
            "Roc 0.8807148891012981\n",
            "Epoch 17 - loss is 0.651701 : \n",
            "Roc 0.8787964057289204\n",
            "Epoch 18 - loss is 0.647194 : \n",
            "Roc 0.8569942132312236\n",
            "Epoch 19 - loss is 0.651740 : \n",
            "Roc 0.8765303522822394\n",
            "Time mean 0.665415346622467\n",
            "Time std 0.10528320914593538\n",
            "test\n",
            "Roc 0.8765303522822394\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.947418 : \n",
            "Roc 0.8037745139758538\n",
            "Epoch 1 - loss is 0.799395 : \n",
            "Roc 0.5505575549044163\n",
            "Epoch 2 - loss is 0.782888 : \n",
            "Roc 0.7303425168289787\n",
            "Epoch 3 - loss is 0.775831 : \n",
            "Roc 0.8193327578450802\n",
            "Epoch 4 - loss is 0.779088 : \n",
            "Roc 0.7905350419660624\n",
            "Epoch 5 - loss is 0.765196 : \n",
            "Roc 0.8009059887252935\n",
            "Epoch 6 - loss is 0.764271 : \n",
            "Roc 0.802777541368284\n",
            "Epoch 7 - loss is 0.763853 : \n",
            "Roc 0.8010075097164302\n",
            "Epoch 8 - loss is 0.764876 : \n",
            "Roc 0.7746355578791231\n",
            "Epoch 9 - loss is 0.721810 : \n",
            "Roc 0.8573538888010797\n",
            "Epoch 10 - loss is 0.692905 : \n",
            "Roc 0.8530614353973909\n",
            "Epoch 11 - loss is 0.687739 : \n",
            "Roc 0.8597075141744378\n",
            "Epoch 12 - loss is 0.686108 : \n",
            "Roc 0.8730669471158717\n",
            "Epoch 13 - loss is 0.678514 : \n",
            "Roc 0.8629474725123304\n",
            "Epoch 14 - loss is 0.664840 : \n",
            "Roc 0.8719240760779259\n",
            "Epoch 15 - loss is 0.670555 : \n",
            "Roc 0.875020567625949\n",
            "Epoch 16 - loss is 0.661597 : \n",
            "Roc 0.8709764455089627\n",
            "Epoch 17 - loss is 0.656483 : \n",
            "Roc 0.8784039927537113\n",
            "Epoch 18 - loss is 0.652378 : \n",
            "Roc 0.8720663270475425\n",
            "Epoch 19 - loss is 0.644446 : \n",
            "Roc 0.8799418027372167\n",
            "Time mean 0.6795350790023804\n",
            "Time std 0.12354653076914042\n",
            "test\n",
            "Roc 0.8799418027372167\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977240 : \n",
            "Roc 0.8055280269309102\n",
            "Epoch 1 - loss is 0.805808 : \n",
            "Roc 0.8180020589884894\n",
            "Epoch 2 - loss is 0.791662 : \n",
            "Roc 0.778176762945115\n",
            "Epoch 3 - loss is 0.781338 : \n",
            "Roc 0.8171554638356294\n",
            "Epoch 4 - loss is 0.776189 : \n",
            "Roc 0.8234311398656045\n",
            "Epoch 5 - loss is 0.755276 : \n",
            "Roc 0.8269109324596042\n",
            "Epoch 6 - loss is 0.756269 : \n",
            "Roc 0.8352117449476089\n",
            "Epoch 7 - loss is 0.756722 : \n",
            "Roc 0.8094411255280427\n",
            "Epoch 8 - loss is 0.749048 : \n",
            "Roc 0.8174981052674547\n",
            "Epoch 9 - loss is 0.709317 : \n",
            "Roc 0.8537561821659255\n",
            "Epoch 10 - loss is 0.684525 : \n",
            "Roc 0.8640363082171538\n",
            "Epoch 11 - loss is 0.677205 : \n",
            "Roc 0.8658360588307095\n",
            "Epoch 12 - loss is 0.670828 : \n",
            "Roc 0.8688109164853702\n",
            "Epoch 13 - loss is 0.669042 : \n",
            "Roc 0.8798352966203309\n",
            "Epoch 14 - loss is 0.658942 : \n",
            "Roc 0.8767387678959502\n",
            "Epoch 15 - loss is 0.661029 : \n",
            "Roc 0.8803463151131561\n",
            "Epoch 16 - loss is 0.654300 : \n",
            "Roc 0.8710648730519382\n",
            "Epoch 17 - loss is 0.645300 : \n",
            "Roc 0.8800951232272819\n",
            "Epoch 18 - loss is 0.645880 : \n",
            "Roc 0.8612989053921086\n",
            "Epoch 19 - loss is 0.640104 : \n",
            "Roc 0.8797893756036947\n",
            "Time mean 0.6603176593780518\n",
            "Time std 0.05107356242923114\n",
            "test\n",
            "Roc 0.8797893756036947\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.015827 : \n",
            "Roc 0.8081989299762737\n",
            "Epoch 1 - loss is 0.811329 : \n",
            "Roc 0.794709489729021\n",
            "Epoch 2 - loss is 0.785346 : \n",
            "Roc 0.8158478240509491\n",
            "Epoch 3 - loss is 0.782284 : \n",
            "Roc 0.7682013299200798\n",
            "Epoch 4 - loss is 0.774215 : \n",
            "Roc 0.8211382367632367\n",
            "Epoch 5 - loss is 0.766255 : \n",
            "Roc 0.840063061938062\n",
            "Epoch 6 - loss is 0.760513 : \n",
            "Roc 0.8255923763736264\n",
            "Epoch 7 - loss is 0.767864 : \n",
            "Roc 0.8202678181193805\n",
            "Epoch 8 - loss is 0.756157 : \n",
            "Roc 0.8309196662712288\n",
            "Epoch 9 - loss is 0.703971 : \n",
            "Roc 0.8601049341283716\n",
            "Epoch 10 - loss is 0.682612 : \n",
            "Roc 0.8659510411463537\n",
            "Epoch 11 - loss is 0.678403 : \n",
            "Roc 0.8693998384428071\n",
            "Epoch 12 - loss is 0.670757 : \n",
            "Roc 0.8773050191995504\n",
            "Epoch 13 - loss is 0.656498 : \n",
            "Roc 0.8702826470404597\n",
            "Epoch 14 - loss is 0.653236 : \n",
            "Roc 0.880682403533966\n",
            "Epoch 15 - loss is 0.648274 : \n",
            "Roc 0.8778787618631368\n",
            "Epoch 16 - loss is 0.641947 : \n",
            "Roc 0.8760013424075923\n",
            "Epoch 17 - loss is 0.642631 : \n",
            "Roc 0.8741102647352647\n",
            "Epoch 18 - loss is 0.637816 : \n",
            "Roc 0.8798954951298701\n",
            "Epoch 19 - loss is 0.633098 : \n",
            "Roc 0.8791509271978022\n",
            "Time mean 0.6868726015090942\n",
            "Time std 0.1173754244871496\n",
            "test\n",
            "Roc 0.8791509271978022\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.953262 : \n",
            "Roc 0.760498913062034\n",
            "Epoch 1 - loss is 0.815552 : \n",
            "Roc 0.8058634276290013\n",
            "Epoch 2 - loss is 0.790291 : \n",
            "Roc 0.8185450645188556\n",
            "Epoch 3 - loss is 0.790266 : \n",
            "Roc 0.8002320022350826\n",
            "Epoch 4 - loss is 0.780943 : \n",
            "Roc 0.7881686771167602\n",
            "Epoch 5 - loss is 0.780234 : \n",
            "Roc 0.8163013422570086\n",
            "Epoch 6 - loss is 0.772265 : \n",
            "Roc 0.8226544436630538\n",
            "Epoch 7 - loss is 0.766618 : \n",
            "Roc 0.8130893479246584\n",
            "Epoch 8 - loss is 0.764960 : \n",
            "Roc 0.7822466243716433\n",
            "Epoch 9 - loss is 0.723286 : \n",
            "Roc 0.8516761661838839\n",
            "Epoch 10 - loss is 0.693607 : \n",
            "Roc 0.8638433760072549\n",
            "Epoch 11 - loss is 0.686703 : \n",
            "Roc 0.8670552662467144\n",
            "Epoch 12 - loss is 0.681775 : \n",
            "Roc 0.8700966523309938\n",
            "Epoch 13 - loss is 0.667268 : \n",
            "Roc 0.8737923663270324\n",
            "Epoch 14 - loss is 0.661519 : \n",
            "Roc 0.8769819806878543\n",
            "Epoch 15 - loss is 0.654209 : \n",
            "Roc 0.8728159750110442\n",
            "Epoch 16 - loss is 0.653379 : \n",
            "Roc 0.8752167734451541\n",
            "Epoch 17 - loss is 0.653572 : \n",
            "Roc 0.8816782355505536\n",
            "Epoch 18 - loss is 0.642668 : \n",
            "Roc 0.8712537489054631\n",
            "Epoch 19 - loss is 0.642332 : \n",
            "Roc 0.8839801457383745\n",
            "Time mean 0.6645326614379883\n",
            "Time std 0.06598368291258593\n",
            "test\n",
            "Roc 0.8839801457383745\n",
            "test\n",
            "Crossvalidation run 4\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.017468 : \n",
            "Roc 0.7987195962838752\n",
            "Epoch 1 - loss is 0.816940 : \n",
            "Roc 0.8359482241076747\n",
            "Epoch 2 - loss is 0.817626 : \n",
            "Roc 0.7239255389623005\n",
            "Epoch 3 - loss is 0.800294 : \n",
            "Roc 0.817127440020638\n",
            "Epoch 4 - loss is 0.784227 : \n",
            "Roc 0.8240436186035549\n",
            "Epoch 5 - loss is 0.781525 : \n",
            "Roc 0.8440563462034568\n",
            "Epoch 6 - loss is 0.775473 : \n",
            "Roc 0.8124493404101496\n",
            "Epoch 7 - loss is 0.765566 : \n",
            "Roc 0.8179921365169451\n",
            "Epoch 8 - loss is 0.763622 : \n",
            "Roc 0.84383808199807\n",
            "Epoch 9 - loss is 0.714627 : \n",
            "Roc 0.8767890100737938\n",
            "Epoch 10 - loss is 0.692341 : \n",
            "Roc 0.8796857632419589\n",
            "Epoch 11 - loss is 0.685008 : \n",
            "Roc 0.8820909491915168\n",
            "Epoch 12 - loss is 0.679700 : \n",
            "Roc 0.8876358353796224\n",
            "Epoch 13 - loss is 0.670088 : \n",
            "Roc 0.883013570798419\n",
            "Epoch 14 - loss is 0.662257 : \n",
            "Roc 0.8818735807352629\n",
            "Epoch 15 - loss is 0.661866 : \n",
            "Roc 0.8921597666115682\n",
            "Epoch 16 - loss is 0.651712 : \n",
            "Roc 0.8985068459120398\n",
            "Epoch 17 - loss is 0.649630 : \n",
            "Roc 0.8893958828981187\n",
            "Epoch 18 - loss is 0.643487 : \n",
            "Roc 0.8993189917925493\n",
            "Epoch 19 - loss is 0.642249 : \n",
            "Roc 0.9047762936209922\n",
            "Time mean 0.6650456428527832\n",
            "Time std 0.11347116183573057\n",
            "test\n",
            "Roc 0.9047762936209922\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.976449 : \n",
            "Roc 0.8172904739287823\n",
            "Epoch 1 - loss is 0.771655 : \n",
            "Roc 0.8241009136479165\n",
            "Epoch 2 - loss is 0.762363 : \n",
            "Roc 0.7916029514646608\n",
            "Epoch 3 - loss is 0.765896 : \n",
            "Roc 0.8129135805242484\n",
            "Epoch 4 - loss is 0.750407 : \n",
            "Roc 0.797282563348823\n",
            "Epoch 5 - loss is 0.758248 : \n",
            "Roc 0.8198792633693616\n",
            "Epoch 6 - loss is 0.754129 : \n",
            "Roc 0.822448697562322\n",
            "Epoch 7 - loss is 0.756711 : \n",
            "Roc 0.8098063111345022\n",
            "Epoch 8 - loss is 0.753717 : \n",
            "Roc 0.8218376533978589\n",
            "Epoch 9 - loss is 0.703414 : \n",
            "Roc 0.8447201692511104\n",
            "Epoch 10 - loss is 0.676945 : \n",
            "Roc 0.8498395821031554\n",
            "Epoch 11 - loss is 0.668824 : \n",
            "Roc 0.850651499640574\n",
            "Epoch 12 - loss is 0.662336 : \n",
            "Roc 0.8584798448691946\n",
            "Epoch 13 - loss is 0.659826 : \n",
            "Roc 0.8556252968473211\n",
            "Epoch 14 - loss is 0.657381 : \n",
            "Roc 0.8643485966342327\n",
            "Epoch 15 - loss is 0.655706 : \n",
            "Roc 0.8563218851890837\n",
            "Epoch 16 - loss is 0.646837 : \n",
            "Roc 0.8634939571512926\n",
            "Epoch 17 - loss is 0.640902 : \n",
            "Roc 0.8611334192318554\n",
            "Epoch 18 - loss is 0.637045 : \n",
            "Roc 0.8606311856563373\n",
            "Epoch 19 - loss is 0.629628 : \n",
            "Roc 0.8642880237349491\n",
            "Time mean 0.655487334728241\n",
            "Time std 0.03097193794506973\n",
            "test\n",
            "Roc 0.8642880237349491\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.970086 : \n",
            "Roc 0.794160981491268\n",
            "Epoch 1 - loss is 0.792760 : \n",
            "Roc 0.7925261012043021\n",
            "Epoch 2 - loss is 0.796055 : \n",
            "Roc 0.7268776868327268\n",
            "Epoch 3 - loss is 0.801544 : \n",
            "Roc 0.7636696271049987\n",
            "Epoch 4 - loss is 0.782522 : \n",
            "Roc 0.8110893355354947\n",
            "Epoch 5 - loss is 0.761252 : \n",
            "Roc 0.8037379861790409\n",
            "Epoch 6 - loss is 0.762519 : \n",
            "Roc 0.8254264975400701\n",
            "Epoch 7 - loss is 0.764855 : \n",
            "Roc 0.7702214184155928\n",
            "Epoch 8 - loss is 0.756336 : \n",
            "Roc 0.825474056310345\n",
            "Epoch 9 - loss is 0.704508 : \n",
            "Roc 0.8596626294977756\n",
            "Epoch 10 - loss is 0.679119 : \n",
            "Roc 0.864445424789229\n",
            "Epoch 11 - loss is 0.675625 : \n",
            "Roc 0.8639052522764482\n",
            "Epoch 12 - loss is 0.663919 : \n",
            "Roc 0.8650800490197756\n",
            "Epoch 13 - loss is 0.662723 : \n",
            "Roc 0.8642422537226151\n",
            "Epoch 14 - loss is 0.659226 : \n",
            "Roc 0.8713235643814488\n",
            "Epoch 15 - loss is 0.649960 : \n",
            "Roc 0.8652206327447078\n",
            "Epoch 16 - loss is 0.658637 : \n",
            "Roc 0.8726709994608737\n",
            "Epoch 17 - loss is 0.635169 : \n",
            "Roc 0.8751814367085984\n",
            "Epoch 18 - loss is 0.635531 : \n",
            "Roc 0.87708559475286\n",
            "Epoch 19 - loss is 0.632234 : \n",
            "Roc 0.8794617260333285\n",
            "Time mean 0.6632772922515869\n",
            "Time std 0.05596029431143766\n",
            "test\n",
            "Roc 0.8794617260333285\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.963603 : \n",
            "Roc 0.7823219762275949\n",
            "Epoch 1 - loss is 0.804899 : \n",
            "Roc 0.7883716044054185\n",
            "Epoch 2 - loss is 0.784741 : \n",
            "Roc 0.8144947754640603\n",
            "Epoch 3 - loss is 0.786590 : \n",
            "Roc 0.7965964846903919\n",
            "Epoch 4 - loss is 0.778469 : \n",
            "Roc 0.8028942498264515\n",
            "Epoch 5 - loss is 0.771852 : \n",
            "Roc 0.8035459901460638\n",
            "Epoch 6 - loss is 0.758422 : \n",
            "Roc 0.7818717919085143\n",
            "Epoch 7 - loss is 0.751985 : \n",
            "Roc 0.8174005820850816\n",
            "Epoch 8 - loss is 0.762260 : \n",
            "Roc 0.8276880946933873\n",
            "Epoch 9 - loss is 0.715317 : \n",
            "Roc 0.8528734554579911\n",
            "Epoch 10 - loss is 0.692996 : \n",
            "Roc 0.8534450057744664\n",
            "Epoch 11 - loss is 0.679132 : \n",
            "Roc 0.8589462758426043\n",
            "Epoch 12 - loss is 0.673738 : \n",
            "Roc 0.8628877717320805\n",
            "Epoch 13 - loss is 0.667650 : \n",
            "Roc 0.8550556848815901\n",
            "Epoch 14 - loss is 0.658552 : \n",
            "Roc 0.8672188180779714\n",
            "Epoch 15 - loss is 0.655888 : \n",
            "Roc 0.866925574242971\n",
            "Epoch 16 - loss is 0.649914 : \n",
            "Roc 0.8619005305905205\n",
            "Epoch 17 - loss is 0.648106 : \n",
            "Roc 0.865250821750988\n",
            "Epoch 18 - loss is 0.638587 : \n",
            "Roc 0.8670523452038122\n",
            "Epoch 19 - loss is 0.637469 : \n",
            "Roc 0.8680256317124732\n",
            "Time mean 0.6844684958457947\n",
            "Time std 0.0938100999645373\n",
            "test\n",
            "Roc 0.8680256317124732\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.002525 : \n",
            "Roc 0.7615105617642974\n",
            "Epoch 1 - loss is 0.794414 : \n",
            "Roc 0.8068986880658716\n",
            "Epoch 2 - loss is 0.779057 : \n",
            "Roc 0.812046860679508\n",
            "Epoch 3 - loss is 0.775158 : \n",
            "Roc 0.7642823691597302\n",
            "Epoch 4 - loss is 0.788437 : \n",
            "Roc 0.8084413814608346\n",
            "Epoch 5 - loss is 0.774786 : \n",
            "Roc 0.8257656172795727\n",
            "Epoch 6 - loss is 0.771199 : \n",
            "Roc 0.8209388391232203\n",
            "Epoch 7 - loss is 0.764780 : \n",
            "Roc 0.8032923296526342\n",
            "Epoch 8 - loss is 0.763701 : \n",
            "Roc 0.8287290890899573\n",
            "Epoch 9 - loss is 0.715581 : \n",
            "Roc 0.8563492793828288\n",
            "Epoch 10 - loss is 0.698284 : \n",
            "Roc 0.8648020512976814\n",
            "Epoch 11 - loss is 0.684302 : \n",
            "Roc 0.8666372234176012\n",
            "Epoch 12 - loss is 0.676122 : \n",
            "Roc 0.8705227749490506\n",
            "Epoch 13 - loss is 0.673359 : \n",
            "Roc 0.8743501042627065\n",
            "Epoch 14 - loss is 0.666562 : \n",
            "Roc 0.8697818093081696\n",
            "Epoch 15 - loss is 0.663229 : \n",
            "Roc 0.880768615333036\n",
            "Epoch 16 - loss is 0.655788 : \n",
            "Roc 0.8773897729353044\n",
            "Epoch 17 - loss is 0.647642 : \n",
            "Roc 0.868048428381104\n",
            "Epoch 18 - loss is 0.644864 : \n",
            "Roc 0.8801438010628877\n",
            "Epoch 19 - loss is 0.641962 : \n",
            "Roc 0.8861129460227289\n",
            "Time mean 0.6597706794738769\n",
            "Time std 0.07009962819704446\n",
            "test\n",
            "Roc 0.8861129460227289\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.993540 : \n",
            "Roc 0.8120491597596412\n",
            "Epoch 1 - loss is 0.805192 : \n",
            "Roc 0.8267516899625134\n",
            "Epoch 2 - loss is 0.795509 : \n",
            "Roc 0.8154539864726379\n",
            "Epoch 3 - loss is 0.786992 : \n",
            "Roc 0.8281095980676292\n",
            "Epoch 4 - loss is 0.775256 : \n",
            "Roc 0.8279280073096027\n",
            "Epoch 5 - loss is 0.776322 : \n",
            "Roc 0.8073155407592716\n",
            "Epoch 6 - loss is 0.755826 : \n",
            "Roc 0.832156888285289\n",
            "Epoch 7 - loss is 0.753442 : \n",
            "Roc 0.8472616764964477\n",
            "Epoch 8 - loss is 0.759542 : \n",
            "Roc 0.8250047408979548\n",
            "Epoch 9 - loss is 0.705764 : \n",
            "Roc 0.8612263505812628\n",
            "Epoch 10 - loss is 0.685152 : \n",
            "Roc 0.8702306087696078\n",
            "Epoch 11 - loss is 0.678801 : \n",
            "Roc 0.8682938322354243\n",
            "Epoch 12 - loss is 0.667931 : \n",
            "Roc 0.8718051615449005\n",
            "Epoch 13 - loss is 0.662005 : \n",
            "Roc 0.8769253313360904\n",
            "Epoch 14 - loss is 0.661539 : \n",
            "Roc 0.8689602396691524\n",
            "Epoch 15 - loss is 0.649465 : \n",
            "Roc 0.8806082907291023\n",
            "Epoch 16 - loss is 0.647682 : \n",
            "Roc 0.8788174763864972\n",
            "Epoch 17 - loss is 0.643375 : \n",
            "Roc 0.8793800629438007\n",
            "Epoch 18 - loss is 0.636316 : \n",
            "Roc 0.8800265490285468\n",
            "Epoch 19 - loss is 0.640936 : \n",
            "Roc 0.8822395427284203\n",
            "Time mean 0.666470468044281\n",
            "Time std 0.08956236348557205\n",
            "test\n",
            "Roc 0.8822395427284203\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.964327 : \n",
            "Roc 0.7962157594621211\n",
            "Epoch 1 - loss is 0.785260 : \n",
            "Roc 0.8306321860042879\n",
            "Epoch 2 - loss is 0.775056 : \n",
            "Roc 0.8142299602426777\n",
            "Epoch 3 - loss is 0.803501 : \n",
            "Roc 0.8073247091150044\n",
            "Epoch 4 - loss is 0.789418 : \n",
            "Roc 0.8224983485108228\n",
            "Epoch 5 - loss is 0.773412 : \n",
            "Roc 0.8220424159158977\n",
            "Epoch 6 - loss is 0.772832 : \n",
            "Roc 0.821293875913385\n",
            "Epoch 7 - loss is 0.769247 : \n",
            "Roc 0.8299592294941782\n",
            "Epoch 8 - loss is 0.766961 : \n",
            "Roc 0.8165316093002144\n",
            "Epoch 9 - loss is 0.720960 : \n",
            "Roc 0.8535264866442146\n",
            "Epoch 10 - loss is 0.697626 : \n",
            "Roc 0.8604030646775849\n",
            "Epoch 11 - loss is 0.685772 : \n",
            "Roc 0.8618331732503334\n",
            "Epoch 12 - loss is 0.688054 : \n",
            "Roc 0.8664115470502174\n",
            "Epoch 13 - loss is 0.671204 : \n",
            "Roc 0.8677485440552468\n",
            "Epoch 14 - loss is 0.667121 : \n",
            "Roc 0.8686256570495285\n",
            "Epoch 15 - loss is 0.670205 : \n",
            "Roc 0.8752398205449305\n",
            "Epoch 16 - loss is 0.659623 : \n",
            "Roc 0.8800866879840484\n",
            "Epoch 17 - loss is 0.650508 : \n",
            "Roc 0.8787517173461076\n",
            "Epoch 18 - loss is 0.646486 : \n",
            "Roc 0.8805160751701134\n",
            "Epoch 19 - loss is 0.638794 : \n",
            "Roc 0.8814668466080642\n",
            "Time mean 0.654677951335907\n",
            "Time std 0.07271194249078457\n",
            "test\n",
            "Roc 0.8814668466080642\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.990819 : \n",
            "Roc 0.8027480392314628\n",
            "Epoch 1 - loss is 0.813224 : \n",
            "Roc 0.791762220975857\n",
            "Epoch 2 - loss is 0.794177 : \n",
            "Roc 0.8256471232838001\n",
            "Epoch 3 - loss is 0.772442 : \n",
            "Roc 0.823326738238038\n",
            "Epoch 4 - loss is 0.753332 : \n",
            "Roc 0.8161225353268027\n",
            "Epoch 5 - loss is 0.766240 : \n",
            "Roc 0.8164237693461907\n",
            "Epoch 6 - loss is 0.755726 : \n",
            "Roc 0.8173568098316505\n",
            "Epoch 7 - loss is 0.759950 : \n",
            "Roc 0.8349931295094339\n",
            "Epoch 8 - loss is 0.763047 : \n",
            "Roc 0.8152893828647101\n",
            "Epoch 9 - loss is 0.714409 : \n",
            "Roc 0.8591822212662975\n",
            "Epoch 10 - loss is 0.691368 : \n",
            "Roc 0.8658633366995663\n",
            "Epoch 11 - loss is 0.687714 : \n",
            "Roc 0.8658831573093111\n",
            "Epoch 12 - loss is 0.678578 : \n",
            "Roc 0.8716197520029609\n",
            "Epoch 13 - loss is 0.666180 : \n",
            "Roc 0.8662095105172865\n",
            "Epoch 14 - loss is 0.662590 : \n",
            "Roc 0.8727960365060307\n",
            "Epoch 15 - loss is 0.655787 : \n",
            "Roc 0.8742527532004398\n",
            "Epoch 16 - loss is 0.656452 : \n",
            "Roc 0.8810527922921769\n",
            "Epoch 17 - loss is 0.644988 : \n",
            "Roc 0.8767085463721809\n",
            "Epoch 18 - loss is 0.637410 : \n",
            "Roc 0.8845898880351417\n",
            "Epoch 19 - loss is 0.634132 : \n",
            "Roc 0.8844391729036173\n",
            "Time mean 0.6491669654846192\n",
            "Time std 0.04734540718627438\n",
            "test\n",
            "Roc 0.8844391729036173\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.972934 : \n",
            "Roc 0.7876537524975026\n",
            "Epoch 1 - loss is 0.821103 : \n",
            "Roc 0.8126480940934065\n",
            "Epoch 2 - loss is 0.784360 : \n",
            "Roc 0.8108959399975025\n",
            "Epoch 3 - loss is 0.762686 : \n",
            "Roc 0.7881255463286714\n",
            "Epoch 4 - loss is 0.770148 : \n",
            "Roc 0.8200336772602397\n",
            "Epoch 5 - loss is 0.767865 : \n",
            "Roc 0.8321424669080918\n",
            "Epoch 6 - loss is 0.767034 : \n",
            "Roc 0.8272883366633367\n",
            "Epoch 7 - loss is 0.766888 : \n",
            "Roc 0.825379893543956\n",
            "Epoch 8 - loss is 0.758138 : \n",
            "Roc 0.8279671500374626\n",
            "Epoch 9 - loss is 0.714334 : \n",
            "Roc 0.8634721723588911\n",
            "Epoch 10 - loss is 0.690102 : \n",
            "Roc 0.8647919463349151\n",
            "Epoch 11 - loss is 0.678638 : \n",
            "Roc 0.8719046578421579\n",
            "Epoch 12 - loss is 0.674250 : \n",
            "Roc 0.8690696022727272\n",
            "Epoch 13 - loss is 0.672537 : \n",
            "Roc 0.8705836156031468\n",
            "Epoch 14 - loss is 0.659509 : \n",
            "Roc 0.8754388189935065\n",
            "Epoch 15 - loss is 0.656137 : \n",
            "Roc 0.878926151973027\n",
            "Epoch 16 - loss is 0.650943 : \n",
            "Roc 0.8771141943993507\n",
            "Epoch 17 - loss is 0.643752 : \n",
            "Roc 0.871982119443057\n",
            "Epoch 18 - loss is 0.649493 : \n",
            "Roc 0.8826205044955047\n",
            "Epoch 19 - loss is 0.640311 : \n",
            "Roc 0.8798551058316683\n",
            "Time mean 0.6714365601539611\n",
            "Time std 0.10387768323733898\n",
            "test\n",
            "Roc 0.8798551058316683\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977907 : \n",
            "Roc 0.7995973478796902\n",
            "Epoch 1 - loss is 0.814957 : \n",
            "Roc 0.8115385720370687\n",
            "Epoch 2 - loss is 0.790025 : \n",
            "Roc 0.7928839809784813\n",
            "Epoch 3 - loss is 0.776051 : \n",
            "Roc 0.8130516662981778\n",
            "Epoch 4 - loss is 0.802783 : \n",
            "Roc 0.7987032940403488\n",
            "Epoch 5 - loss is 0.770855 : \n",
            "Roc 0.8388373490288757\n",
            "Epoch 6 - loss is 0.759937 : \n",
            "Roc 0.7992907943161951\n",
            "Epoch 7 - loss is 0.758313 : \n",
            "Roc 0.8304422573834129\n",
            "Epoch 8 - loss is 0.757312 : \n",
            "Roc 0.8008047213204307\n",
            "Epoch 9 - loss is 0.718834 : \n",
            "Roc 0.8556029663975657\n",
            "Epoch 10 - loss is 0.695837 : \n",
            "Roc 0.8638179773418923\n",
            "Epoch 11 - loss is 0.686532 : \n",
            "Roc 0.8571683152898717\n",
            "Epoch 12 - loss is 0.677274 : \n",
            "Roc 0.8548405900651247\n",
            "Epoch 13 - loss is 0.668680 : \n",
            "Roc 0.8691897950660802\n",
            "Epoch 14 - loss is 0.664884 : \n",
            "Roc 0.8691489906528748\n",
            "Epoch 15 - loss is 0.656689 : \n",
            "Roc 0.873155317835151\n",
            "Epoch 16 - loss is 0.659458 : \n",
            "Roc 0.8763447240101911\n",
            "Epoch 17 - loss is 0.656690 : \n",
            "Roc 0.8747866616202433\n",
            "Epoch 18 - loss is 0.644504 : \n",
            "Roc 0.8831642656600468\n",
            "Epoch 19 - loss is 0.639967 : \n",
            "Roc 0.8821968263326699\n",
            "Time mean 0.6718567252159119\n",
            "Time std 0.09739501676499558\n",
            "test\n",
            "Roc 0.8821968263326699\n",
            "test\n",
            "Crossvalidation run 5\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.981590 : \n",
            "Roc 0.8208836147179943\n",
            "Epoch 1 - loss is 0.799526 : \n",
            "Roc 0.8100868518359274\n",
            "Epoch 2 - loss is 0.810012 : \n",
            "Roc 0.794502807476838\n",
            "Epoch 3 - loss is 0.791826 : \n",
            "Roc 0.8376932429462742\n",
            "Epoch 4 - loss is 0.769693 : \n",
            "Roc 0.8036499985668014\n",
            "Epoch 5 - loss is 0.785738 : \n",
            "Roc 0.8466318240159499\n",
            "Epoch 6 - loss is 0.772547 : \n",
            "Roc 0.7694158600943363\n",
            "Epoch 7 - loss is 0.772370 : \n",
            "Roc 0.8195815935894618\n",
            "Epoch 8 - loss is 0.768174 : \n",
            "Roc 0.831558157607259\n",
            "Epoch 9 - loss is 0.723052 : \n",
            "Roc 0.8745230633505636\n",
            "Epoch 10 - loss is 0.698177 : \n",
            "Roc 0.882375399305058\n",
            "Epoch 11 - loss is 0.691626 : \n",
            "Roc 0.8816064484382913\n",
            "Epoch 12 - loss is 0.682536 : \n",
            "Roc 0.8872069706003192\n",
            "Epoch 13 - loss is 0.675413 : \n",
            "Roc 0.8848852883754854\n",
            "Epoch 14 - loss is 0.667101 : \n",
            "Roc 0.8880522591987464\n",
            "Epoch 15 - loss is 0.660091 : \n",
            "Roc 0.8868768372809992\n",
            "Epoch 16 - loss is 0.659052 : \n",
            "Roc 0.8910022596764794\n",
            "Epoch 17 - loss is 0.654810 : \n",
            "Roc 0.8932991595086359\n",
            "Epoch 18 - loss is 0.648385 : \n",
            "Roc 0.8977398855989018\n",
            "Epoch 19 - loss is 0.643798 : \n",
            "Roc 0.8937111045820952\n",
            "Time mean 0.675475025177002\n",
            "Time std 0.1179666704867534\n",
            "test\n",
            "Roc 0.8937111045820952\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.034579 : \n",
            "Roc 0.7738328284254576\n",
            "Epoch 1 - loss is 0.817998 : \n",
            "Roc 0.7681079874393468\n",
            "Epoch 2 - loss is 0.800921 : \n",
            "Roc 0.8024618470963518\n",
            "Epoch 3 - loss is 0.771276 : \n",
            "Roc 0.8191758555621166\n",
            "Epoch 4 - loss is 0.767705 : \n",
            "Roc 0.8083960857872712\n",
            "Epoch 5 - loss is 0.765548 : \n",
            "Roc 0.7548117346152858\n",
            "Epoch 6 - loss is 0.752312 : \n",
            "Roc 0.8286330501784293\n",
            "Epoch 7 - loss is 0.755260 : \n",
            "Roc 0.8173420210649277\n",
            "Epoch 8 - loss is 0.762752 : \n",
            "Roc 0.8054927986444508\n",
            "Epoch 9 - loss is 0.703226 : \n",
            "Roc 0.8464346631023594\n",
            "Epoch 10 - loss is 0.689158 : \n",
            "Roc 0.8476834275756719\n",
            "Epoch 11 - loss is 0.680090 : \n",
            "Roc 0.8539707741778132\n",
            "Epoch 12 - loss is 0.667818 : \n",
            "Roc 0.8545470190110653\n",
            "Epoch 13 - loss is 0.662708 : \n",
            "Roc 0.8601494104772662\n",
            "Epoch 14 - loss is 0.657866 : \n",
            "Roc 0.8574725697029602\n",
            "Epoch 15 - loss is 0.654365 : \n",
            "Roc 0.8580047888693487\n",
            "Epoch 16 - loss is 0.644192 : \n",
            "Roc 0.8638317212651794\n",
            "Epoch 17 - loss is 0.644477 : \n",
            "Roc 0.8628095034466381\n",
            "Epoch 18 - loss is 0.645526 : \n",
            "Roc 0.8657253263715952\n",
            "Epoch 19 - loss is 0.638697 : \n",
            "Roc 0.8607798098893482\n",
            "Time mean 0.6528536200523376\n",
            "Time std 0.044234920995305294\n",
            "test\n",
            "Roc 0.8607798098893482\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.001416 : \n",
            "Roc 0.6822570973855232\n",
            "Epoch 1 - loss is 0.829238 : \n",
            "Roc 0.8032960700856477\n",
            "Epoch 2 - loss is 0.789308 : \n",
            "Roc 0.8045539044418748\n",
            "Epoch 3 - loss is 0.770544 : \n",
            "Roc 0.7823459561919046\n",
            "Epoch 4 - loss is 0.775914 : \n",
            "Roc 0.8322159875844976\n",
            "Epoch 5 - loss is 0.771130 : \n",
            "Roc 0.8130844259485217\n",
            "Epoch 6 - loss is 0.754824 : \n",
            "Roc 0.8113180932205163\n",
            "Epoch 7 - loss is 0.756059 : \n",
            "Roc 0.816906438962884\n",
            "Epoch 8 - loss is 0.757470 : \n",
            "Roc 0.8362033148843353\n",
            "Epoch 9 - loss is 0.710083 : \n",
            "Roc 0.8567517474043373\n",
            "Epoch 10 - loss is 0.680130 : \n",
            "Roc 0.8596389452301787\n",
            "Epoch 11 - loss is 0.675685 : \n",
            "Roc 0.8661179716222526\n",
            "Epoch 12 - loss is 0.669690 : \n",
            "Roc 0.8617899332921665\n",
            "Epoch 13 - loss is 0.655617 : \n",
            "Roc 0.8671598891614322\n",
            "Epoch 14 - loss is 0.651521 : \n",
            "Roc 0.8696350378016129\n",
            "Epoch 15 - loss is 0.656394 : \n",
            "Roc 0.8701659839129606\n",
            "Epoch 16 - loss is 0.646731 : \n",
            "Roc 0.8688624931372695\n",
            "Epoch 17 - loss is 0.643681 : \n",
            "Roc 0.8727543224263952\n",
            "Epoch 18 - loss is 0.643303 : \n",
            "Roc 0.8782443166318346\n",
            "Epoch 19 - loss is 0.636027 : \n",
            "Roc 0.8774297300145683\n",
            "Time mean 0.6693512082099915\n",
            "Time std 0.09722226979965624\n",
            "test\n",
            "Roc 0.8774297300145683\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.006414 : \n",
            "Roc 0.8012049530691937\n",
            "Epoch 1 - loss is 0.778824 : \n",
            "Roc 0.8205110894126397\n",
            "Epoch 2 - loss is 0.789561 : \n",
            "Roc 0.797394355567191\n",
            "Epoch 3 - loss is 0.800940 : \n",
            "Roc 0.7963931597927324\n",
            "Epoch 4 - loss is 0.766998 : \n",
            "Roc 0.8139714767303547\n",
            "Epoch 5 - loss is 0.771815 : \n",
            "Roc 0.8025768310640349\n",
            "Epoch 6 - loss is 0.779724 : \n",
            "Roc 0.8169875446056892\n",
            "Epoch 7 - loss is 0.761737 : \n",
            "Roc 0.7895066467292305\n",
            "Epoch 8 - loss is 0.764768 : \n",
            "Roc 0.8059402906808996\n",
            "Epoch 9 - loss is 0.715020 : \n",
            "Roc 0.8474156216415523\n",
            "Epoch 10 - loss is 0.695331 : \n",
            "Roc 0.8530361939937688\n",
            "Epoch 11 - loss is 0.683658 : \n",
            "Roc 0.8533337617995266\n",
            "Epoch 12 - loss is 0.676702 : \n",
            "Roc 0.860695616830152\n",
            "Epoch 13 - loss is 0.673145 : \n",
            "Roc 0.8652358843974626\n",
            "Epoch 14 - loss is 0.660076 : \n",
            "Roc 0.8681457201730532\n",
            "Epoch 15 - loss is 0.659602 : \n",
            "Roc 0.8705285228765568\n",
            "Epoch 16 - loss is 0.653188 : \n",
            "Roc 0.8698264672608589\n",
            "Epoch 17 - loss is 0.656880 : \n",
            "Roc 0.8715492747914864\n",
            "Epoch 18 - loss is 0.647386 : \n",
            "Roc 0.8744522315226904\n",
            "Epoch 19 - loss is 0.646419 : \n",
            "Roc 0.8717247886954108\n",
            "Time mean 0.6680746436119079\n",
            "Time std 0.06708906444803706\n",
            "test\n",
            "Roc 0.8717247886954108\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.994364 : \n",
            "Roc 0.8066620870398711\n",
            "Epoch 1 - loss is 0.785487 : \n",
            "Roc 0.7846251729815303\n",
            "Epoch 2 - loss is 0.803673 : \n",
            "Roc 0.7750435152465037\n",
            "Epoch 3 - loss is 0.770824 : \n",
            "Roc 0.8025744184469867\n",
            "Epoch 4 - loss is 0.780428 : \n",
            "Roc 0.8096785059006654\n",
            "Epoch 5 - loss is 0.793764 : \n",
            "Roc 0.8237950491967972\n",
            "Epoch 6 - loss is 0.777567 : \n",
            "Roc 0.8236916949913848\n",
            "Epoch 7 - loss is 0.773881 : \n",
            "Roc 0.8109056661354377\n",
            "Epoch 8 - loss is 0.765232 : \n",
            "Roc 0.8039110872491194\n",
            "Epoch 9 - loss is 0.726085 : \n",
            "Roc 0.8581084500171052\n",
            "Epoch 10 - loss is 0.691357 : \n",
            "Roc 0.8609080985737314\n",
            "Epoch 11 - loss is 0.688998 : \n",
            "Roc 0.8684157363322855\n",
            "Epoch 12 - loss is 0.678875 : \n",
            "Roc 0.8697610993917733\n",
            "Epoch 13 - loss is 0.675013 : \n",
            "Roc 0.8777072598611928\n",
            "Epoch 14 - loss is 0.674254 : \n",
            "Roc 0.8745615017112058\n",
            "Epoch 15 - loss is 0.659801 : \n",
            "Roc 0.8744165322964308\n",
            "Epoch 16 - loss is 0.653715 : \n",
            "Roc 0.8781687393190073\n",
            "Epoch 17 - loss is 0.648342 : \n",
            "Roc 0.8820879933087433\n",
            "Epoch 18 - loss is 0.646156 : \n",
            "Roc 0.8793779249093598\n",
            "Epoch 19 - loss is 0.640761 : \n",
            "Roc 0.8867662852720648\n",
            "Time mean 0.6787268996238709\n",
            "Time std 0.10695526239297658\n",
            "test\n",
            "Roc 0.8867662852720648\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977450 : \n",
            "Roc 0.8132526774101481\n",
            "Epoch 1 - loss is 0.796628 : \n",
            "Roc 0.8204105713404577\n",
            "Epoch 2 - loss is 0.791446 : \n",
            "Roc 0.820947873108669\n",
            "Epoch 3 - loss is 0.781716 : \n",
            "Roc 0.8314266942245327\n",
            "Epoch 4 - loss is 0.776041 : \n",
            "Roc 0.8131860175116319\n",
            "Epoch 5 - loss is 0.762335 : \n",
            "Roc 0.8313283325639351\n",
            "Epoch 6 - loss is 0.762771 : \n",
            "Roc 0.8210961339174375\n",
            "Epoch 7 - loss is 0.765817 : \n",
            "Roc 0.8258500573696539\n",
            "Epoch 8 - loss is 0.762437 : \n",
            "Roc 0.8308880515962939\n",
            "Epoch 9 - loss is 0.715898 : \n",
            "Roc 0.8575359206823828\n",
            "Epoch 10 - loss is 0.693248 : \n",
            "Roc 0.8664801274966335\n",
            "Epoch 11 - loss is 0.678459 : \n",
            "Roc 0.8586885814275561\n",
            "Epoch 12 - loss is 0.676526 : \n",
            "Roc 0.8686300049994924\n",
            "Epoch 13 - loss is 0.666123 : \n",
            "Roc 0.8628800140981854\n",
            "Epoch 14 - loss is 0.657870 : \n",
            "Roc 0.8696258808971503\n",
            "Epoch 15 - loss is 0.660131 : \n",
            "Roc 0.8740375977151745\n",
            "Epoch 16 - loss is 0.653321 : \n",
            "Roc 0.8805814735285498\n",
            "Epoch 17 - loss is 0.640492 : \n",
            "Roc 0.876210652941368\n",
            "Epoch 18 - loss is 0.636317 : \n",
            "Roc 0.8749399007380477\n",
            "Epoch 19 - loss is 0.639254 : \n",
            "Roc 0.8789766556269191\n",
            "Time mean 0.6563409924507141\n",
            "Time std 0.04876165494154297\n",
            "test\n",
            "Roc 0.8789766556269191\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.965941 : \n",
            "Roc 0.817354010383105\n",
            "Epoch 1 - loss is 0.789185 : \n",
            "Roc 0.824481351343684\n",
            "Epoch 2 - loss is 0.796416 : \n",
            "Roc 0.7994501452905202\n",
            "Epoch 3 - loss is 0.766573 : \n",
            "Roc 0.7962380495000952\n",
            "Epoch 4 - loss is 0.796239 : \n",
            "Roc 0.7543133249847009\n",
            "Epoch 5 - loss is 0.798982 : \n",
            "Roc 0.7986218677430729\n",
            "Epoch 6 - loss is 0.774961 : \n",
            "Roc 0.8201072353463265\n",
            "Epoch 7 - loss is 0.768116 : \n",
            "Roc 0.8250457958962014\n",
            "Epoch 8 - loss is 0.768098 : \n",
            "Roc 0.8200892006792382\n",
            "Epoch 9 - loss is 0.721068 : \n",
            "Roc 0.8514616185809757\n",
            "Epoch 10 - loss is 0.694686 : \n",
            "Roc 0.8622570892452593\n",
            "Epoch 11 - loss is 0.690901 : \n",
            "Roc 0.8635170843009237\n",
            "Epoch 12 - loss is 0.682471 : \n",
            "Roc 0.865494210669228\n",
            "Epoch 13 - loss is 0.677506 : \n",
            "Roc 0.8662330241097156\n",
            "Epoch 14 - loss is 0.666070 : \n",
            "Roc 0.8756479308765658\n",
            "Epoch 15 - loss is 0.662082 : \n",
            "Roc 0.8744967517335571\n",
            "Epoch 16 - loss is 0.656565 : \n",
            "Roc 0.874800098886714\n",
            "Epoch 17 - loss is 0.650748 : \n",
            "Roc 0.8751062829537949\n",
            "Epoch 18 - loss is 0.649489 : \n",
            "Roc 0.8811799130283245\n",
            "Epoch 19 - loss is 0.645989 : \n",
            "Roc 0.8769962755372913\n",
            "Time mean 0.7071672916412354\n",
            "Time std 0.1481294902340999\n",
            "test\n",
            "Roc 0.8769962755372913\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.965617 : \n",
            "Roc 0.8352895555591314\n",
            "Epoch 1 - loss is 0.786084 : \n",
            "Roc 0.8308725032409641\n",
            "Epoch 2 - loss is 0.810141 : \n",
            "Roc 0.7783823281798439\n",
            "Epoch 3 - loss is 0.778310 : \n",
            "Roc 0.8367123221002153\n",
            "Epoch 4 - loss is 0.777955 : \n",
            "Roc 0.8162270350761994\n",
            "Epoch 5 - loss is 0.764852 : \n",
            "Roc 0.8019132186982529\n",
            "Epoch 6 - loss is 0.761042 : \n",
            "Roc 0.7917687951384951\n",
            "Epoch 7 - loss is 0.762157 : \n",
            "Roc 0.8236950875894332\n",
            "Epoch 8 - loss is 0.766344 : \n",
            "Roc 0.8177738276109335\n",
            "Epoch 9 - loss is 0.716732 : \n",
            "Roc 0.8626461181237692\n",
            "Epoch 10 - loss is 0.691302 : \n",
            "Roc 0.8642154786794999\n",
            "Epoch 11 - loss is 0.680051 : \n",
            "Roc 0.8694386999563947\n",
            "Epoch 12 - loss is 0.672921 : \n",
            "Roc 0.8677360899549542\n",
            "Epoch 13 - loss is 0.666786 : \n",
            "Roc 0.8765818710891092\n",
            "Epoch 14 - loss is 0.664228 : \n",
            "Roc 0.8792267450280687\n",
            "Epoch 15 - loss is 0.661397 : \n",
            "Roc 0.8786215295780408\n",
            "Epoch 16 - loss is 0.662685 : \n",
            "Roc 0.880824757158282\n",
            "Epoch 17 - loss is 0.647168 : \n",
            "Roc 0.8762936872731668\n",
            "Epoch 18 - loss is 0.645792 : \n",
            "Roc 0.8793278105134009\n",
            "Epoch 19 - loss is 0.642841 : \n",
            "Roc 0.8839220708573061\n",
            "Time mean 0.6821683883666992\n",
            "Time std 0.10707260899741167\n",
            "test\n",
            "Roc 0.8839220708573061\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.960333 : \n",
            "Roc 0.8198496815684316\n",
            "Epoch 1 - loss is 0.790664 : \n",
            "Roc 0.8035052837787213\n",
            "Epoch 2 - loss is 0.784521 : \n",
            "Roc 0.8038775677447553\n",
            "Epoch 3 - loss is 0.788008 : \n",
            "Roc 0.829604379995005\n",
            "Epoch 4 - loss is 0.772583 : \n",
            "Roc 0.8146425839785214\n",
            "Epoch 5 - loss is 0.767224 : \n",
            "Roc 0.8362379807692307\n",
            "Epoch 6 - loss is 0.775054 : \n",
            "Roc 0.8210980425824177\n",
            "Epoch 7 - loss is 0.767650 : \n",
            "Roc 0.8357103833666334\n",
            "Epoch 8 - loss is 0.777122 : \n",
            "Roc 0.7972420157967032\n",
            "Epoch 9 - loss is 0.728468 : \n",
            "Roc 0.8626345334353147\n",
            "Epoch 10 - loss is 0.704502 : \n",
            "Roc 0.8613136863136863\n",
            "Epoch 11 - loss is 0.689853 : \n",
            "Roc 0.8646979973151848\n",
            "Epoch 12 - loss is 0.687805 : \n",
            "Roc 0.8665986357392608\n",
            "Epoch 13 - loss is 0.679674 : \n",
            "Roc 0.8721655297827172\n",
            "Epoch 14 - loss is 0.673563 : \n",
            "Roc 0.8721955778596404\n",
            "Epoch 15 - loss is 0.664236 : \n",
            "Roc 0.8759170516983018\n",
            "Epoch 16 - loss is 0.658170 : \n",
            "Roc 0.8794357985764236\n",
            "Epoch 17 - loss is 0.649145 : \n",
            "Roc 0.8814999453671328\n",
            "Epoch 18 - loss is 0.644221 : \n",
            "Roc 0.8811778065684316\n",
            "Epoch 19 - loss is 0.640754 : \n",
            "Roc 0.8797778393481518\n",
            "Time mean 0.6910595893859863\n",
            "Time std 0.10255416769092174\n",
            "test\n",
            "Roc 0.8797778393481518\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.959518 : \n",
            "Roc 0.8208557185095064\n",
            "Epoch 1 - loss is 0.773664 : \n",
            "Roc 0.809905666858614\n",
            "Epoch 2 - loss is 0.789653 : \n",
            "Roc 0.7986926765654841\n",
            "Epoch 3 - loss is 0.783045 : \n",
            "Roc 0.8176450773847368\n",
            "Epoch 4 - loss is 0.780020 : \n",
            "Roc 0.8062479467677284\n",
            "Epoch 5 - loss is 0.776048 : \n",
            "Roc 0.7782082365789913\n",
            "Epoch 6 - loss is 0.779824 : \n",
            "Roc 0.7901395719117409\n",
            "Epoch 7 - loss is 0.756127 : \n",
            "Roc 0.8179188416876205\n",
            "Epoch 8 - loss is 0.769877 : \n",
            "Roc 0.81041166240094\n",
            "Epoch 9 - loss is 0.725956 : \n",
            "Roc 0.8610380725993785\n",
            "Epoch 10 - loss is 0.695732 : \n",
            "Roc 0.8678790573847452\n",
            "Epoch 11 - loss is 0.685903 : \n",
            "Roc 0.8701341257716926\n",
            "Epoch 12 - loss is 0.669870 : \n",
            "Roc 0.869906370526556\n",
            "Epoch 13 - loss is 0.670770 : \n",
            "Roc 0.8772959248465982\n",
            "Epoch 14 - loss is 0.668299 : \n",
            "Roc 0.8739310180576183\n",
            "Epoch 15 - loss is 0.659764 : \n",
            "Roc 0.8788845905464502\n",
            "Epoch 16 - loss is 0.646993 : \n",
            "Roc 0.8804753381041188\n",
            "Epoch 17 - loss is 0.657662 : \n",
            "Roc 0.8787705047381003\n",
            "Epoch 18 - loss is 0.646652 : \n",
            "Roc 0.8771879805188073\n",
            "Epoch 19 - loss is 0.637591 : \n",
            "Roc 0.8745834722973425\n",
            "Time mean 0.6706069707870483\n",
            "Time std 0.07067409911787854\n",
            "test\n",
            "Roc 0.8745834722973425\n",
            "test\n",
            "Crossvalidation run 6\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.995341 : \n",
            "Roc 0.8167814817999701\n",
            "Epoch 1 - loss is 0.799860 : \n",
            "Roc 0.845261526897953\n",
            "Epoch 2 - loss is 0.793889 : \n",
            "Roc 0.8435386032046321\n",
            "Epoch 3 - loss is 0.792534 : \n",
            "Roc 0.781636665042375\n",
            "Epoch 4 - loss is 0.792124 : \n",
            "Roc 0.8513079330728096\n",
            "Epoch 5 - loss is 0.779362 : \n",
            "Roc 0.8126327699270344\n",
            "Epoch 6 - loss is 0.772165 : \n",
            "Roc 0.8240647184720192\n",
            "Epoch 7 - loss is 0.769126 : \n",
            "Roc 0.7901361299815596\n",
            "Epoch 8 - loss is 0.760492 : \n",
            "Roc 0.8174156721860738\n",
            "Epoch 9 - loss is 0.717806 : \n",
            "Roc 0.8756411573238042\n",
            "Epoch 10 - loss is 0.696256 : \n",
            "Roc 0.8705292165499406\n",
            "Epoch 11 - loss is 0.686871 : \n",
            "Roc 0.8773052601573972\n",
            "Epoch 12 - loss is 0.680539 : \n",
            "Roc 0.8754253813104532\n",
            "Epoch 13 - loss is 0.673609 : \n",
            "Roc 0.8835532098871594\n",
            "Epoch 14 - loss is 0.662369 : \n",
            "Roc 0.8870716129535675\n",
            "Epoch 15 - loss is 0.674036 : \n",
            "Roc 0.8893887169050554\n",
            "Epoch 16 - loss is 0.665030 : \n",
            "Roc 0.89387442950733\n",
            "Epoch 17 - loss is 0.651640 : \n",
            "Roc 0.8947739606921392\n",
            "Epoch 18 - loss is 0.648353 : \n",
            "Roc 0.8962577193669721\n",
            "Epoch 19 - loss is 0.643598 : \n",
            "Roc 0.9011972781965902\n",
            "Time mean 0.6715410470962524\n",
            "Time std 0.07333972517906971\n",
            "test\n",
            "Roc 0.9011972781965902\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.000358 : \n",
            "Roc 0.8082440518215194\n",
            "Epoch 1 - loss is 0.796126 : \n",
            "Roc 0.7552303294523889\n",
            "Epoch 2 - loss is 0.795148 : \n",
            "Roc 0.8264504200790737\n",
            "Epoch 3 - loss is 0.774918 : \n",
            "Roc 0.7245062305845806\n",
            "Epoch 4 - loss is 0.770980 : \n",
            "Roc 0.8130812591461067\n",
            "Epoch 5 - loss is 0.776571 : \n",
            "Roc 0.8152654938255757\n",
            "Epoch 6 - loss is 0.762774 : \n",
            "Roc 0.8021719195270982\n",
            "Epoch 7 - loss is 0.755219 : \n",
            "Roc 0.8084295813971399\n",
            "Epoch 8 - loss is 0.758193 : \n",
            "Roc 0.7379750760570973\n",
            "Epoch 9 - loss is 0.714365 : \n",
            "Roc 0.8402646514210161\n",
            "Epoch 10 - loss is 0.684806 : \n",
            "Roc 0.8457005677004441\n",
            "Epoch 11 - loss is 0.675154 : \n",
            "Roc 0.8547112879001821\n",
            "Epoch 12 - loss is 0.667627 : \n",
            "Roc 0.8539693701702138\n",
            "Epoch 13 - loss is 0.658620 : \n",
            "Roc 0.8545241537444481\n",
            "Epoch 14 - loss is 0.653959 : \n",
            "Roc 0.8584870654797053\n",
            "Epoch 15 - loss is 0.646800 : \n",
            "Roc 0.8639717208800801\n",
            "Epoch 16 - loss is 0.646012 : \n",
            "Roc 0.8685031554067931\n",
            "Epoch 17 - loss is 0.645139 : \n",
            "Roc 0.8606067158096069\n",
            "Epoch 18 - loss is 0.639982 : \n",
            "Roc 0.8647107303085929\n",
            "Epoch 19 - loss is 0.631206 : \n",
            "Roc 0.866708933981156\n",
            "Time mean 0.6607588171958924\n",
            "Time std 0.051032752768827856\n",
            "test\n",
            "Roc 0.866708933981156\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.974054 : \n",
            "Roc 0.8152130613884802\n",
            "Epoch 1 - loss is 0.810753 : \n",
            "Roc 0.8259601069425532\n",
            "Epoch 2 - loss is 0.795946 : \n",
            "Roc 0.8033424874454359\n",
            "Epoch 3 - loss is 0.780274 : \n",
            "Roc 0.7899888408101428\n",
            "Epoch 4 - loss is 0.776904 : \n",
            "Roc 0.8004720303067312\n",
            "Epoch 5 - loss is 0.768971 : \n",
            "Roc 0.833157080530695\n",
            "Epoch 6 - loss is 0.761172 : \n",
            "Roc 0.7921701713675657\n",
            "Epoch 7 - loss is 0.758575 : \n",
            "Roc 0.7868072541963005\n",
            "Epoch 8 - loss is 0.769320 : \n",
            "Roc 0.7988774798569585\n",
            "Epoch 9 - loss is 0.708188 : \n",
            "Roc 0.8531821763425936\n",
            "Epoch 10 - loss is 0.693914 : \n",
            "Roc 0.8591201741640213\n",
            "Epoch 11 - loss is 0.677829 : \n",
            "Roc 0.8575901134067412\n",
            "Epoch 12 - loss is 0.677861 : \n",
            "Roc 0.8620992555340337\n",
            "Epoch 13 - loss is 0.669328 : \n",
            "Roc 0.8666036417842986\n",
            "Epoch 14 - loss is 0.666576 : \n",
            "Roc 0.8661750421465824\n",
            "Epoch 15 - loss is 0.667068 : \n",
            "Roc 0.8673349054360436\n",
            "Epoch 16 - loss is 0.656407 : \n",
            "Roc 0.8757866696191152\n",
            "Epoch 17 - loss is 0.650709 : \n",
            "Roc 0.8722220446694798\n",
            "Epoch 18 - loss is 0.646670 : \n",
            "Roc 0.876236480468374\n",
            "Epoch 19 - loss is 0.638510 : \n",
            "Roc 0.8778978985491531\n",
            "Time mean 0.6831258296966553\n",
            "Time std 0.10048082606738626\n",
            "test\n",
            "Roc 0.8778978985491531\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.000053 : \n",
            "Roc 0.8073538360303119\n",
            "Epoch 1 - loss is 0.798228 : \n",
            "Roc 0.7954912188015684\n",
            "Epoch 2 - loss is 0.791337 : \n",
            "Roc 0.8180766938762354\n",
            "Epoch 3 - loss is 0.784187 : \n",
            "Roc 0.7997674293364906\n",
            "Epoch 4 - loss is 0.785234 : \n",
            "Roc 0.8203479577886114\n",
            "Epoch 5 - loss is 0.772343 : \n",
            "Roc 0.7903111018342284\n",
            "Epoch 6 - loss is 0.746492 : \n",
            "Roc 0.8063334772036331\n",
            "Epoch 7 - loss is 0.751087 : \n",
            "Roc 0.7682359535809947\n",
            "Epoch 8 - loss is 0.757648 : \n",
            "Roc 0.8254775629157399\n",
            "Epoch 9 - loss is 0.707651 : \n",
            "Roc 0.854338495368241\n",
            "Epoch 10 - loss is 0.682442 : \n",
            "Roc 0.8576780748738777\n",
            "Epoch 11 - loss is 0.675211 : \n",
            "Roc 0.8586872306854123\n",
            "Epoch 12 - loss is 0.665343 : \n",
            "Roc 0.8642936518606046\n",
            "Epoch 13 - loss is 0.655319 : \n",
            "Roc 0.8583926110415344\n",
            "Epoch 14 - loss is 0.653705 : \n",
            "Roc 0.8705069030227699\n",
            "Epoch 15 - loss is 0.651186 : \n",
            "Roc 0.862237603765471\n",
            "Epoch 16 - loss is 0.649275 : \n",
            "Roc 0.8729189908009489\n",
            "Epoch 17 - loss is 0.646543 : \n",
            "Roc 0.8752820408198563\n",
            "Epoch 18 - loss is 0.634751 : \n",
            "Roc 0.8653146985917219\n",
            "Epoch 19 - loss is 0.634797 : \n",
            "Roc 0.8691068209459432\n",
            "Time mean 0.663078224658966\n",
            "Time std 0.05593751195637643\n",
            "test\n",
            "Roc 0.8691068209459432\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.958427 : \n",
            "Roc 0.794679348950564\n",
            "Epoch 1 - loss is 0.825656 : \n",
            "Roc 0.8102853455263943\n",
            "Epoch 2 - loss is 0.790943 : \n",
            "Roc 0.7991009161011979\n",
            "Epoch 3 - loss is 0.788679 : \n",
            "Roc 0.7977738207509846\n",
            "Epoch 4 - loss is 0.784573 : \n",
            "Roc 0.7983365052719438\n",
            "Epoch 5 - loss is 0.765671 : \n",
            "Roc 0.8127775690504757\n",
            "Epoch 6 - loss is 0.767261 : \n",
            "Roc 0.8312437809195632\n",
            "Epoch 7 - loss is 0.770849 : \n",
            "Roc 0.8093571114433815\n",
            "Epoch 8 - loss is 0.761346 : \n",
            "Roc 0.8335306636688087\n",
            "Epoch 9 - loss is 0.710915 : \n",
            "Roc 0.8553370333748117\n",
            "Epoch 10 - loss is 0.695432 : \n",
            "Roc 0.8622115533590019\n",
            "Epoch 11 - loss is 0.682049 : \n",
            "Roc 0.8678823582967929\n",
            "Epoch 12 - loss is 0.671310 : \n",
            "Roc 0.8699390874468405\n",
            "Epoch 13 - loss is 0.667280 : \n",
            "Roc 0.8713637733936579\n",
            "Epoch 14 - loss is 0.669444 : \n",
            "Roc 0.8741019760190891\n",
            "Epoch 15 - loss is 0.663419 : \n",
            "Roc 0.8808297681993763\n",
            "Epoch 16 - loss is 0.649160 : \n",
            "Roc 0.8757726410672562\n",
            "Epoch 17 - loss is 0.643371 : \n",
            "Roc 0.8850840930062807\n",
            "Epoch 18 - loss is 0.637653 : \n",
            "Roc 0.8861838677175582\n",
            "Epoch 19 - loss is 0.633195 : \n",
            "Roc 0.8846140169793962\n",
            "Time mean 0.655909514427185\n",
            "Time std 0.08401978165434029\n",
            "test\n",
            "Roc 0.8846140169793962\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.991281 : \n",
            "Roc 0.8342235366907191\n",
            "Epoch 1 - loss is 0.806879 : \n",
            "Roc 0.7340084244320021\n",
            "Epoch 2 - loss is 0.804408 : \n",
            "Roc 0.8087590723547227\n",
            "Epoch 3 - loss is 0.783360 : \n",
            "Roc 0.7203305028416656\n",
            "Epoch 4 - loss is 0.790154 : \n",
            "Roc 0.7664562793432851\n",
            "Epoch 5 - loss is 0.777908 : \n",
            "Roc 0.8282134189440536\n",
            "Epoch 6 - loss is 0.757906 : \n",
            "Roc 0.8255870572528077\n",
            "Epoch 7 - loss is 0.762802 : \n",
            "Roc 0.8230179694398845\n",
            "Epoch 8 - loss is 0.755558 : \n",
            "Roc 0.8071333753469475\n",
            "Epoch 9 - loss is 0.717402 : \n",
            "Roc 0.8581034875769318\n",
            "Epoch 10 - loss is 0.688122 : \n",
            "Roc 0.8656266700890523\n",
            "Epoch 11 - loss is 0.687213 : \n",
            "Roc 0.8627575169570906\n",
            "Epoch 12 - loss is 0.669772 : \n",
            "Roc 0.868586905927176\n",
            "Epoch 13 - loss is 0.671246 : \n",
            "Roc 0.8698537313261296\n",
            "Epoch 14 - loss is 0.664454 : \n",
            "Roc 0.866852024602866\n",
            "Epoch 15 - loss is 0.656033 : \n",
            "Roc 0.8679563186113287\n",
            "Epoch 16 - loss is 0.654408 : \n",
            "Roc 0.8708242266589791\n",
            "Epoch 17 - loss is 0.645753 : \n",
            "Roc 0.8796336770404537\n",
            "Epoch 18 - loss is 0.655542 : \n",
            "Roc 0.8743208065081515\n",
            "Epoch 19 - loss is 0.642335 : \n",
            "Roc 0.8776266011305366\n",
            "Time mean 0.6600527882575988\n",
            "Time std 0.05198970643454168\n",
            "test\n",
            "Roc 0.8776266011305366\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.973661 : \n",
            "Roc 0.7848249421472197\n",
            "Epoch 1 - loss is 0.791029 : \n",
            "Roc 0.8002079052632859\n",
            "Epoch 2 - loss is 0.794029 : \n",
            "Roc 0.8071155880314654\n",
            "Epoch 3 - loss is 0.784832 : \n",
            "Roc 0.8153908659477116\n",
            "Epoch 4 - loss is 0.777227 : \n",
            "Roc 0.8107688239370691\n",
            "Epoch 5 - loss is 0.767785 : \n",
            "Roc 0.808024211033974\n",
            "Epoch 6 - loss is 0.761006 : \n",
            "Roc 0.8232817420272588\n",
            "Epoch 7 - loss is 0.761065 : \n",
            "Roc 0.788994395068633\n",
            "Epoch 8 - loss is 0.759585 : \n",
            "Roc 0.822373828253231\n",
            "Epoch 9 - loss is 0.714541 : \n",
            "Roc 0.8613520123851557\n",
            "Epoch 10 - loss is 0.690929 : \n",
            "Roc 0.8559400924833938\n",
            "Epoch 11 - loss is 0.682638 : \n",
            "Roc 0.8630129241692909\n",
            "Epoch 12 - loss is 0.673532 : \n",
            "Roc 0.8722606556513353\n",
            "Epoch 13 - loss is 0.667700 : \n",
            "Roc 0.8735175098380122\n",
            "Epoch 14 - loss is 0.661012 : \n",
            "Roc 0.8704924477298611\n",
            "Epoch 15 - loss is 0.657270 : \n",
            "Roc 0.8783934556448509\n",
            "Epoch 16 - loss is 0.644070 : \n",
            "Roc 0.8789105845258505\n",
            "Epoch 17 - loss is 0.650269 : \n",
            "Roc 0.8773873643853826\n",
            "Epoch 18 - loss is 0.639354 : \n",
            "Roc 0.8713960048146482\n",
            "Epoch 19 - loss is 0.636992 : \n",
            "Roc 0.8817930917093216\n",
            "Time mean 0.6702039361000061\n",
            "Time std 0.11856598362927073\n",
            "test\n",
            "Roc 0.8817930917093216\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.952280 : \n",
            "Roc 0.7940380587106086\n",
            "Epoch 1 - loss is 0.806337 : \n",
            "Roc 0.815759190188759\n",
            "Epoch 2 - loss is 0.775447 : \n",
            "Roc 0.8215603490468248\n",
            "Epoch 3 - loss is 0.783954 : \n",
            "Roc 0.782632082777146\n",
            "Epoch 4 - loss is 0.765123 : \n",
            "Roc 0.8322466013541597\n",
            "Epoch 5 - loss is 0.766117 : \n",
            "Roc 0.8228759665490906\n",
            "Epoch 6 - loss is 0.771102 : \n",
            "Roc 0.8094222861366019\n",
            "Epoch 7 - loss is 0.765436 : \n",
            "Roc 0.8173383629275314\n",
            "Epoch 8 - loss is 0.756484 : \n",
            "Roc 0.7989040184029456\n",
            "Epoch 9 - loss is 0.707437 : \n",
            "Roc 0.8659237797471047\n",
            "Epoch 10 - loss is 0.687190 : \n",
            "Roc 0.8697000965126324\n",
            "Epoch 11 - loss is 0.687674 : \n",
            "Roc 0.8651927721104788\n",
            "Epoch 12 - loss is 0.683458 : \n",
            "Roc 0.8746801718937474\n",
            "Epoch 13 - loss is 0.671186 : \n",
            "Roc 0.8663432505721484\n",
            "Epoch 14 - loss is 0.661567 : \n",
            "Roc 0.8728087923439853\n",
            "Epoch 15 - loss is 0.653412 : \n",
            "Roc 0.8834964183569461\n",
            "Epoch 16 - loss is 0.647902 : \n",
            "Roc 0.8813873877339077\n",
            "Epoch 17 - loss is 0.647304 : \n",
            "Roc 0.8813130113864496\n",
            "Epoch 18 - loss is 0.641558 : \n",
            "Roc 0.8782463116985164\n",
            "Epoch 19 - loss is 0.640121 : \n",
            "Roc 0.8854015518163724\n",
            "Time mean 0.6642911314964295\n",
            "Time std 0.0722310842072839\n",
            "test\n",
            "Roc 0.8854015518163724\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.960878 : \n",
            "Roc 0.8130109148663837\n",
            "Epoch 1 - loss is 0.813572 : \n",
            "Roc 0.7995124016608393\n",
            "Epoch 2 - loss is 0.787838 : \n",
            "Roc 0.8158232392607393\n",
            "Epoch 3 - loss is 0.791982 : \n",
            "Roc 0.825320772977023\n",
            "Epoch 4 - loss is 0.790413 : \n",
            "Roc 0.8336821381743257\n",
            "Epoch 5 - loss is 0.764137 : \n",
            "Roc 0.8248790272227773\n",
            "Epoch 6 - loss is 0.764116 : \n",
            "Roc 0.83269396619006\n",
            "Epoch 7 - loss is 0.765874 : \n",
            "Roc 0.8212199909465535\n",
            "Epoch 8 - loss is 0.758322 : \n",
            "Roc 0.8423000827297702\n",
            "Epoch 9 - loss is 0.713563 : \n",
            "Roc 0.8652591549075925\n",
            "Epoch 10 - loss is 0.691948 : \n",
            "Roc 0.8644628808691309\n",
            "Epoch 11 - loss is 0.687943 : \n",
            "Roc 0.86802465113012\n",
            "Epoch 12 - loss is 0.677435 : \n",
            "Roc 0.8704422530594406\n",
            "Epoch 13 - loss is 0.662557 : \n",
            "Roc 0.87302599744006\n",
            "Epoch 14 - loss is 0.661279 : \n",
            "Roc 0.8781360241321179\n",
            "Epoch 15 - loss is 0.654687 : \n",
            "Roc 0.8819024725274726\n",
            "Epoch 16 - loss is 0.648839 : \n",
            "Roc 0.8708648773101898\n",
            "Epoch 17 - loss is 0.640101 : \n",
            "Roc 0.8794112137862137\n",
            "Epoch 18 - loss is 0.638025 : \n",
            "Roc 0.8826473331356144\n",
            "Epoch 19 - loss is 0.635368 : \n",
            "Roc 0.8775382820304696\n",
            "Time mean 0.658037292957306\n",
            "Time std 0.06832206749977333\n",
            "test\n",
            "Roc 0.8775382820304696\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.958095 : \n",
            "Roc 0.8169786746976414\n",
            "Epoch 1 - loss is 0.809927 : \n",
            "Roc 0.8051632988452768\n",
            "Epoch 2 - loss is 0.792791 : \n",
            "Roc 0.7880162851245847\n",
            "Epoch 3 - loss is 0.786483 : \n",
            "Roc 0.8064915241322713\n",
            "Epoch 4 - loss is 0.770338 : \n",
            "Roc 0.8243544887560941\n",
            "Epoch 5 - loss is 0.775180 : \n",
            "Roc 0.8341165282439406\n",
            "Epoch 6 - loss is 0.752915 : \n",
            "Roc 0.8200317191856938\n",
            "Epoch 7 - loss is 0.761909 : \n",
            "Roc 0.8163568237678213\n",
            "Epoch 8 - loss is 0.764160 : \n",
            "Roc 0.8094339178773711\n",
            "Epoch 9 - loss is 0.722422 : \n",
            "Roc 0.8600999874672159\n",
            "Epoch 10 - loss is 0.697939 : \n",
            "Roc 0.8624797591373781\n",
            "Epoch 11 - loss is 0.689296 : \n",
            "Roc 0.8652559165358219\n",
            "Epoch 12 - loss is 0.679110 : \n",
            "Roc 0.8664332071311126\n",
            "Epoch 13 - loss is 0.671932 : \n",
            "Roc 0.8707139231736174\n",
            "Epoch 14 - loss is 0.667359 : \n",
            "Roc 0.8688588837661307\n",
            "Epoch 15 - loss is 0.661726 : \n",
            "Roc 0.8681153482469299\n",
            "Epoch 16 - loss is 0.653368 : \n",
            "Roc 0.8740236607304573\n",
            "Epoch 17 - loss is 0.647487 : \n",
            "Roc 0.8715410452841549\n",
            "Epoch 18 - loss is 0.645258 : \n",
            "Roc 0.8722803129948316\n",
            "Epoch 19 - loss is 0.644083 : \n",
            "Roc 0.8814677597252947\n",
            "Time mean 0.6644696116447448\n",
            "Time std 0.05528592075332476\n",
            "test\n",
            "Roc 0.8814677597252947\n",
            "test\n",
            "Crossvalidation run 7\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.993330 : \n",
            "Roc 0.808146659214034\n",
            "Epoch 1 - loss is 0.817497 : \n",
            "Roc 0.7009231988674547\n",
            "Epoch 2 - loss is 0.828609 : \n",
            "Roc 0.8357399126704312\n",
            "Epoch 3 - loss is 0.795411 : \n",
            "Roc 0.7984401225544058\n",
            "Epoch 4 - loss is 0.774986 : \n",
            "Roc 0.8236608351407559\n",
            "Epoch 5 - loss is 0.770911 : \n",
            "Roc 0.8328428609192218\n",
            "Epoch 6 - loss is 0.759199 : \n",
            "Roc 0.7068918739231106\n",
            "Epoch 7 - loss is 0.777956 : \n",
            "Roc 0.836982515773147\n",
            "Epoch 8 - loss is 0.764544 : \n",
            "Roc 0.7953274938452082\n",
            "Epoch 9 - loss is 0.720276 : \n",
            "Roc 0.8766108555240252\n",
            "Epoch 10 - loss is 0.698771 : \n",
            "Roc 0.8824544242841172\n",
            "Epoch 11 - loss is 0.692211 : \n",
            "Roc 0.8713367841571041\n",
            "Epoch 12 - loss is 0.681659 : \n",
            "Roc 0.8795599602526252\n",
            "Epoch 13 - loss is 0.678894 : \n",
            "Roc 0.8802419239258176\n",
            "Epoch 14 - loss is 0.671644 : \n",
            "Roc 0.8888923723577392\n",
            "Epoch 15 - loss is 0.658829 : \n",
            "Roc 0.8857756630136027\n",
            "Epoch 16 - loss is 0.660756 : \n",
            "Roc 0.8988317042642436\n",
            "Epoch 17 - loss is 0.645202 : \n",
            "Roc 0.8954997165451631\n",
            "Epoch 18 - loss is 0.643262 : \n",
            "Roc 0.8973563059146514\n",
            "Epoch 19 - loss is 0.641013 : \n",
            "Roc 0.9000967010952822\n",
            "Time mean 0.6768536329269409\n",
            "Time std 0.07348780511518845\n",
            "test\n",
            "Roc 0.9000967010952822\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.980455 : \n",
            "Roc 0.7734058095427588\n",
            "Epoch 1 - loss is 0.796993 : \n",
            "Roc 0.7976012730738621\n",
            "Epoch 2 - loss is 0.803146 : \n",
            "Roc 0.7960249736848862\n",
            "Epoch 3 - loss is 0.771972 : \n",
            "Roc 0.80808158808503\n",
            "Epoch 4 - loss is 0.767499 : \n",
            "Roc 0.7928051831146825\n",
            "Epoch 5 - loss is 0.757599 : \n",
            "Roc 0.7983733769672152\n",
            "Epoch 6 - loss is 0.749384 : \n",
            "Roc 0.8068625083438166\n",
            "Epoch 7 - loss is 0.742815 : \n",
            "Roc 0.815301797450643\n",
            "Epoch 8 - loss is 0.748292 : \n",
            "Roc 0.8151246919206182\n",
            "Epoch 9 - loss is 0.700206 : \n",
            "Roc 0.8461888614862263\n",
            "Epoch 10 - loss is 0.682740 : \n",
            "Roc 0.8504880330415137\n",
            "Epoch 11 - loss is 0.673374 : \n",
            "Roc 0.8538339837231392\n",
            "Epoch 12 - loss is 0.661972 : \n",
            "Roc 0.8545748985905368\n",
            "Epoch 13 - loss is 0.658014 : \n",
            "Roc 0.8532599451875433\n",
            "Epoch 14 - loss is 0.658076 : \n",
            "Roc 0.8552085071628456\n",
            "Epoch 15 - loss is 0.649434 : \n",
            "Roc 0.86449561628713\n",
            "Epoch 16 - loss is 0.643670 : \n",
            "Roc 0.86280599342764\n",
            "Epoch 17 - loss is 0.639561 : \n",
            "Roc 0.8548833791173526\n",
            "Epoch 18 - loss is 0.637566 : \n",
            "Roc 0.862742010795615\n",
            "Epoch 19 - loss is 0.634432 : \n",
            "Roc 0.8642666627621884\n",
            "Time mean 0.667014193534851\n",
            "Time std 0.06794438084234257\n",
            "test\n",
            "Roc 0.8642666627621884\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.994608 : \n",
            "Roc 0.7834671066423622\n",
            "Epoch 1 - loss is 0.799464 : \n",
            "Roc 0.789621972456243\n",
            "Epoch 2 - loss is 0.781495 : \n",
            "Roc 0.8386907336872467\n",
            "Epoch 3 - loss is 0.774393 : \n",
            "Roc 0.8283388063966166\n",
            "Epoch 4 - loss is 0.755515 : \n",
            "Roc 0.8327972508747961\n",
            "Epoch 5 - loss is 0.751834 : \n",
            "Roc 0.746098088251576\n",
            "Epoch 6 - loss is 0.767626 : \n",
            "Roc 0.7494107468362955\n",
            "Epoch 7 - loss is 0.764989 : \n",
            "Roc 0.8169593243154295\n",
            "Epoch 8 - loss is 0.764879 : \n",
            "Roc 0.773447995626115\n",
            "Epoch 9 - loss is 0.726230 : \n",
            "Roc 0.8532660700133582\n",
            "Epoch 10 - loss is 0.694800 : \n",
            "Roc 0.8552972099742535\n",
            "Epoch 11 - loss is 0.683779 : \n",
            "Roc 0.8619898703624017\n",
            "Epoch 12 - loss is 0.675040 : \n",
            "Roc 0.8684348446749586\n",
            "Epoch 13 - loss is 0.662703 : \n",
            "Roc 0.8714281936760532\n",
            "Epoch 14 - loss is 0.659199 : \n",
            "Roc 0.8666808772272248\n",
            "Epoch 15 - loss is 0.656189 : \n",
            "Roc 0.8706927448525241\n",
            "Epoch 16 - loss is 0.658211 : \n",
            "Roc 0.8659967918755922\n",
            "Epoch 17 - loss is 0.653585 : \n",
            "Roc 0.8778889575003415\n",
            "Epoch 18 - loss is 0.655398 : \n",
            "Roc 0.875732642856083\n",
            "Epoch 19 - loss is 0.648064 : \n",
            "Roc 0.867925014657613\n",
            "Time mean 0.6607748389244079\n",
            "Time std 0.04514612569685032\n",
            "test\n",
            "Roc 0.867925014657613\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977155 : \n",
            "Roc 0.8252452477595935\n",
            "Epoch 1 - loss is 0.804080 : \n",
            "Roc 0.7443337311386431\n",
            "Epoch 2 - loss is 0.794833 : \n",
            "Roc 0.8067007199018222\n",
            "Epoch 3 - loss is 0.778104 : \n",
            "Roc 0.807534460081495\n",
            "Epoch 4 - loss is 0.764602 : \n",
            "Roc 0.8138248548128547\n",
            "Epoch 5 - loss is 0.769325 : \n",
            "Roc 0.7844997834083739\n",
            "Epoch 6 - loss is 0.759537 : \n",
            "Roc 0.8225219323589462\n",
            "Epoch 7 - loss is 0.745358 : \n",
            "Roc 0.8094499791270139\n",
            "Epoch 8 - loss is 0.755712 : \n",
            "Roc 0.8096709929959536\n",
            "Epoch 9 - loss is 0.705923 : \n",
            "Roc 0.8510408780333637\n",
            "Epoch 10 - loss is 0.683630 : \n",
            "Roc 0.858036374814364\n",
            "Epoch 11 - loss is 0.668657 : \n",
            "Roc 0.8614730471179162\n",
            "Epoch 12 - loss is 0.668137 : \n",
            "Roc 0.8569438842936873\n",
            "Epoch 13 - loss is 0.658878 : \n",
            "Roc 0.8639385948981861\n",
            "Epoch 14 - loss is 0.653688 : \n",
            "Roc 0.8671879606502938\n",
            "Epoch 15 - loss is 0.657896 : \n",
            "Roc 0.8703169431256333\n",
            "Epoch 16 - loss is 0.651834 : \n",
            "Roc 0.8760982885723743\n",
            "Epoch 17 - loss is 0.645213 : \n",
            "Roc 0.8781179760043208\n",
            "Epoch 18 - loss is 0.633513 : \n",
            "Roc 0.872534747035918\n",
            "Epoch 19 - loss is 0.624173 : \n",
            "Roc 0.8836638615889099\n",
            "Time mean 0.6711580872535705\n",
            "Time std 0.05579941398571745\n",
            "test\n",
            "Roc 0.8836638615889099\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.037389 : \n",
            "Roc 0.8190438817729485\n",
            "Epoch 1 - loss is 0.800712 : \n",
            "Roc 0.8217340217622147\n",
            "Epoch 2 - loss is 0.792449 : \n",
            "Roc 0.8088809787428339\n",
            "Epoch 3 - loss is 0.797997 : \n",
            "Roc 0.8168138536055084\n",
            "Epoch 4 - loss is 0.785915 : \n",
            "Roc 0.7836914683545639\n",
            "Epoch 5 - loss is 0.783712 : \n",
            "Roc 0.823155190930776\n",
            "Epoch 6 - loss is 0.755964 : \n",
            "Roc 0.738565537409631\n",
            "Epoch 7 - loss is 0.765335 : \n",
            "Roc 0.8439829193987638\n",
            "Epoch 8 - loss is 0.757874 : \n",
            "Roc 0.8234562662248031\n",
            "Epoch 9 - loss is 0.711718 : \n",
            "Roc 0.8619386122910228\n",
            "Epoch 10 - loss is 0.691177 : \n",
            "Roc 0.8624696458076779\n",
            "Epoch 11 - loss is 0.674903 : \n",
            "Roc 0.8696872470483972\n",
            "Epoch 12 - loss is 0.672579 : \n",
            "Roc 0.8701874110670276\n",
            "Epoch 13 - loss is 0.660783 : \n",
            "Roc 0.867569755785149\n",
            "Epoch 14 - loss is 0.661099 : \n",
            "Roc 0.8769062159251052\n",
            "Epoch 15 - loss is 0.655834 : \n",
            "Roc 0.8746886918581308\n",
            "Epoch 16 - loss is 0.655797 : \n",
            "Roc 0.8758548946031325\n",
            "Epoch 17 - loss is 0.644467 : \n",
            "Roc 0.8804599203527876\n",
            "Epoch 18 - loss is 0.651037 : \n",
            "Roc 0.8752158178433904\n",
            "Epoch 19 - loss is 0.644340 : \n",
            "Roc 0.8852333607055909\n",
            "Time mean 0.6741190791130066\n",
            "Time std 0.07373880625113738\n",
            "test\n",
            "Roc 0.8852333607055909\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.988088 : \n",
            "Roc 0.763359275782344\n",
            "Epoch 1 - loss is 0.837863 : \n",
            "Roc 0.8198322775656877\n",
            "Epoch 2 - loss is 0.789912 : \n",
            "Roc 0.7821723081756067\n",
            "Epoch 3 - loss is 0.780142 : \n",
            "Roc 0.8241465904802769\n",
            "Epoch 4 - loss is 0.770191 : \n",
            "Roc 0.818573593102616\n",
            "Epoch 5 - loss is 0.757233 : \n",
            "Roc 0.8121209915468353\n",
            "Epoch 6 - loss is 0.769035 : \n",
            "Roc 0.8224386221322356\n",
            "Epoch 7 - loss is 0.765516 : \n",
            "Roc 0.8216656163263117\n",
            "Epoch 8 - loss is 0.761719 : \n",
            "Roc 0.8374523276372322\n",
            "Epoch 9 - loss is 0.704789 : \n",
            "Roc 0.8601781811425276\n",
            "Epoch 10 - loss is 0.686096 : \n",
            "Roc 0.8656879665474578\n",
            "Epoch 11 - loss is 0.674501 : \n",
            "Roc 0.8702789755063183\n",
            "Epoch 12 - loss is 0.664623 : \n",
            "Roc 0.8694525268507219\n",
            "Epoch 13 - loss is 0.660337 : \n",
            "Roc 0.8684219801437785\n",
            "Epoch 14 - loss is 0.649409 : \n",
            "Roc 0.8748174993726691\n",
            "Epoch 15 - loss is 0.656341 : \n",
            "Roc 0.8848231884502148\n",
            "Epoch 16 - loss is 0.643514 : \n",
            "Roc 0.8823992966231398\n",
            "Epoch 17 - loss is 0.641775 : \n",
            "Roc 0.8763087272748169\n",
            "Epoch 18 - loss is 0.638846 : \n",
            "Roc 0.8847278916125375\n",
            "Epoch 19 - loss is 0.631997 : \n",
            "Roc 0.8764267229572477\n",
            "Time mean 0.6523853063583374\n",
            "Time std 0.03766807555563086\n",
            "test\n",
            "Roc 0.8764267229572477\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.958111 : \n",
            "Roc 0.7712754359728792\n",
            "Epoch 1 - loss is 0.796889 : \n",
            "Roc 0.8048371408770927\n",
            "Epoch 2 - loss is 0.790262 : \n",
            "Roc 0.8170986881299469\n",
            "Epoch 3 - loss is 0.787469 : \n",
            "Roc 0.7840583674776188\n",
            "Epoch 4 - loss is 0.764100 : \n",
            "Roc 0.8320288595200753\n",
            "Epoch 5 - loss is 0.757392 : \n",
            "Roc 0.798501096264595\n",
            "Epoch 6 - loss is 0.761486 : \n",
            "Roc 0.8049327854036725\n",
            "Epoch 7 - loss is 0.760979 : \n",
            "Roc 0.8127640862908161\n",
            "Epoch 8 - loss is 0.765242 : \n",
            "Roc 0.7887473809205381\n",
            "Epoch 9 - loss is 0.721956 : \n",
            "Roc 0.8539887009771141\n",
            "Epoch 10 - loss is 0.696117 : \n",
            "Roc 0.85852958698586\n",
            "Epoch 11 - loss is 0.693911 : \n",
            "Roc 0.8628572991768897\n",
            "Epoch 12 - loss is 0.686611 : \n",
            "Roc 0.8628036004490429\n",
            "Epoch 13 - loss is 0.670019 : \n",
            "Roc 0.8617468500123608\n",
            "Epoch 14 - loss is 0.672637 : \n",
            "Roc 0.8663649406071807\n",
            "Epoch 15 - loss is 0.666401 : \n",
            "Roc 0.8785467503151001\n",
            "Epoch 16 - loss is 0.655817 : \n",
            "Roc 0.8757853185651701\n",
            "Epoch 17 - loss is 0.652811 : \n",
            "Roc 0.8750400207499989\n",
            "Epoch 18 - loss is 0.652592 : \n",
            "Roc 0.8783134141448528\n",
            "Epoch 19 - loss is 0.652083 : \n",
            "Roc 0.8776708531410716\n",
            "Time mean 0.6610299944877625\n",
            "Time std 0.06077542668488249\n",
            "test\n",
            "Roc 0.8776708531410716\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.029302 : \n",
            "Roc 0.7937863762155822\n",
            "Epoch 1 - loss is 0.820255 : \n",
            "Roc 0.7849602194475114\n",
            "Epoch 2 - loss is 0.790274 : \n",
            "Roc 0.8244798659891288\n",
            "Epoch 3 - loss is 0.790047 : \n",
            "Roc 0.8211255712162359\n",
            "Epoch 4 - loss is 0.782050 : \n",
            "Roc 0.8169375352502677\n",
            "Epoch 5 - loss is 0.788580 : \n",
            "Roc 0.8101101201678432\n",
            "Epoch 6 - loss is 0.770305 : \n",
            "Roc 0.8364725123466699\n",
            "Epoch 7 - loss is 0.766706 : \n",
            "Roc 0.8260780743630028\n",
            "Epoch 8 - loss is 0.756905 : \n",
            "Roc 0.8290687296323609\n",
            "Epoch 9 - loss is 0.707559 : \n",
            "Roc 0.863003870317479\n",
            "Epoch 10 - loss is 0.685118 : \n",
            "Roc 0.8725888032001847\n",
            "Epoch 11 - loss is 0.673657 : \n",
            "Roc 0.8702839214036288\n",
            "Epoch 12 - loss is 0.670235 : \n",
            "Roc 0.8636029040921905\n",
            "Epoch 13 - loss is 0.664946 : \n",
            "Roc 0.8752196457173942\n",
            "Epoch 14 - loss is 0.652783 : \n",
            "Roc 0.88258996888753\n",
            "Epoch 15 - loss is 0.648658 : \n",
            "Roc 0.8807475352777416\n",
            "Epoch 16 - loss is 0.651553 : \n",
            "Roc 0.8866494652556486\n",
            "Epoch 17 - loss is 0.645260 : \n",
            "Roc 0.8833859331758935\n",
            "Epoch 18 - loss is 0.640648 : \n",
            "Roc 0.8866600624133338\n",
            "Epoch 19 - loss is 0.636429 : \n",
            "Roc 0.8847857392086592\n",
            "Time mean 0.6537361025810242\n",
            "Time std 0.03702872894383442\n",
            "test\n",
            "Roc 0.8847857392086592\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.968991 : \n",
            "Roc 0.7496447888049451\n",
            "Epoch 1 - loss is 0.795149 : \n",
            "Roc 0.8192906702672327\n",
            "Epoch 2 - loss is 0.800924 : \n",
            "Roc 0.8349814248251748\n",
            "Epoch 3 - loss is 0.784348 : \n",
            "Roc 0.8386182177197802\n",
            "Epoch 4 - loss is 0.773473 : \n",
            "Roc 0.801934783966034\n",
            "Epoch 5 - loss is 0.767035 : \n",
            "Roc 0.8272216065184815\n",
            "Epoch 6 - loss is 0.756862 : \n",
            "Roc 0.8321957339535465\n",
            "Epoch 7 - loss is 0.753326 : \n",
            "Roc 0.810112933941059\n",
            "Epoch 8 - loss is 0.767650 : \n",
            "Roc 0.7835561899038461\n",
            "Epoch 9 - loss is 0.728723 : \n",
            "Roc 0.8619015749875124\n",
            "Epoch 10 - loss is 0.693395 : \n",
            "Roc 0.8669174575424575\n",
            "Epoch 11 - loss is 0.693864 : \n",
            "Roc 0.8714097426011489\n",
            "Epoch 12 - loss is 0.686080 : \n",
            "Roc 0.8756645698051948\n",
            "Epoch 13 - loss is 0.671287 : \n",
            "Roc 0.878503235046204\n",
            "Epoch 14 - loss is 0.668710 : \n",
            "Roc 0.8717538321053947\n",
            "Epoch 15 - loss is 0.653429 : \n",
            "Roc 0.8824011925574426\n",
            "Epoch 16 - loss is 0.652309 : \n",
            "Roc 0.8834977522477523\n",
            "Epoch 17 - loss is 0.652566 : \n",
            "Roc 0.8860253028221778\n",
            "Epoch 18 - loss is 0.648252 : \n",
            "Roc 0.8812741945554445\n",
            "Epoch 19 - loss is 0.636212 : \n",
            "Roc 0.8819929094343157\n",
            "Time mean 0.6726281046867371\n",
            "Time std 0.09885825557765081\n",
            "test\n",
            "Roc 0.8819929094343157\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.977268 : \n",
            "Roc 0.8248015677222102\n",
            "Epoch 1 - loss is 0.797704 : \n",
            "Roc 0.8136112696792814\n",
            "Epoch 2 - loss is 0.795311 : \n",
            "Roc 0.742571619031678\n",
            "Epoch 3 - loss is 0.793665 : \n",
            "Roc 0.8080581221392671\n",
            "Epoch 4 - loss is 0.768691 : \n",
            "Roc 0.7933316845019426\n",
            "Epoch 5 - loss is 0.779863 : \n",
            "Roc 0.80060403022691\n",
            "Epoch 6 - loss is 0.778663 : \n",
            "Roc 0.8320546562623741\n",
            "Epoch 7 - loss is 0.765884 : \n",
            "Roc 0.800261148244515\n",
            "Epoch 8 - loss is 0.784806 : \n",
            "Roc 0.7970838168448111\n",
            "Epoch 9 - loss is 0.732583 : \n",
            "Roc 0.8500386809182325\n",
            "Epoch 10 - loss is 0.702787 : \n",
            "Roc 0.8603466876184838\n",
            "Epoch 11 - loss is 0.694390 : \n",
            "Roc 0.8572384738982912\n",
            "Epoch 12 - loss is 0.683624 : \n",
            "Roc 0.8600386767545167\n",
            "Epoch 13 - loss is 0.675724 : \n",
            "Roc 0.8709067032074351\n",
            "Epoch 14 - loss is 0.662264 : \n",
            "Roc 0.8711738055653055\n",
            "Epoch 15 - loss is 0.661407 : \n",
            "Roc 0.8748217409244532\n",
            "Epoch 16 - loss is 0.652932 : \n",
            "Roc 0.8551822229328297\n",
            "Epoch 17 - loss is 0.652628 : \n",
            "Roc 0.8820854469394817\n",
            "Epoch 18 - loss is 0.644087 : \n",
            "Roc 0.8793896492527586\n",
            "Epoch 19 - loss is 0.640979 : \n",
            "Roc 0.8811575629106204\n",
            "Time mean 0.6561452865600585\n",
            "Time std 0.04206197501697599\n",
            "test\n",
            "Roc 0.8811575629106204\n",
            "test\n",
            "Crossvalidation run 8\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.954426 : \n",
            "Roc 0.7860856519302001\n",
            "Epoch 1 - loss is 0.811247 : \n",
            "Roc 0.7903564842682567\n",
            "Epoch 2 - loss is 0.838480 : \n",
            "Roc 0.8019428997748286\n",
            "Epoch 3 - loss is 0.801781 : \n",
            "Roc 0.8446124073596342\n",
            "Epoch 4 - loss is 0.780683 : \n",
            "Roc 0.8088354902972454\n",
            "Epoch 5 - loss is 0.774942 : \n",
            "Roc 0.8459795195918249\n",
            "Epoch 6 - loss is 0.765774 : \n",
            "Roc 0.8341586168677924\n",
            "Epoch 7 - loss is 0.776834 : \n",
            "Roc 0.8478661663211066\n",
            "Epoch 8 - loss is 0.760616 : \n",
            "Roc 0.8470447643662236\n",
            "Epoch 9 - loss is 0.717843 : \n",
            "Roc 0.8776486306583478\n",
            "Epoch 10 - loss is 0.692477 : \n",
            "Roc 0.8743098750569298\n",
            "Epoch 11 - loss is 0.689826 : \n",
            "Roc 0.8859904437501394\n",
            "Epoch 12 - loss is 0.676250 : \n",
            "Roc 0.8810062965192383\n",
            "Epoch 13 - loss is 0.675484 : \n",
            "Roc 0.8823608682635685\n",
            "Epoch 14 - loss is 0.671019 : \n",
            "Roc 0.895601035724864\n",
            "Epoch 15 - loss is 0.671678 : \n",
            "Roc 0.8954874746403468\n",
            "Epoch 16 - loss is 0.662803 : \n",
            "Roc 0.8924635250953078\n",
            "Epoch 17 - loss is 0.658489 : \n",
            "Roc 0.8947054856473122\n",
            "Epoch 18 - loss is 0.648180 : \n",
            "Roc 0.8988522069666192\n",
            "Epoch 19 - loss is 0.658918 : \n",
            "Roc 0.9009148186366778\n",
            "Time mean 0.6756814360618592\n",
            "Time std 0.09330515613511862\n",
            "test\n",
            "Roc 0.9009148186366778\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.972634 : \n",
            "Roc 0.7153996367230623\n",
            "Epoch 1 - loss is 0.799130 : \n",
            "Roc 0.8258703643680522\n",
            "Epoch 2 - loss is 0.796071 : \n",
            "Roc 0.7162498636106903\n",
            "Epoch 3 - loss is 0.776317 : \n",
            "Roc 0.7558926198942261\n",
            "Epoch 4 - loss is 0.778803 : \n",
            "Roc 0.8098170417640113\n",
            "Epoch 5 - loss is 0.771313 : \n",
            "Roc 0.8153134306564657\n",
            "Epoch 6 - loss is 0.760583 : \n",
            "Roc 0.804060109175631\n",
            "Epoch 7 - loss is 0.753885 : \n",
            "Roc 0.8163544020050834\n",
            "Epoch 8 - loss is 0.748693 : \n",
            "Roc 0.8080529062155016\n",
            "Epoch 9 - loss is 0.703001 : \n",
            "Roc 0.8466797627146928\n",
            "Epoch 10 - loss is 0.684274 : \n",
            "Roc 0.8510686904700778\n",
            "Epoch 11 - loss is 0.673873 : \n",
            "Roc 0.849049928114811\n",
            "Epoch 12 - loss is 0.663871 : \n",
            "Roc 0.8570086454776513\n",
            "Epoch 13 - loss is 0.664540 : \n",
            "Roc 0.8577670101537829\n",
            "Epoch 14 - loss is 0.667450 : \n",
            "Roc 0.8512632458088367\n",
            "Epoch 15 - loss is 0.652788 : \n",
            "Roc 0.8604161719211317\n",
            "Epoch 16 - loss is 0.654738 : \n",
            "Roc 0.8662801099458293\n",
            "Epoch 17 - loss is 0.640463 : \n",
            "Roc 0.8635477105850942\n",
            "Epoch 18 - loss is 0.635630 : \n",
            "Roc 0.864285616864779\n",
            "Epoch 19 - loss is 0.630089 : \n",
            "Roc 0.8688327963338554\n",
            "Time mean 0.6675854206085206\n",
            "Time std 0.06465377317837526\n",
            "test\n",
            "Roc 0.8688327963338554\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.957300 : \n",
            "Roc 0.8020926429626146\n",
            "Epoch 1 - loss is 0.796440 : \n",
            "Roc 0.7126553316995944\n",
            "Epoch 2 - loss is 0.803333 : \n",
            "Roc 0.7957275673841193\n",
            "Epoch 3 - loss is 0.786031 : \n",
            "Roc 0.8062968382549051\n",
            "Epoch 4 - loss is 0.783600 : \n",
            "Roc 0.82315566137699\n",
            "Epoch 5 - loss is 0.767637 : \n",
            "Roc 0.705020532072303\n",
            "Epoch 6 - loss is 0.769438 : \n",
            "Roc 0.8112739586817015\n",
            "Epoch 7 - loss is 0.761500 : \n",
            "Roc 0.8147950197977648\n",
            "Epoch 8 - loss is 0.765436 : \n",
            "Roc 0.8001350288605642\n",
            "Epoch 9 - loss is 0.731810 : \n",
            "Roc 0.8526702537393558\n",
            "Epoch 10 - loss is 0.695964 : \n",
            "Roc 0.8553432468638795\n",
            "Epoch 11 - loss is 0.688536 : \n",
            "Roc 0.8544809112412572\n",
            "Epoch 12 - loss is 0.676744 : \n",
            "Roc 0.8542551021999927\n",
            "Epoch 13 - loss is 0.672207 : \n",
            "Roc 0.863364699293505\n",
            "Epoch 14 - loss is 0.659906 : \n",
            "Roc 0.8631409828381322\n",
            "Epoch 15 - loss is 0.651251 : \n",
            "Roc 0.8661636280417163\n",
            "Epoch 16 - loss is 0.645928 : \n",
            "Roc 0.8715594558820116\n",
            "Epoch 17 - loss is 0.644262 : \n",
            "Roc 0.8741183079578758\n",
            "Epoch 18 - loss is 0.643314 : \n",
            "Roc 0.874596558951759\n",
            "Epoch 19 - loss is 0.633102 : \n",
            "Roc 0.8727269285747168\n",
            "Time mean 0.6573607802391053\n",
            "Time std 0.04694633606684747\n",
            "test\n",
            "Roc 0.8727269285747168\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.964587 : \n",
            "Roc 0.802372425173686\n",
            "Epoch 1 - loss is 0.799234 : \n",
            "Roc 0.7926666831763732\n",
            "Epoch 2 - loss is 0.781125 : \n",
            "Roc 0.7930646850301774\n",
            "Epoch 3 - loss is 0.796180 : \n",
            "Roc 0.8126678978190678\n",
            "Epoch 4 - loss is 0.769505 : \n",
            "Roc 0.8261802081638141\n",
            "Epoch 5 - loss is 0.769317 : \n",
            "Roc 0.7839886704104392\n",
            "Epoch 6 - loss is 0.771471 : \n",
            "Roc 0.815422562007706\n",
            "Epoch 7 - loss is 0.751889 : \n",
            "Roc 0.8081862004011072\n",
            "Epoch 8 - loss is 0.751486 : \n",
            "Roc 0.8110180081589398\n",
            "Epoch 9 - loss is 0.706972 : \n",
            "Roc 0.8524614989712881\n",
            "Epoch 10 - loss is 0.683312 : \n",
            "Roc 0.8511246058307567\n",
            "Epoch 11 - loss is 0.681830 : \n",
            "Roc 0.8582766500075867\n",
            "Epoch 12 - loss is 0.667195 : \n",
            "Roc 0.8607079008379854\n",
            "Epoch 13 - loss is 0.665138 : \n",
            "Roc 0.867488083529681\n",
            "Epoch 14 - loss is 0.652143 : \n",
            "Roc 0.8625392989978609\n",
            "Epoch 15 - loss is 0.652034 : \n",
            "Roc 0.8700453191444198\n",
            "Epoch 16 - loss is 0.647460 : \n",
            "Roc 0.8711614932321996\n",
            "Epoch 17 - loss is 0.644680 : \n",
            "Roc 0.8722769794155406\n",
            "Epoch 18 - loss is 0.642265 : \n",
            "Roc 0.8748441405086089\n",
            "Epoch 19 - loss is 0.638655 : \n",
            "Roc 0.8745656374830088\n",
            "Time mean 0.6737962007522583\n",
            "Time std 0.07325790433164972\n",
            "test\n",
            "Roc 0.8745656374830088\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.969926 : \n",
            "Roc 0.7947236017436187\n",
            "Epoch 1 - loss is 0.799149 : \n",
            "Roc 0.762108804820956\n",
            "Epoch 2 - loss is 0.807064 : \n",
            "Roc 0.7999467989600496\n",
            "Epoch 3 - loss is 0.786504 : \n",
            "Roc 0.8362882085746478\n",
            "Epoch 4 - loss is 0.784523 : \n",
            "Roc 0.8092174171959906\n",
            "Epoch 5 - loss is 0.766432 : \n",
            "Roc 0.7746108538327314\n",
            "Epoch 6 - loss is 0.765853 : \n",
            "Roc 0.7263589954596439\n",
            "Epoch 7 - loss is 0.759156 : \n",
            "Roc 0.8274866894827425\n",
            "Epoch 8 - loss is 0.757081 : \n",
            "Roc 0.8252292109072488\n",
            "Epoch 9 - loss is 0.712082 : \n",
            "Roc 0.859922521467489\n",
            "Epoch 10 - loss is 0.685682 : \n",
            "Roc 0.8615761887540856\n",
            "Epoch 11 - loss is 0.676498 : \n",
            "Roc 0.8689547802883018\n",
            "Epoch 12 - loss is 0.669348 : \n",
            "Roc 0.8687375215427091\n",
            "Epoch 13 - loss is 0.662277 : \n",
            "Roc 0.8691734066698825\n",
            "Epoch 14 - loss is 0.658390 : \n",
            "Roc 0.874950105708493\n",
            "Epoch 15 - loss is 0.657263 : \n",
            "Roc 0.867575030952533\n",
            "Epoch 16 - loss is 0.659663 : \n",
            "Roc 0.8845700572511962\n",
            "Epoch 17 - loss is 0.644126 : \n",
            "Roc 0.8849139200140046\n",
            "Epoch 18 - loss is 0.642417 : \n",
            "Roc 0.8813076592890521\n",
            "Epoch 19 - loss is 0.640391 : \n",
            "Roc 0.8813160604815524\n",
            "Time mean 0.6686657786369323\n",
            "Time std 0.0689395265333048\n",
            "test\n",
            "Roc 0.8813160604815524\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.984465 : \n",
            "Roc 0.4365749263005863\n",
            "Epoch 1 - loss is 0.808674 : \n",
            "Roc 0.8163159679189661\n",
            "Epoch 2 - loss is 0.782485 : \n",
            "Roc 0.8106696063426511\n",
            "Epoch 3 - loss is 0.796136 : \n",
            "Roc 0.7725026003106965\n",
            "Epoch 4 - loss is 0.774201 : \n",
            "Roc 0.810857709849383\n",
            "Epoch 5 - loss is 0.782883 : \n",
            "Roc 0.8100987830737492\n",
            "Epoch 6 - loss is 0.769003 : \n",
            "Roc 0.827066600517572\n",
            "Epoch 7 - loss is 0.756619 : \n",
            "Roc 0.8120506921711015\n",
            "Epoch 8 - loss is 0.755508 : \n",
            "Roc 0.8222464002697044\n",
            "Epoch 9 - loss is 0.719236 : \n",
            "Roc 0.8593451239625096\n",
            "Epoch 10 - loss is 0.689607 : \n",
            "Roc 0.8663741037787352\n",
            "Epoch 11 - loss is 0.676522 : \n",
            "Roc 0.8667031891398\n",
            "Epoch 12 - loss is 0.678955 : \n",
            "Roc 0.8704618113486562\n",
            "Epoch 13 - loss is 0.667651 : \n",
            "Roc 0.8639948434354366\n",
            "Epoch 14 - loss is 0.666561 : \n",
            "Roc 0.8761374802941464\n",
            "Epoch 15 - loss is 0.660985 : \n",
            "Roc 0.8695425560240051\n",
            "Epoch 16 - loss is 0.650533 : \n",
            "Roc 0.8754083397662689\n",
            "Epoch 17 - loss is 0.654618 : \n",
            "Roc 0.8821177160173392\n",
            "Epoch 18 - loss is 0.648399 : \n",
            "Roc 0.8849055555661973\n",
            "Epoch 19 - loss is 0.640469 : \n",
            "Roc 0.8788103889834941\n",
            "Time mean 0.6591058492660522\n",
            "Time std 0.046966849184755376\n",
            "test\n",
            "Roc 0.8788103889834941\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.022856 : \n",
            "Roc 0.7480066626949872\n",
            "Epoch 1 - loss is 0.820156 : \n",
            "Roc 0.797344243293738\n",
            "Epoch 2 - loss is 0.797779 : \n",
            "Roc 0.802269531139183\n",
            "Epoch 3 - loss is 0.783488 : \n",
            "Roc 0.8234831629158612\n",
            "Epoch 4 - loss is 0.778901 : \n",
            "Roc 0.8306733212561854\n",
            "Epoch 5 - loss is 0.768159 : \n",
            "Roc 0.8249409313993685\n",
            "Epoch 6 - loss is 0.764961 : \n",
            "Roc 0.826788978184132\n",
            "Epoch 7 - loss is 0.763771 : \n",
            "Roc 0.8316757650548943\n",
            "Epoch 8 - loss is 0.761694 : \n",
            "Roc 0.8105328534896068\n",
            "Epoch 9 - loss is 0.721637 : \n",
            "Roc 0.8619944720705826\n",
            "Epoch 10 - loss is 0.692825 : \n",
            "Roc 0.868455746169153\n",
            "Epoch 11 - loss is 0.675898 : \n",
            "Roc 0.8731502308032114\n",
            "Epoch 12 - loss is 0.672451 : \n",
            "Roc 0.8698069277438023\n",
            "Epoch 13 - loss is 0.673084 : \n",
            "Roc 0.8727364466437282\n",
            "Epoch 14 - loss is 0.654858 : \n",
            "Roc 0.8738734412171172\n",
            "Epoch 15 - loss is 0.656909 : \n",
            "Roc 0.878812001766992\n",
            "Epoch 16 - loss is 0.649492 : \n",
            "Roc 0.875860902057573\n",
            "Epoch 17 - loss is 0.643729 : \n",
            "Roc 0.8811778866612361\n",
            "Epoch 18 - loss is 0.639343 : \n",
            "Roc 0.877959407814482\n",
            "Epoch 19 - loss is 0.639317 : \n",
            "Roc 0.8785239536853537\n",
            "Time mean 0.6971354603767395\n",
            "Time std 0.10404842956396927\n",
            "test\n",
            "Roc 0.8785239536853537\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.973895 : \n",
            "Roc 0.7804725194114417\n",
            "Epoch 1 - loss is 0.812011 : \n",
            "Roc 0.7818547616365622\n",
            "Epoch 2 - loss is 0.796269 : \n",
            "Roc 0.8000833446827583\n",
            "Epoch 3 - loss is 0.783918 : \n",
            "Roc 0.82164806996322\n",
            "Epoch 4 - loss is 0.783356 : \n",
            "Roc 0.7876108825744657\n",
            "Epoch 5 - loss is 0.784513 : \n",
            "Roc 0.8023747837885467\n",
            "Epoch 6 - loss is 0.758668 : \n",
            "Roc 0.8207096327770873\n",
            "Epoch 7 - loss is 0.762933 : \n",
            "Roc 0.8155876732291855\n",
            "Epoch 8 - loss is 0.751056 : \n",
            "Roc 0.8255945299826952\n",
            "Epoch 9 - loss is 0.717260 : \n",
            "Roc 0.8578049832545285\n",
            "Epoch 10 - loss is 0.692728 : \n",
            "Roc 0.8577260933028712\n",
            "Epoch 11 - loss is 0.684479 : \n",
            "Roc 0.8664538338750313\n",
            "Epoch 12 - loss is 0.676904 : \n",
            "Roc 0.8708391928419733\n",
            "Epoch 13 - loss is 0.666026 : \n",
            "Roc 0.8604238549084268\n",
            "Epoch 14 - loss is 0.661818 : \n",
            "Roc 0.8764858098171284\n",
            "Epoch 15 - loss is 0.652937 : \n",
            "Roc 0.870625385373489\n",
            "Epoch 16 - loss is 0.653609 : \n",
            "Roc 0.8643854256897867\n",
            "Epoch 17 - loss is 0.650669 : \n",
            "Roc 0.8763193951927368\n",
            "Epoch 18 - loss is 0.643123 : \n",
            "Roc 0.885360536891257\n",
            "Epoch 19 - loss is 0.639366 : \n",
            "Roc 0.8781700710362803\n",
            "Time mean 0.6583102107048034\n",
            "Time std 0.04084147490112779\n",
            "test\n",
            "Roc 0.8781700710362803\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.972660 : \n",
            "Roc 0.8164818579857643\n",
            "Epoch 1 - loss is 0.800472 : \n",
            "Roc 0.8151583767794706\n",
            "Epoch 2 - loss is 0.790706 : \n",
            "Roc 0.7608809159590411\n",
            "Epoch 3 - loss is 0.787802 : \n",
            "Roc 0.7567114526098901\n",
            "Epoch 4 - loss is 0.801299 : \n",
            "Roc 0.7734091104208293\n",
            "Epoch 5 - loss is 0.783535 : \n",
            "Roc 0.829440383834915\n",
            "Epoch 6 - loss is 0.768923 : \n",
            "Roc 0.8206881009615385\n",
            "Epoch 7 - loss is 0.773218 : \n",
            "Roc 0.8240318275474525\n",
            "Epoch 8 - loss is 0.765363 : \n",
            "Roc 0.8272177041708292\n",
            "Epoch 9 - loss is 0.713879 : \n",
            "Roc 0.8601461038961039\n",
            "Epoch 10 - loss is 0.692521 : \n",
            "Roc 0.8635140250374624\n",
            "Epoch 11 - loss is 0.685318 : \n",
            "Roc 0.8632529579795205\n",
            "Epoch 12 - loss is 0.677770 : \n",
            "Roc 0.874413086913087\n",
            "Epoch 13 - loss is 0.677477 : \n",
            "Roc 0.8709576556256244\n",
            "Epoch 14 - loss is 0.661441 : \n",
            "Roc 0.8777229606331169\n",
            "Epoch 15 - loss is 0.656077 : \n",
            "Roc 0.8754394043456544\n",
            "Epoch 16 - loss is 0.650602 : \n",
            "Roc 0.8763726507867133\n",
            "Epoch 17 - loss is 0.650650 : \n",
            "Roc 0.8794252622377623\n",
            "Epoch 18 - loss is 0.647875 : \n",
            "Roc 0.8771811196615884\n",
            "Epoch 19 - loss is 0.644302 : \n",
            "Roc 0.8808976180069931\n",
            "Time mean 0.6567178130149841\n",
            "Time std 0.04403581462008711\n",
            "test\n",
            "Roc 0.8808976180069931\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.009265 : \n",
            "Roc 0.8097483825005694\n",
            "Epoch 1 - loss is 0.805284 : \n",
            "Roc 0.7827328422647115\n",
            "Epoch 2 - loss is 0.831943 : \n",
            "Roc 0.813854430672261\n",
            "Epoch 3 - loss is 0.792897 : \n",
            "Roc 0.8014517627298319\n",
            "Epoch 4 - loss is 0.801247 : \n",
            "Roc 0.8076042771352471\n",
            "Epoch 5 - loss is 0.812193 : \n",
            "Roc 0.7823147011222463\n",
            "Epoch 6 - loss is 0.773643 : \n",
            "Roc 0.8287717746713683\n",
            "Epoch 7 - loss is 0.776437 : \n",
            "Roc 0.8378796944332372\n",
            "Epoch 8 - loss is 0.766602 : \n",
            "Roc 0.7699580422375643\n",
            "Epoch 9 - loss is 0.721746 : \n",
            "Roc 0.8542990988470255\n",
            "Epoch 10 - loss is 0.699223 : \n",
            "Roc 0.8632102830452251\n",
            "Epoch 11 - loss is 0.684835 : \n",
            "Roc 0.8714683884463553\n",
            "Epoch 12 - loss is 0.676155 : \n",
            "Roc 0.8740746662469641\n",
            "Epoch 13 - loss is 0.670347 : \n",
            "Roc 0.8686195742101119\n",
            "Epoch 14 - loss is 0.660800 : \n",
            "Roc 0.8756555249800038\n",
            "Epoch 15 - loss is 0.660433 : \n",
            "Roc 0.8748522401414663\n",
            "Epoch 16 - loss is 0.655568 : \n",
            "Roc 0.8742899303452011\n",
            "Epoch 17 - loss is 0.654851 : \n",
            "Roc 0.8768783041685871\n",
            "Epoch 18 - loss is 0.649592 : \n",
            "Roc 0.8727033465031659\n",
            "Epoch 19 - loss is 0.638007 : \n",
            "Roc 0.8794521049872569\n",
            "Time mean 0.6636814594268798\n",
            "Time std 0.05442143728583975\n",
            "test\n",
            "Roc 0.8794521049872569\n",
            "test\n",
            "Crossvalidation run 9\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.950170 : \n",
            "Roc 0.8189435216237821\n",
            "Epoch 1 - loss is 0.808646 : \n",
            "Roc 0.8103677189529369\n",
            "Epoch 2 - loss is 0.794691 : \n",
            "Roc 0.7706372638009065\n",
            "Epoch 3 - loss is 0.789034 : \n",
            "Roc 0.8287668042537334\n",
            "Epoch 4 - loss is 0.780782 : \n",
            "Roc 0.8488715949589629\n",
            "Epoch 5 - loss is 0.769407 : \n",
            "Roc 0.84151042811235\n",
            "Epoch 6 - loss is 0.770860 : \n",
            "Roc 0.8310366325565397\n",
            "Epoch 7 - loss is 0.763400 : \n",
            "Roc 0.8230264456992894\n",
            "Epoch 8 - loss is 0.769222 : \n",
            "Roc 0.8084924183793389\n",
            "Epoch 9 - loss is 0.722219 : \n",
            "Roc 0.8760973922155022\n",
            "Epoch 10 - loss is 0.700794 : \n",
            "Roc 0.8809523525159005\n",
            "Epoch 11 - loss is 0.692788 : \n",
            "Roc 0.8813200077711214\n",
            "Epoch 12 - loss is 0.686024 : \n",
            "Roc 0.8790046953178994\n",
            "Epoch 13 - loss is 0.675918 : \n",
            "Roc 0.8841290770519422\n",
            "Epoch 14 - loss is 0.670888 : \n",
            "Roc 0.8868127414541551\n",
            "Epoch 15 - loss is 0.668825 : \n",
            "Roc 0.8914240579904007\n",
            "Epoch 16 - loss is 0.663764 : \n",
            "Roc 0.8910844695413446\n",
            "Epoch 17 - loss is 0.657328 : \n",
            "Roc 0.8954292509467072\n",
            "Epoch 18 - loss is 0.652144 : \n",
            "Roc 0.8958060627486202\n",
            "Epoch 19 - loss is 0.650549 : \n",
            "Roc 0.8943719683868235\n",
            "Time mean 0.6618665933609009\n",
            "Time std 0.038679769897435934\n",
            "test\n",
            "Roc 0.8943719683868235\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.959747 : \n",
            "Roc 0.8218342436651177\n",
            "Epoch 1 - loss is 0.814919 : \n",
            "Roc 0.7978594098996176\n",
            "Epoch 2 - loss is 0.801235 : \n",
            "Roc 0.8158313088880902\n",
            "Epoch 3 - loss is 0.792623 : \n",
            "Roc 0.7771755699468563\n",
            "Epoch 4 - loss is 0.781816 : \n",
            "Roc 0.8060321381350928\n",
            "Epoch 5 - loss is 0.761596 : \n",
            "Roc 0.8074227073759338\n",
            "Epoch 6 - loss is 0.744758 : \n",
            "Roc 0.8173275798439064\n",
            "Epoch 7 - loss is 0.759143 : \n",
            "Roc 0.7958536847577725\n",
            "Epoch 8 - loss is 0.763243 : \n",
            "Roc 0.7811781950399219\n",
            "Epoch 9 - loss is 0.712679 : \n",
            "Roc 0.8401342792867962\n",
            "Epoch 10 - loss is 0.686041 : \n",
            "Roc 0.8473226979089113\n",
            "Epoch 11 - loss is 0.678512 : \n",
            "Roc 0.8503947668224179\n",
            "Epoch 12 - loss is 0.674522 : \n",
            "Roc 0.8603441663885394\n",
            "Epoch 13 - loss is 0.661407 : \n",
            "Roc 0.8655321750404356\n",
            "Epoch 14 - loss is 0.654564 : \n",
            "Roc 0.8628986579291932\n",
            "Epoch 15 - loss is 0.653047 : \n",
            "Roc 0.8636877101999949\n",
            "Epoch 16 - loss is 0.652134 : \n",
            "Roc 0.8669299648917872\n",
            "Epoch 17 - loss is 0.639617 : \n",
            "Roc 0.8641203451130908\n",
            "Epoch 18 - loss is 0.644755 : \n",
            "Roc 0.8617920993684373\n",
            "Epoch 19 - loss is 0.655210 : \n",
            "Roc 0.8708463438037534\n",
            "Time mean 0.6727213501930237\n",
            "Time std 0.08107576210130925\n",
            "test\n",
            "Roc 0.8708463438037534\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.972535 : \n",
            "Roc 0.8286857951845414\n",
            "Epoch 1 - loss is 0.795648 : \n",
            "Roc 0.8341462078348698\n",
            "Epoch 2 - loss is 0.788193 : \n",
            "Roc 0.8189790501714589\n",
            "Epoch 3 - loss is 0.765605 : \n",
            "Roc 0.8162455622911458\n",
            "Epoch 4 - loss is 0.780372 : \n",
            "Roc 0.7629540578474443\n",
            "Epoch 5 - loss is 0.783732 : \n",
            "Roc 0.8138881691661655\n",
            "Epoch 6 - loss is 0.750761 : \n",
            "Roc 0.8252613734896761\n",
            "Epoch 7 - loss is 0.750054 : \n",
            "Roc 0.8202237583070905\n",
            "Epoch 8 - loss is 0.754104 : \n",
            "Roc 0.8270547195991976\n",
            "Epoch 9 - loss is 0.714076 : \n",
            "Roc 0.8562826276943469\n",
            "Epoch 10 - loss is 0.683729 : \n",
            "Roc 0.8592601871837104\n",
            "Epoch 11 - loss is 0.671299 : \n",
            "Roc 0.8617836555344902\n",
            "Epoch 12 - loss is 0.661658 : \n",
            "Roc 0.8725390714321315\n",
            "Epoch 13 - loss is 0.662047 : \n",
            "Roc 0.8681163911491988\n",
            "Epoch 14 - loss is 0.652348 : \n",
            "Roc 0.8730704931311819\n",
            "Epoch 15 - loss is 0.650268 : \n",
            "Roc 0.8730868533481564\n",
            "Epoch 16 - loss is 0.651905 : \n",
            "Roc 0.8653123260537978\n",
            "Epoch 17 - loss is 0.644030 : \n",
            "Roc 0.8732902146498513\n",
            "Epoch 18 - loss is 0.640139 : \n",
            "Roc 0.8747995397832917\n",
            "Epoch 19 - loss is 0.632809 : \n",
            "Roc 0.8808800236804629\n",
            "Time mean 0.6938362836837768\n",
            "Time std 0.11718749813358104\n",
            "test\n",
            "Roc 0.8808800236804629\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.968649 : \n",
            "Roc 0.8140349604828381\n",
            "Epoch 1 - loss is 0.803351 : \n",
            "Roc 0.7875300024607326\n",
            "Epoch 2 - loss is 0.776388 : \n",
            "Roc 0.7864722019781774\n",
            "Epoch 3 - loss is 0.798861 : \n",
            "Roc 0.784584592198456\n",
            "Epoch 4 - loss is 0.803703 : \n",
            "Roc 0.8060721717889997\n",
            "Epoch 5 - loss is 0.792445 : \n",
            "Roc 0.7995333452832162\n",
            "Epoch 6 - loss is 0.766353 : \n",
            "Roc 0.7989402733850166\n",
            "Epoch 7 - loss is 0.756971 : \n",
            "Roc 0.8284074461921148\n",
            "Epoch 8 - loss is 0.748525 : \n",
            "Roc 0.8102023500387978\n",
            "Epoch 9 - loss is 0.712306 : \n",
            "Roc 0.8462139508592516\n",
            "Epoch 10 - loss is 0.685755 : \n",
            "Roc 0.8538134277374076\n",
            "Epoch 11 - loss is 0.677871 : \n",
            "Roc 0.8585066066342291\n",
            "Epoch 12 - loss is 0.671481 : \n",
            "Roc 0.8527132719958427\n",
            "Epoch 13 - loss is 0.668519 : \n",
            "Roc 0.8650291199776097\n",
            "Epoch 14 - loss is 0.656682 : \n",
            "Roc 0.8641493884726086\n",
            "Epoch 15 - loss is 0.654011 : \n",
            "Roc 0.868258733045121\n",
            "Epoch 16 - loss is 0.650514 : \n",
            "Roc 0.8705268522514916\n",
            "Epoch 17 - loss is 0.638653 : \n",
            "Roc 0.8717169269303975\n",
            "Epoch 18 - loss is 0.639586 : \n",
            "Roc 0.867902398545888\n",
            "Epoch 19 - loss is 0.637492 : \n",
            "Roc 0.8729220372348917\n",
            "Time mean 0.6820915818214417\n",
            "Time std 0.09727091944882725\n",
            "test\n",
            "Roc 0.8729220372348917\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.044421 : \n",
            "Roc 0.7935814303166995\n",
            "Epoch 1 - loss is 0.803185 : \n",
            "Roc 0.7962487112473017\n",
            "Epoch 2 - loss is 0.795040 : \n",
            "Roc 0.7750115711773451\n",
            "Epoch 3 - loss is 0.778821 : \n",
            "Roc 0.8185959809867338\n",
            "Epoch 4 - loss is 0.777634 : \n",
            "Roc 0.807250365891471\n",
            "Epoch 5 - loss is 0.761302 : \n",
            "Roc 0.8289389235258985\n",
            "Epoch 6 - loss is 0.759267 : \n",
            "Roc 0.8161811265842841\n",
            "Epoch 7 - loss is 0.757915 : \n",
            "Roc 0.7865947641814571\n",
            "Epoch 8 - loss is 0.763315 : \n",
            "Roc 0.8335316405516574\n",
            "Epoch 9 - loss is 0.713877 : \n",
            "Roc 0.8631989865426573\n",
            "Epoch 10 - loss is 0.696793 : \n",
            "Roc 0.8657418125983112\n",
            "Epoch 11 - loss is 0.689342 : \n",
            "Roc 0.8679208474810393\n",
            "Epoch 12 - loss is 0.669592 : \n",
            "Roc 0.8735666442178989\n",
            "Epoch 13 - loss is 0.663301 : \n",
            "Roc 0.8732534555765455\n",
            "Epoch 14 - loss is 0.658060 : \n",
            "Roc 0.8775723523397614\n",
            "Epoch 15 - loss is 0.654012 : \n",
            "Roc 0.8761202159848903\n",
            "Epoch 16 - loss is 0.646227 : \n",
            "Roc 0.8749057552271535\n",
            "Epoch 17 - loss is 0.643291 : \n",
            "Roc 0.8786029637453376\n",
            "Epoch 18 - loss is 0.639057 : \n",
            "Roc 0.8831649089613567\n",
            "Epoch 19 - loss is 0.637827 : \n",
            "Roc 0.8863290325089028\n",
            "Time mean 0.6782134652137757\n",
            "Time std 0.10916934021891181\n",
            "test\n",
            "Roc 0.8863290325089028\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.970419 : \n",
            "Roc 0.81454258475672\n",
            "Epoch 1 - loss is 0.788928 : \n",
            "Roc 0.8348023093440704\n",
            "Epoch 2 - loss is 0.782113 : \n",
            "Roc 0.8171465349303615\n",
            "Epoch 3 - loss is 0.768722 : \n",
            "Roc 0.7975475670094798\n",
            "Epoch 4 - loss is 0.770540 : \n",
            "Roc 0.7912512714226334\n",
            "Epoch 5 - loss is 0.770543 : \n",
            "Roc 0.8250304087899121\n",
            "Epoch 6 - loss is 0.769287 : \n",
            "Roc 0.8352335873943832\n",
            "Epoch 7 - loss is 0.769349 : \n",
            "Roc 0.8039980614995029\n",
            "Epoch 8 - loss is 0.769535 : \n",
            "Roc 0.782765159859248\n",
            "Epoch 9 - loss is 0.726769 : \n",
            "Roc 0.8498344037865888\n",
            "Epoch 10 - loss is 0.693078 : \n",
            "Roc 0.8630855487852765\n",
            "Epoch 11 - loss is 0.681258 : \n",
            "Roc 0.8547032580983157\n",
            "Epoch 12 - loss is 0.681368 : \n",
            "Roc 0.8678214664028364\n",
            "Epoch 13 - loss is 0.669847 : \n",
            "Roc 0.8665807877744214\n",
            "Epoch 14 - loss is 0.661555 : \n",
            "Roc 0.8707870656810707\n",
            "Epoch 15 - loss is 0.660272 : \n",
            "Roc 0.8746454861862685\n",
            "Epoch 16 - loss is 0.649418 : \n",
            "Roc 0.8711835771463816\n",
            "Epoch 17 - loss is 0.652265 : \n",
            "Roc 0.8787680561169077\n",
            "Epoch 18 - loss is 0.645888 : \n",
            "Roc 0.8831766123362954\n",
            "Epoch 19 - loss is 0.642386 : \n",
            "Roc 0.882142809254999\n",
            "Time mean 0.6811719179153443\n",
            "Time std 0.0844860094954432\n",
            "test\n",
            "Roc 0.882142809254999\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.988704 : \n",
            "Roc 0.7913087089204732\n",
            "Epoch 1 - loss is 0.790598 : \n",
            "Roc 0.8073425411453836\n",
            "Epoch 2 - loss is 0.817572 : \n",
            "Roc 0.7337456990358546\n",
            "Epoch 3 - loss is 0.784732 : \n",
            "Roc 0.8196048989450733\n",
            "Epoch 4 - loss is 0.788328 : \n",
            "Roc 0.8022732799182969\n",
            "Epoch 5 - loss is 0.787596 : \n",
            "Roc 0.8191554507248316\n",
            "Epoch 6 - loss is 0.762274 : \n",
            "Roc 0.8259261510778246\n",
            "Epoch 7 - loss is 0.770135 : \n",
            "Roc 0.8353151203459415\n",
            "Epoch 8 - loss is 0.757850 : \n",
            "Roc 0.8194538332786214\n",
            "Epoch 9 - loss is 0.708144 : \n",
            "Roc 0.860041763425695\n",
            "Epoch 10 - loss is 0.688135 : \n",
            "Roc 0.8649515495629125\n",
            "Epoch 11 - loss is 0.684434 : \n",
            "Roc 0.8661744621008565\n",
            "Epoch 12 - loss is 0.682310 : \n",
            "Roc 0.8675768094444918\n",
            "Epoch 13 - loss is 0.671663 : \n",
            "Roc 0.873387923662699\n",
            "Epoch 14 - loss is 0.662894 : \n",
            "Roc 0.871858219147548\n",
            "Epoch 15 - loss is 0.660015 : \n",
            "Roc 0.8734323011019384\n",
            "Epoch 16 - loss is 0.656752 : \n",
            "Roc 0.8596167329288704\n",
            "Epoch 17 - loss is 0.655370 : \n",
            "Roc 0.8754921032474559\n",
            "Epoch 18 - loss is 0.643555 : \n",
            "Roc 0.878961851613191\n",
            "Epoch 19 - loss is 0.644759 : \n",
            "Roc 0.8831286702573893\n",
            "Time mean 0.6723188281059265\n",
            "Time std 0.048127084058095314\n",
            "test\n",
            "Roc 0.8831286702573893\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.000614 : \n",
            "Roc 0.8130819360457608\n",
            "Epoch 1 - loss is 0.800051 : \n",
            "Roc 0.8193911697417394\n",
            "Epoch 2 - loss is 0.808038 : \n",
            "Roc 0.8052112896623158\n",
            "Epoch 3 - loss is 0.782454 : \n",
            "Roc 0.8031181547758054\n",
            "Epoch 4 - loss is 0.793295 : \n",
            "Roc 0.8322223652620461\n",
            "Epoch 5 - loss is 0.772710 : \n",
            "Roc 0.8156534148555665\n",
            "Epoch 6 - loss is 0.758977 : \n",
            "Roc 0.8133817963595231\n",
            "Epoch 7 - loss is 0.762239 : \n",
            "Roc 0.8197543186361222\n",
            "Epoch 8 - loss is 0.751750 : \n",
            "Roc 0.8274792541013943\n",
            "Epoch 9 - loss is 0.705745 : \n",
            "Roc 0.8641430647686503\n",
            "Epoch 10 - loss is 0.682544 : \n",
            "Roc 0.8666650901759246\n",
            "Epoch 11 - loss is 0.672261 : \n",
            "Roc 0.86981882392744\n",
            "Epoch 12 - loss is 0.661204 : \n",
            "Roc 0.8706984861371515\n",
            "Epoch 13 - loss is 0.659292 : \n",
            "Roc 0.8761179510748854\n",
            "Epoch 14 - loss is 0.652171 : \n",
            "Roc 0.8715116217458385\n",
            "Epoch 15 - loss is 0.651398 : \n",
            "Roc 0.8820887625777469\n",
            "Epoch 16 - loss is 0.651473 : \n",
            "Roc 0.8828631400634182\n",
            "Epoch 17 - loss is 0.637108 : \n",
            "Roc 0.8788782162864184\n",
            "Epoch 18 - loss is 0.639684 : \n",
            "Roc 0.8852062893738375\n",
            "Epoch 19 - loss is 0.633178 : \n",
            "Roc 0.8882994819559842\n",
            "Time mean 0.6846091032028199\n",
            "Time std 0.04129043846761874\n",
            "test\n",
            "Roc 0.8882994819559842\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.001960 : \n",
            "Roc 0.8197109531093906\n",
            "Epoch 1 - loss is 0.797457 : \n",
            "Roc 0.8176930491383616\n",
            "Epoch 2 - loss is 0.817211 : \n",
            "Roc 0.8038421539398103\n",
            "Epoch 3 - loss is 0.799750 : \n",
            "Roc 0.7856699940684315\n",
            "Epoch 4 - loss is 0.798626 : \n",
            "Roc 0.8104702914273227\n",
            "Epoch 5 - loss is 0.782302 : \n",
            "Roc 0.816003332604895\n",
            "Epoch 6 - loss is 0.771722 : \n",
            "Roc 0.8330563967282718\n",
            "Epoch 7 - loss is 0.767415 : \n",
            "Roc 0.8190485295954045\n",
            "Epoch 8 - loss is 0.765506 : \n",
            "Roc 0.8334481924325675\n",
            "Epoch 9 - loss is 0.718940 : \n",
            "Roc 0.8640625\n",
            "Epoch 10 - loss is 0.695900 : \n",
            "Roc 0.8641619123064435\n",
            "Epoch 11 - loss is 0.682237 : \n",
            "Roc 0.8675851492257742\n",
            "Epoch 12 - loss is 0.681118 : \n",
            "Roc 0.8740868506493507\n",
            "Epoch 13 - loss is 0.666825 : \n",
            "Roc 0.8755892544955045\n",
            "Epoch 14 - loss is 0.662276 : \n",
            "Roc 0.8762036791333666\n",
            "Epoch 15 - loss is 0.658368 : \n",
            "Roc 0.8728882445679321\n",
            "Epoch 16 - loss is 0.647350 : \n",
            "Roc 0.8791505369630369\n",
            "Epoch 17 - loss is 0.645154 : \n",
            "Roc 0.8792647781905594\n",
            "Epoch 18 - loss is 0.635971 : \n",
            "Roc 0.8768729317557443\n",
            "Epoch 19 - loss is 0.635899 : \n",
            "Roc 0.877163266421079\n",
            "Time mean 0.663332176208496\n",
            "Time std 0.031840440577267856\n",
            "test\n",
            "Roc 0.877163266421079\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.946223 : \n",
            "Roc 0.7908723858631861\n",
            "Epoch 1 - loss is 0.812774 : \n",
            "Roc 0.7921002656034203\n",
            "Epoch 2 - loss is 0.796960 : \n",
            "Roc 0.6750550963671165\n",
            "Epoch 3 - loss is 0.798640 : \n",
            "Roc 0.8172830423104293\n",
            "Epoch 4 - loss is 0.793203 : \n",
            "Roc 0.8115210844314092\n",
            "Epoch 5 - loss is 0.774034 : \n",
            "Roc 0.8153054855704354\n",
            "Epoch 6 - loss is 0.761799 : \n",
            "Roc 0.8153877189541912\n",
            "Epoch 7 - loss is 0.761519 : \n",
            "Roc 0.8216801342048823\n",
            "Epoch 8 - loss is 0.756890 : \n",
            "Roc 0.785141864037197\n",
            "Epoch 9 - loss is 0.715150 : \n",
            "Roc 0.8642101993545408\n",
            "Epoch 10 - loss is 0.692220 : \n",
            "Roc 0.8635621170162314\n",
            "Epoch 11 - loss is 0.680721 : \n",
            "Roc 0.8616727269547708\n",
            "Epoch 12 - loss is 0.673958 : \n",
            "Roc 0.8659287729821489\n",
            "Epoch 13 - loss is 0.680176 : \n",
            "Roc 0.866320578623234\n",
            "Epoch 14 - loss is 0.667198 : \n",
            "Roc 0.8726871080121963\n",
            "Epoch 15 - loss is 0.659161 : \n",
            "Roc 0.8730849510409497\n",
            "Epoch 16 - loss is 0.651832 : \n",
            "Roc 0.871637851672627\n",
            "Epoch 17 - loss is 0.646050 : \n",
            "Roc 0.8745620291618315\n",
            "Epoch 18 - loss is 0.644327 : \n",
            "Roc 0.8729086176838832\n",
            "Epoch 19 - loss is 0.650405 : \n",
            "Roc 0.8800552191967277\n",
            "Time mean 0.6851312994956971\n",
            "Time std 0.07398830987331427\n",
            "test\n",
            "Roc 0.8800552191967277\n",
            "test\n",
            "Crossvalidation run 10\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.983482 : \n",
            "Roc 0.8222176837599487\n",
            "Epoch 1 - loss is 0.810913 : \n",
            "Roc 0.8082680829853846\n",
            "Epoch 2 - loss is 0.776876 : \n",
            "Roc 0.8098475872897578\n",
            "Epoch 3 - loss is 0.781734 : \n",
            "Roc 0.7945956668036167\n",
            "Epoch 4 - loss is 0.771545 : \n",
            "Roc 0.8218552039441627\n",
            "Epoch 5 - loss is 0.765446 : \n",
            "Roc 0.7997501058974531\n",
            "Epoch 6 - loss is 0.760343 : \n",
            "Roc 0.836885078173022\n",
            "Epoch 7 - loss is 0.757002 : \n",
            "Roc 0.8297021654038594\n",
            "Epoch 8 - loss is 0.766711 : \n",
            "Roc 0.8088642537971802\n",
            "Epoch 9 - loss is 0.719580 : \n",
            "Roc 0.874537594392053\n",
            "Epoch 10 - loss is 0.690831 : \n",
            "Roc 0.880330105451569\n",
            "Epoch 11 - loss is 0.680486 : \n",
            "Roc 0.880094423901931\n",
            "Epoch 12 - loss is 0.669417 : \n",
            "Roc 0.8912192300220075\n",
            "Epoch 13 - loss is 0.661599 : \n",
            "Roc 0.8755792511059515\n",
            "Epoch 14 - loss is 0.658932 : \n",
            "Roc 0.8848978288633461\n",
            "Epoch 15 - loss is 0.658674 : \n",
            "Roc 0.8856757372214419\n",
            "Epoch 16 - loss is 0.653740 : \n",
            "Roc 0.8968588665628394\n",
            "Epoch 17 - loss is 0.646728 : \n",
            "Roc 0.8882252383090804\n",
            "Epoch 18 - loss is 0.654323 : \n",
            "Roc 0.8927242876206674\n",
            "Epoch 19 - loss is 0.637643 : \n",
            "Roc 0.8993297407821443\n",
            "Time mean 0.6647890090942383\n",
            "Time std 0.03875811222743859\n",
            "test\n",
            "Roc 0.8993297407821443\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.971380 : \n",
            "Roc 0.7922632361813561\n",
            "Epoch 1 - loss is 0.805769 : \n",
            "Roc 0.8067365488049087\n",
            "Epoch 2 - loss is 0.782870 : \n",
            "Roc 0.7764860016430901\n",
            "Epoch 3 - loss is 0.773192 : \n",
            "Roc 0.8032014582424072\n",
            "Epoch 4 - loss is 0.770947 : \n",
            "Roc 0.7764615317963595\n",
            "Epoch 5 - loss is 0.767262 : \n",
            "Roc 0.8002033003003773\n",
            "Epoch 6 - loss is 0.767395 : \n",
            "Roc 0.788604793522631\n",
            "Epoch 7 - loss is 0.755420 : \n",
            "Roc 0.8122492843572694\n",
            "Epoch 8 - loss is 0.750655 : \n",
            "Roc 0.8140562421375575\n",
            "Epoch 9 - loss is 0.710404 : \n",
            "Roc 0.8460555810505506\n",
            "Epoch 10 - loss is 0.679375 : \n",
            "Roc 0.8495880641703679\n",
            "Epoch 11 - loss is 0.679163 : \n",
            "Roc 0.850984149155349\n",
            "Epoch 12 - loss is 0.666477 : \n",
            "Roc 0.8612711122628429\n",
            "Epoch 13 - loss is 0.661360 : \n",
            "Roc 0.8570579863161408\n",
            "Epoch 14 - loss is 0.651572 : \n",
            "Roc 0.8573967532925983\n",
            "Epoch 15 - loss is 0.653536 : \n",
            "Roc 0.8539950434520294\n",
            "Epoch 16 - loss is 0.646618 : \n",
            "Roc 0.8628166237708916\n",
            "Epoch 17 - loss is 0.634730 : \n",
            "Roc 0.869592364445072\n",
            "Epoch 18 - loss is 0.634822 : \n",
            "Roc 0.8570698200944776\n",
            "Epoch 19 - loss is 0.630725 : \n",
            "Roc 0.8658095668275526\n",
            "Time mean 0.6622982501983643\n",
            "Time std 0.03928694137277123\n",
            "test\n",
            "Roc 0.8658095668275526\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.020979 : \n",
            "Roc 0.7899649663074648\n",
            "Epoch 1 - loss is 0.806959 : \n",
            "Roc 0.7776885258188955\n",
            "Epoch 2 - loss is 0.799373 : \n",
            "Roc 0.7811205569170045\n",
            "Epoch 3 - loss is 0.782426 : \n",
            "Roc 0.7705090538582148\n",
            "Epoch 4 - loss is 0.787608 : \n",
            "Roc 0.7809911970618573\n",
            "Epoch 5 - loss is 0.771022 : \n",
            "Roc 0.8121709170890836\n",
            "Epoch 6 - loss is 0.767526 : \n",
            "Roc 0.8108243380675235\n",
            "Epoch 7 - loss is 0.762598 : \n",
            "Roc 0.8290267915674114\n",
            "Epoch 8 - loss is 0.749327 : \n",
            "Roc 0.8213794364247675\n",
            "Epoch 9 - loss is 0.708574 : \n",
            "Roc 0.8550687376418535\n",
            "Epoch 10 - loss is 0.688503 : \n",
            "Roc 0.8653041459453105\n",
            "Epoch 11 - loss is 0.680212 : \n",
            "Roc 0.862710290614524\n",
            "Epoch 12 - loss is 0.675222 : \n",
            "Roc 0.863069264212558\n",
            "Epoch 13 - loss is 0.668982 : \n",
            "Roc 0.869324003444016\n",
            "Epoch 14 - loss is 0.661573 : \n",
            "Roc 0.8712122768590057\n",
            "Epoch 15 - loss is 0.657272 : \n",
            "Roc 0.8701347853596604\n",
            "Epoch 16 - loss is 0.657148 : \n",
            "Roc 0.8717030833682415\n",
            "Epoch 17 - loss is 0.649109 : \n",
            "Roc 0.873043860219828\n",
            "Epoch 18 - loss is 0.647997 : \n",
            "Roc 0.8670198761417434\n",
            "Epoch 19 - loss is 0.652708 : \n",
            "Roc 0.8780613104838172\n",
            "Time mean 0.6673204660415649\n",
            "Time std 0.04561809871923765\n",
            "test\n",
            "Roc 0.8780613104838172\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.003398 : \n",
            "Roc 0.816739800735704\n",
            "Epoch 1 - loss is 0.782900 : \n",
            "Roc 0.5555118244876684\n",
            "Epoch 2 - loss is 0.799304 : \n",
            "Roc 0.7631378937859823\n",
            "Epoch 3 - loss is 0.791615 : \n",
            "Roc 0.7640905431614831\n",
            "Epoch 4 - loss is 0.778920 : \n",
            "Roc 0.7947636124495765\n",
            "Epoch 5 - loss is 0.769590 : \n",
            "Roc 0.7772328591902854\n",
            "Epoch 6 - loss is 0.771837 : \n",
            "Roc 0.8108409219020125\n",
            "Epoch 7 - loss is 0.758836 : \n",
            "Roc 0.7929054842886556\n",
            "Epoch 8 - loss is 0.766029 : \n",
            "Roc 0.8123550978435964\n",
            "Epoch 9 - loss is 0.712916 : \n",
            "Roc 0.8536266125462763\n",
            "Epoch 10 - loss is 0.687787 : \n",
            "Roc 0.8540210766058245\n",
            "Epoch 11 - loss is 0.685734 : \n",
            "Roc 0.8570063853255439\n",
            "Epoch 12 - loss is 0.677797 : \n",
            "Roc 0.8612747340954527\n",
            "Epoch 13 - loss is 0.666694 : \n",
            "Roc 0.8617020210239319\n",
            "Epoch 14 - loss is 0.662884 : \n",
            "Roc 0.8590836601862137\n",
            "Epoch 15 - loss is 0.667132 : \n",
            "Roc 0.8651043963776132\n",
            "Epoch 16 - loss is 0.660269 : \n",
            "Roc 0.8693863066921702\n",
            "Epoch 17 - loss is 0.649877 : \n",
            "Roc 0.8735253294276084\n",
            "Epoch 18 - loss is 0.644586 : \n",
            "Roc 0.8726721313795275\n",
            "Epoch 19 - loss is 0.645878 : \n",
            "Roc 0.8717501428875792\n",
            "Time mean 0.6731295824050904\n",
            "Time std 0.05441440677757832\n",
            "test\n",
            "Roc 0.8717501428875792\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.999692 : \n",
            "Roc 0.7754457956036757\n",
            "Epoch 1 - loss is 0.813556 : \n",
            "Roc 0.7930054601889955\n",
            "Epoch 2 - loss is 0.795446 : \n",
            "Roc 0.8180763769994105\n",
            "Epoch 3 - loss is 0.781337 : \n",
            "Roc 0.7746343967093896\n",
            "Epoch 4 - loss is 0.769606 : \n",
            "Roc 0.795894493526295\n",
            "Epoch 5 - loss is 0.769161 : \n",
            "Roc 0.8347902564141639\n",
            "Epoch 6 - loss is 0.760420 : \n",
            "Roc 0.7865645785014266\n",
            "Epoch 7 - loss is 0.759451 : \n",
            "Roc 0.8186661211752838\n",
            "Epoch 8 - loss is 0.764038 : \n",
            "Roc 0.8139118277263186\n",
            "Epoch 9 - loss is 0.715705 : \n",
            "Roc 0.859023398493373\n",
            "Epoch 10 - loss is 0.693602 : \n",
            "Roc 0.8666028371413205\n",
            "Epoch 11 - loss is 0.679363 : \n",
            "Roc 0.8670209430006443\n",
            "Epoch 12 - loss is 0.678529 : \n",
            "Roc 0.8767310608302996\n",
            "Epoch 13 - loss is 0.666288 : \n",
            "Roc 0.8724486994856321\n",
            "Epoch 14 - loss is 0.666226 : \n",
            "Roc 0.8689542918468771\n",
            "Epoch 15 - loss is 0.663542 : \n",
            "Roc 0.8793391426602591\n",
            "Epoch 16 - loss is 0.651884 : \n",
            "Roc 0.8794646721063412\n",
            "Epoch 17 - loss is 0.649028 : \n",
            "Roc 0.8855870899851729\n",
            "Epoch 18 - loss is 0.648007 : \n",
            "Roc 0.8853365195344333\n",
            "Epoch 19 - loss is 0.639470 : \n",
            "Roc 0.8711322521584716\n",
            "Time mean 0.6728394627571106\n",
            "Time std 0.053883395658964287\n",
            "test\n",
            "Roc 0.8711322521584716\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.952729 : \n",
            "Roc 0.8312717291156262\n",
            "Epoch 1 - loss is 0.777427 : \n",
            "Roc 0.8219625210467137\n",
            "Epoch 2 - loss is 0.767928 : \n",
            "Roc 0.8257429801188768\n",
            "Epoch 3 - loss is 0.775039 : \n",
            "Roc 0.8115798587499736\n",
            "Epoch 4 - loss is 0.765369 : \n",
            "Roc 0.8342302409908572\n",
            "Epoch 5 - loss is 0.771743 : \n",
            "Roc 0.8111509750925673\n",
            "Epoch 6 - loss is 0.763845 : \n",
            "Roc 0.8073287578081153\n",
            "Epoch 7 - loss is 0.758380 : \n",
            "Roc 0.8193232296337728\n",
            "Epoch 8 - loss is 0.759689 : \n",
            "Roc 0.8319839173417258\n",
            "Epoch 9 - loss is 0.708048 : \n",
            "Roc 0.8602343057122552\n",
            "Epoch 10 - loss is 0.687139 : \n",
            "Roc 0.8641040277519716\n",
            "Epoch 11 - loss is 0.678730 : \n",
            "Roc 0.8693172915393648\n",
            "Epoch 12 - loss is 0.678224 : \n",
            "Roc 0.8714312531486267\n",
            "Epoch 13 - loss is 0.668558 : \n",
            "Roc 0.8740857729004528\n",
            "Epoch 14 - loss is 0.659290 : \n",
            "Roc 0.8679507636197857\n",
            "Epoch 15 - loss is 0.651401 : \n",
            "Roc 0.8753400516805765\n",
            "Epoch 16 - loss is 0.650463 : \n",
            "Roc 0.8800344026372802\n",
            "Epoch 17 - loss is 0.645178 : \n",
            "Roc 0.8813448059871316\n",
            "Epoch 18 - loss is 0.641665 : \n",
            "Roc 0.8825031174995642\n",
            "Epoch 19 - loss is 0.637786 : \n",
            "Roc 0.8821182906716368\n",
            "Time mean 0.6709365606307983\n",
            "Time std 0.07087817811757431\n",
            "test\n",
            "Roc 0.8821182906716368\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.971748 : \n",
            "Roc 0.8330682845181502\n",
            "Epoch 1 - loss is 0.796529 : \n",
            "Roc 0.8077190401504375\n",
            "Epoch 2 - loss is 0.813389 : \n",
            "Roc 0.8229785975108108\n",
            "Epoch 3 - loss is 0.792644 : \n",
            "Roc 0.7875121075433541\n",
            "Epoch 4 - loss is 0.788588 : \n",
            "Roc 0.8199339809602548\n",
            "Epoch 5 - loss is 0.774348 : \n",
            "Roc 0.8209208217323818\n",
            "Epoch 6 - loss is 0.763363 : \n",
            "Roc 0.8181188626406806\n",
            "Epoch 7 - loss is 0.771867 : \n",
            "Roc 0.7716438295095787\n",
            "Epoch 8 - loss is 0.759301 : \n",
            "Roc 0.8365795734092005\n",
            "Epoch 9 - loss is 0.715732 : \n",
            "Roc 0.858860188776358\n",
            "Epoch 10 - loss is 0.691099 : \n",
            "Roc 0.8628593255439783\n",
            "Epoch 11 - loss is 0.681743 : \n",
            "Roc 0.8624046087693062\n",
            "Epoch 12 - loss is 0.673138 : \n",
            "Roc 0.8722797035019676\n",
            "Epoch 13 - loss is 0.684265 : \n",
            "Roc 0.8652364567755636\n",
            "Epoch 14 - loss is 0.661424 : \n",
            "Roc 0.8604303193149259\n",
            "Epoch 15 - loss is 0.652959 : \n",
            "Roc 0.8736031238475037\n",
            "Epoch 16 - loss is 0.647612 : \n",
            "Roc 0.8785154429435819\n",
            "Epoch 17 - loss is 0.648385 : \n",
            "Roc 0.8839296931674956\n",
            "Epoch 18 - loss is 0.650002 : \n",
            "Roc 0.8787892051372458\n",
            "Epoch 19 - loss is 0.643027 : \n",
            "Roc 0.8677838028425877\n",
            "Time mean 0.6639854550361634\n",
            "Time std 0.04529482758615801\n",
            "test\n",
            "Roc 0.8677838028425877\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.993098 : \n",
            "Roc 0.7183436407045461\n",
            "Epoch 1 - loss is 0.813575 : \n",
            "Roc 0.8261163418768664\n",
            "Epoch 2 - loss is 0.797082 : \n",
            "Roc 0.7365752655274853\n",
            "Epoch 3 - loss is 0.802766 : \n",
            "Roc 0.794431036641439\n",
            "Epoch 4 - loss is 0.786622 : \n",
            "Roc 0.8059842934423611\n",
            "Epoch 5 - loss is 0.766985 : \n",
            "Roc 0.8327228847190163\n",
            "Epoch 6 - loss is 0.774520 : \n",
            "Roc 0.7917147300099339\n",
            "Epoch 7 - loss is 0.764795 : \n",
            "Roc 0.8244706425370694\n",
            "Epoch 8 - loss is 0.757251 : \n",
            "Roc 0.831637461030915\n",
            "Epoch 9 - loss is 0.717733 : \n",
            "Roc 0.8600494220035457\n",
            "Epoch 10 - loss is 0.690472 : \n",
            "Roc 0.8688048329318843\n",
            "Epoch 11 - loss is 0.683988 : \n",
            "Roc 0.8655532717154404\n",
            "Epoch 12 - loss is 0.682727 : \n",
            "Roc 0.8719311906927126\n",
            "Epoch 13 - loss is 0.671808 : \n",
            "Roc 0.8705176476036882\n",
            "Epoch 14 - loss is 0.667942 : \n",
            "Roc 0.876072716911062\n",
            "Epoch 15 - loss is 0.659425 : \n",
            "Roc 0.8738692930871601\n",
            "Epoch 16 - loss is 0.664651 : \n",
            "Roc 0.8802105144998554\n",
            "Epoch 17 - loss is 0.657997 : \n",
            "Roc 0.8728144834101497\n",
            "Epoch 18 - loss is 0.643440 : \n",
            "Roc 0.8789194274551947\n",
            "Epoch 19 - loss is 0.638601 : \n",
            "Roc 0.8811836868139566\n",
            "Time mean 0.6692429184913635\n",
            "Time std 0.06990896373856831\n",
            "test\n",
            "Roc 0.8811836868139566\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.970502 : \n",
            "Roc 0.7668056552822178\n",
            "Epoch 1 - loss is 0.818774 : \n",
            "Roc 0.7897647274600399\n",
            "Epoch 2 - loss is 0.811244 : \n",
            "Roc 0.7988708557067933\n",
            "Epoch 3 - loss is 0.788011 : \n",
            "Roc 0.8355265827922077\n",
            "Epoch 4 - loss is 0.774616 : \n",
            "Roc 0.8185085422390108\n",
            "Epoch 5 - loss is 0.756546 : \n",
            "Roc 0.8127368725024975\n",
            "Epoch 6 - loss is 0.758355 : \n",
            "Roc 0.8168794096528472\n",
            "Epoch 7 - loss is 0.756930 : \n",
            "Roc 0.8154698816808192\n",
            "Epoch 8 - loss is 0.762142 : \n",
            "Roc 0.8316622830294705\n",
            "Epoch 9 - loss is 0.707129 : \n",
            "Roc 0.8617140671828172\n",
            "Epoch 10 - loss is 0.685604 : \n",
            "Roc 0.8629257461288711\n",
            "Epoch 11 - loss is 0.680610 : \n",
            "Roc 0.8690694071553446\n",
            "Epoch 12 - loss is 0.674323 : \n",
            "Roc 0.8707614650974027\n",
            "Epoch 13 - loss is 0.665680 : \n",
            "Roc 0.868258206637113\n",
            "Epoch 14 - loss is 0.663047 : \n",
            "Roc 0.8770889266983016\n",
            "Epoch 15 - loss is 0.661068 : \n",
            "Roc 0.8749367819680319\n",
            "Epoch 16 - loss is 0.657552 : \n",
            "Roc 0.8732864791458542\n",
            "Epoch 17 - loss is 0.646020 : \n",
            "Roc 0.8780303680694306\n",
            "Epoch 18 - loss is 0.652922 : \n",
            "Roc 0.8739830482017983\n",
            "Epoch 19 - loss is 0.654069 : \n",
            "Roc 0.8794831145417081\n",
            "Time mean 0.6686457991600037\n",
            "Time std 0.07963993575482498\n",
            "test\n",
            "Roc 0.8794831145417081\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "atrous self run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.998824 : \n",
            "Roc 0.5563065510652658\n",
            "Epoch 1 - loss is 0.806770 : \n",
            "Roc 0.8307460004388556\n",
            "Epoch 2 - loss is 0.789999 : \n",
            "Roc 0.7982023990496736\n",
            "Epoch 3 - loss is 0.790896 : \n",
            "Roc 0.8096109798846735\n",
            "Epoch 4 - loss is 0.776204 : \n",
            "Roc 0.7996986302624681\n",
            "Epoch 5 - loss is 0.760136 : \n",
            "Roc 0.8029785972525305\n",
            "Epoch 6 - loss is 0.774840 : \n",
            "Roc 0.7977762427546142\n",
            "Epoch 7 - loss is 0.753243 : \n",
            "Roc 0.8021361734870411\n",
            "Epoch 8 - loss is 0.758107 : \n",
            "Roc 0.8163593219972012\n",
            "Epoch 9 - loss is 0.715072 : \n",
            "Roc 0.8578576184129499\n",
            "Epoch 10 - loss is 0.687794 : \n",
            "Roc 0.862426463577273\n",
            "Epoch 11 - loss is 0.682654 : \n",
            "Roc 0.8708454965876268\n",
            "Epoch 12 - loss is 0.671242 : \n",
            "Roc 0.8665772716920216\n",
            "Epoch 13 - loss is 0.661918 : \n",
            "Roc 0.8730445629993077\n",
            "Epoch 14 - loss is 0.651040 : \n",
            "Roc 0.8741510704288336\n",
            "Epoch 15 - loss is 0.650311 : \n",
            "Roc 0.8742674462807818\n",
            "Epoch 16 - loss is 0.645179 : \n",
            "Roc 0.8612358490919562\n",
            "Epoch 17 - loss is 0.638750 : \n",
            "Roc 0.879656959796411\n",
            "Epoch 18 - loss is 0.635463 : \n",
            "Roc 0.8727270796822754\n",
            "Epoch 19 - loss is 0.635613 : \n",
            "Roc 0.8718187651168899\n",
            "Time mean 0.6868874192237854\n",
            "Time std 0.11469509378375177\n",
            "test\n",
            "Roc 0.8718187651168899\n",
            "test\n",
            "00:38:43.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkSmTMxnBx_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def open_crossval_results(folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-only_ab-seq\", num_results=NUM_ITERATIONS,\n",
        "                          loop_filter=None, flatten_by_lengths=True):\n",
        "    class_probabilities = []\n",
        "    labels = []\n",
        "    class_probabilities1 = []\n",
        "    labels1 = []\n",
        "    for r in range(num_results):\n",
        "        result_filename = \"{}/run-{}.p\".format(folder, r)\n",
        "        with open(result_filename, \"rb\") as f:\n",
        "            lbl_mat, prob_mat, mask_mat, all_lbls, all_probs = pickle.load(f)\n",
        "            lbl_mat = lbl_mat.data.cpu().numpy()\n",
        "            prob_mat = prob_mat.data.cpu().numpy()\n",
        "            mask_mat = mask_mat.data.cpu().numpy()\n",
        "        # Get entries corresponding to the given loop\n",
        "        if loop_filter is not None:\n",
        "            lbl_mat = lbl_mat[loop_filter::6]\n",
        "            prob_mat = prob_mat[loop_filter::6]\n",
        "            mask_mat = mask_mat[loop_filter::6]\n",
        "        \"\"\"\"\"\n",
        "        if not flatten_by_lengths:\n",
        "            class_probabilities.append(prob_mat)\n",
        "            labels.append(lbl_mat)\n",
        "            continue\n",
        "        \"\"\"\n",
        "        # Discard sequence padding\n",
        "        seq_lens = np.sum(np.squeeze(mask_mat), axis=1)\n",
        "        seq_lens = seq_lens.astype(int)\n",
        "        p = flatten_with_lengths(prob_mat, seq_lens)\n",
        "        l = flatten_with_lengths(lbl_mat, seq_lens)\n",
        "        class_probabilities.append(p)\n",
        "        labels.append(l)\n",
        "        #class_probabilities1 = np.concatenate((class_probabilities1, all_probs))\n",
        "        #labels1 = np.concatenate((labels1,all_lbls))\n",
        "        class_probabilities1.append(all_probs)\n",
        "        labels1.append(all_lbls)\n",
        "    return labels, class_probabilities, labels1, class_probabilities1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZPP7m6XMTK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_cv_results():\n",
        "    \"\"\"\n",
        "    Plots PR curves, output computed metrics\n",
        "    :return:void\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
        "\n",
        "    # Plot ROC per loop type\n",
        "    fig = None\n",
        "    cols = [(\"#D6083B\", \"#EB99A9\"),\n",
        "            (\"#0072CF\", \"#68ACE5\"),\n",
        "            (\"#EA7125\", \"#F3BD48\"),\n",
        "            (\"#55A51C\", \"#AAB300\"),\n",
        "            (\"#8F2BBC\", \"#AF95A3\"),\n",
        "            (\"#00B1C1\", \"#91B9A4\")]\n",
        "\n",
        "\n",
        "    # Plot PR curves\n",
        "    print(\"Plotting PR curves\")\n",
        "    labels, probs, labels1, probs1 = open_crossval_results(\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-only_ab-seq\", NUM_ITERATIONS)\n",
        "\n",
        "    #labels, probs = initial_open_crossval_results(\"parapred-cv-ab-seq\", NUM_ITERATIONS)\n",
        "    #selflabels, selfprobs, selflabels1, selfprobs1 = open_crossval_results(\"self-cv-ab-seq\", NUM_ITERATIONS)\n",
        "    #_,_,aglabels, agprobs = open_crossval_results(\"ag-cv-ab-seq\", NUM_ITERATIONS)\n",
        "\n",
        "\n",
        "    fig1 = plot_pr_curve(labels1, probs1, colours=(\"#0072CF\", \"#68ACE5\"),label=\"Parapred\")\n",
        "\n",
        "    #fig1 = plot_abip_pr(fig1)\n",
        "    #fig1 = plot_pr_curve(selflabels1, selfprobs1, colours=(\"#228B18\", \"#006400\"), label=\"Fast-Parapred\", plot_fig=fig1)\n",
        "    #fig1 = plot_pr_curve(aglabels, agprobs, colours=(\"#FF8C00\", \"#FFA500\"), label=\"AG-Fast-Parapred\", plot_fig=fig1)\n",
        "    #fig1.savefig(\"pr1.pdf\")\n",
        "\n",
        "    print(\"Printing PDB for visualisation\")\n",
        "    if visualisation_flag:\n",
        "        print_probabilities()\n",
        "\n",
        "    # Computing overall classifier metrics\n",
        "    print(\"Computing classifier metrics\")\n",
        "    initial_compute_classifier_metrics(labels, probs, threshold=0.4913739)\n",
        "\n",
        "#run_cv()\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ufcjl2MTvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, class_probabilities, labels1, class_probabilities1 = open_crossval_results(folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-only_ab-seq\", num_results=NUM_ITERATIONS,\n",
        "                          loop_filter=None, flatten_by_lengths=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27uSYZhbMUAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def helper_compute_metrics(matrices, aucs, mcorrs):\n",
        "    matrices = np.stack(matrices)\n",
        "    mean_conf = np.mean(matrices, axis=0)\n",
        "    errs_conf = 2 * np.std(matrices, axis=0)\n",
        "\n",
        "    tps = matrices[:, 1, 1]\n",
        "    fns = matrices[:, 1, 0]\n",
        "    fps = matrices[:, 0, 1]\n",
        "\n",
        "    tpsf = tps.astype(float)\n",
        "    fnsf = fns.astype(float)\n",
        "    fpsf = fps.astype(float)\n",
        "\n",
        "    recalls = tpsf / (tpsf + fnsf)\n",
        "    precisions = tpsf / (tpsf + fpsf)\n",
        "\n",
        "    rec = np.mean(recalls)\n",
        "    rec_err = 2 * np.std(recalls)\n",
        "    prec = np.mean(precisions)\n",
        "    prec_err = 2 * np.std(precisions)\n",
        "\n",
        "    fscores = 2 * precisions * recalls / (precisions + recalls)\n",
        "    fsc = np.mean(fscores)\n",
        "    fsc_err = 2 * np.std(fscores)\n",
        "\n",
        "    auc_scores = np.array(aucs)\n",
        "    auc = np.mean(auc_scores)\n",
        "    auc_err = 2 * np.std(auc_scores)\n",
        "\n",
        "    mcorr_scores = np.array(mcorrs)\n",
        "    mcorr = np.mean(mcorr_scores)\n",
        "    mcorr_err = 2 * np.std(mcorr_scores)\n",
        "\n",
        "    print(\"Mean confusion matrix and error\")\n",
        "    print(mean_conf)\n",
        "    print(errs_conf)\n",
        "\n",
        "    print(\"Recall = {} +/- {}\".format(rec, rec_err))\n",
        "    print(\"Precision = {} +/- {}\".format(prec, prec_err))\n",
        "    print(\"F-score = {} +/- {}\".format(fsc, fsc_err))\n",
        "    print(\"ROC AUC = {} +/- {}\".format(auc, auc_err))\n",
        "    #print(\"ROC AUC - original = {} +/- {}\".format(auc2, auc_err2))\n",
        "    # print(\"ROC AUC - concatenated and iterated = {} +/- {}\".format(auc3, auc_err3))\n",
        "    print(\"MCC = {} +/- {}\".format(mcorr, mcorr_err))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05X1bs-1MUVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_classifier_metrics(labels, probs, labels1, probs1, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Computes metric: precision, recall, mcc, f1\n",
        "    :param labels: ground truth\n",
        "    :param probs: predicted values\n",
        "    :param labels1:\n",
        "    :param probs1:\n",
        "    :param threshold: binding/non-binding threshold\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    matrices = []\n",
        "    matrices1 = []\n",
        "\n",
        "    aucs1 = []\n",
        "    aucs2 = []\n",
        "\n",
        "    mcorrs = []\n",
        "    mcorrs1 = []\n",
        "\n",
        "    #print(\"labels\", labels)\n",
        "    #print(\"probs\", probs)\n",
        "\n",
        "    for l, p in zip(labels, probs):\n",
        "        #print(\"l\", l)\n",
        "        #print(\"p\", p)\n",
        "        aucs2.append(roc_auc_score(l, p))\n",
        "        l_pred = (p > threshold).astype(int)\n",
        "        matrices.append(confusion_matrix(l, l_pred))\n",
        "        mcorrs.append(matthews_corrcoef(l, l_pred))\n",
        "\n",
        "    for l1, p1 in zip(labels1, probs1):\n",
        "        #print(\"in for\")\n",
        "        #print(\"l1\", l1)\n",
        "        #print(\"p1\", p1)\n",
        "        aucs1.append(roc_auc_score(l1, p1))\n",
        "        l_pred1 = (p1 > threshold).astype(int)\n",
        "        matrices1.append(confusion_matrix(l1, l_pred1))\n",
        "        mcorrs1.append(matthews_corrcoef(l1, l_pred1))\n",
        "\n",
        "    print(\"Metrics with the original version\")\n",
        "    helper_compute_metrics(matrices=matrices,  aucs=aucs2, mcorrs =mcorrs)\n",
        "    print(\"Metrics with probabilities concatenated\")\n",
        "    helper_compute_metrics(matrices=matrices1, aucs=aucs1, mcorrs=mcorrs1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvMUl4-xMtIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6c9f96db-cf30-4178-fd0d-00385c8d65b1"
      },
      "source": [
        "compute_classifier_metrics(labels, class_probabilities, labels1, class_probabilities1, threshold=0.5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics with the original version\n",
            "Mean confusion matrix and error\n",
            "[[27219.1  8506.9]\n",
            " [ 2114.2 12063.8]]\n",
            "[[741.07513789 741.07513789]\n",
            " [251.34645412 251.34645412]]\n",
            "Recall = 0.850881647623078 +/- 0.017727920307701504\n",
            "Precision = 0.5866460793814928 +/- 0.016378794445637904\n",
            "F-score = 0.694385120592492 +/- 0.006961589870773184\n",
            "ROC AUC = 0.8771370307227058 +/- 0.0036987674265059993\n",
            "MCC = 0.5615496431071428 +/- 0.009644567420419007\n",
            "Metrics with probabilities concatenated\n",
            "Mean confusion matrix and error\n",
            "[[27219.1  8506.9]\n",
            " [ 2114.2 12063.8]]\n",
            "[[741.07513789 741.07513789]\n",
            " [251.34645412 251.34645412]]\n",
            "Recall = 0.850881647623078 +/- 0.017727920307701504\n",
            "Precision = 0.5866460793814928 +/- 0.016378794445637904\n",
            "F-score = 0.694385120592492 +/- 0.006961589870773184\n",
            "ROC AUC = 0.8771370307227058 +/- 0.0036987674265059993\n",
            "MCC = 0.5615496431071428 +/- 0.009644567420419007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJk6lP6yM0dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}