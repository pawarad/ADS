{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast-parapred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZhDrzofghsKFYXwcVnY5H9SSxL_QZ3T1",
      "authorship_tag": "ABX9TyNM6c68tFdgZWI+zbt8wAIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawarad/ADS/blob/master/Fast_parapred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOxM3TNkq5Pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cb7b7b41-4346-46df-a54e-b43335bc11fd"
      },
      "source": [
        "!pip install Biopython"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Biopython in /usr/local/lib/python3.6/dist-packages (1.77)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Biopython) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZffmQ0Bn2eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bio.PDB import *\n",
        "from Bio.PDB.Model import Model\n",
        "from Bio.PDB.Structure import Structure\n",
        "import numpy as np\n",
        "import Bio.PDB\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-awdLZU0dRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9mE839-0qJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gez2NfTDq2cI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residue_in_contact_with(res, ab_search, dist):\n",
        "    return any(len(ab_search.search(a.coord, dist)) > 0\n",
        "               for a in res.get_unpacked_list())"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5q-XHYL6YWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aaMap(_aa):\n",
        "    aa_df = pd.DataFrame([['Alanine','Ala','A'],\n",
        "                        ['Arginine','Arg','R'],\n",
        "                        ['Asparagine','Asn','N'],\n",
        "                        ['Aspartate','Asp','D'],\n",
        "                        ['Cysteine','Cys','C'],\n",
        "                        ['Glutamate','Glu','E'],\n",
        "                        ['Glutamine','Gln','Q'],\n",
        "                        ['Glycine','Gly','G'],\n",
        "                        ['Histidine','His','H'],\n",
        "                        ['Histidine_D','HID','H'],\n",
        "                        ['Histidine_E','HIE','H'],\n",
        "                        ['Histidine_P','HIP','H'],\n",
        "                        ['Isoleucine','Ile','I'],\n",
        "                        ['Leucine','Leu','L'],\n",
        "                        ['Lysine','Lys','K'],\n",
        "                        ['Methionine','Met','M'],\n",
        "                        ['Phenylalanine','Phe','F'],\n",
        "                        ['Proline','Pro','P'],\n",
        "                        ['Serine','Ser','S'],\n",
        "                        ['Threonine','Thr','T'],\n",
        "                        ['Tryptophan','Trp','W'],\n",
        "                        ['Tyrosine','Tyr','Y'],\n",
        "                        ['Valine','Val','V']],columns=['Full','3','1'])\n",
        "    aa_df['3'] = aa_df['3'].str.upper()\n",
        "    _aa = _aa.upper()\n",
        "    if len(_aa)==1:\n",
        "        toret = aa_df.loc[aa_df['1']==_aa,'3'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        toret = aa_df.loc[aa_df['3']==_aa,'1'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    return toret\n",
        "\n",
        "def maxi_len(all_variables):\n",
        "    max_length = 0\n",
        "    for i in range(len(all_variables)):\n",
        "        length = len(all_variables[i])\n",
        "        max_length = max(max_length,length)\n",
        "    #     max_length = length\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def ext_epitope(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in epitope:\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in ext_epi:\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "\n",
        "def ext_epitope_sorted(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in sorted(epitope):\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in sorted(ext_epi):\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "aa_s = \"CSTPAGNDEQHRKMILVFYWX\"\n",
        "def one_to_number(res_str):\n",
        "    return [aa_s.index(r) for r in res_str]\n",
        "\n",
        "def aa_features():\n",
        "    # Meiler's features\n",
        "    prop1 = [[1.77, 0.13, 2.43,  1.54,  6.35, 0.17, 0.41],\n",
        "             [1.31, 0.06, 1.60, -0.04,  5.70, 0.20, 0.28],\n",
        "             [3.03, 0.11, 2.60,  0.26,  5.60, 0.21, 0.36],\n",
        "             [2.67, 0.00, 2.72,  0.72,  6.80, 0.13, 0.34],\n",
        "             [1.28, 0.05, 1.00,  0.31,  6.11, 0.42, 0.23],\n",
        "             [0.00, 0.00, 0.00,  0.00,  6.07, 0.13, 0.15],\n",
        "             [1.60, 0.13, 2.95, -0.60,  6.52, 0.21, 0.22],\n",
        "             [1.60, 0.11, 2.78, -0.77,  2.95, 0.25, 0.20],\n",
        "             [1.56, 0.15, 3.78, -0.64,  3.09, 0.42, 0.21],\n",
        "             [1.56, 0.18, 3.95, -0.22,  5.65, 0.36, 0.25],\n",
        "             [2.99, 0.23, 4.66,  0.13,  7.69, 0.27, 0.30],\n",
        "             [2.34, 0.29, 6.13, -1.01, 10.74, 0.36, 0.25],\n",
        "             [1.89, 0.22, 4.77, -0.99,  9.99, 0.32, 0.27],\n",
        "             [2.35, 0.22, 4.43,  1.23,  5.71, 0.38, 0.32],\n",
        "             [4.19, 0.19, 4.00,  1.80,  6.04, 0.30, 0.45],\n",
        "             [2.59, 0.19, 4.00,  1.70,  6.04, 0.39, 0.31],\n",
        "             [3.67, 0.14, 3.00,  1.22,  6.02, 0.27, 0.49],\n",
        "             [2.94, 0.29, 5.89,  1.79,  5.67, 0.30, 0.38],\n",
        "             [2.94, 0.30, 6.47,  0.96,  5.66, 0.25, 0.41],\n",
        "             [3.21, 0.41, 8.08,  2.25,  5.94, 0.32, 0.42],\n",
        "             [0.00, 0.00, 0.00,  0.00,  0.00, 0.00, 0.00]]\n",
        "    return np.array(prop1)\n",
        "\n",
        "NUM_FEATURES = len(aa_s) + 7\n",
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" Converts a class vector to binary class matrix. \"\"\"\n",
        "    new_y = torch.LongTensor(y)\n",
        "    n = new_y.size()[0]\n",
        "    categorical = torch.zeros(n, num_classes)\n",
        "    arangedTensor = torch.arange(0, n)\n",
        "    intaranged = arangedTensor.long()\n",
        "    categorical[intaranged, new_y] = 1\n",
        "    return categorical\n",
        "\n",
        "def seq_to_one_hot(res_seq_one):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[new_ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    return torch.cat((onehot, feats), axis=1)\n",
        "\n",
        "def cdrseq_to_one_hot(res_seq_one,i):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    if i%6 == 0:\n",
        "        ext_onehot = [1, 0, 0, 0, 0, 0]\n",
        "    if i%6 == 1:\n",
        "        ext_onehot = [0, 1, 0, 0, 0, 0]\n",
        "    if i%6 == 2:\n",
        "        ext_onehot = [0, 0, 1, 0, 0, 0]\n",
        "    if i%6 == 3:\n",
        "        ext_onehot = [0, 0, 0, 1, 0, 0]\n",
        "    if i%6 == 4:\n",
        "        ext_onehot = [0, 0, 0, 0, 1, 0]\n",
        "    if i%6 == 5:\n",
        "        ext_onehot = [0, 0, 0, 0, 0, 1]\n",
        "    \n",
        "    chain_encoding = torch.Tensor(ext_onehot)\n",
        "    chain_encoding = chain_encoding.expand(onehot.shape[0], 6)\n",
        "    concatenated = torch.cat((onehot, feats,chain_encoding), 1)\n",
        "\n",
        "    return concatenated\n",
        "\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ews0kMC3diMh",
        "colab_type": "text"
      },
      "source": [
        "Epitope Cross Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuwL4zjNdnOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import index_select"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuO7yokLdrTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 16\n",
        "\n",
        "batch_size = 3\n",
        "\n",
        "class EpitopeX(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Declares the building blocks of the neural network.\n",
        "        \"\"\"\n",
        "        super(EpitopeX, self).__init__()\n",
        "        \n",
        "\n",
        "        #self.conv1 = nn.Conv1d(input_layer = NUM_FEATURES, output_channel/filter = 64, filter_size = 3, padding=1)\n",
        "        self.conv1 = nn.Conv1d(NUM_FEATURES, 64, 3, padding=1)  # antibody first a trous convolutional layer\n",
        "\n",
        "        self.agconv1 = nn.Conv1d(AG_NUM_FEATURES, 64, 3, padding=1)   # antigen first a trous convolutional layer\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)  #antibody second a trous convolutional layer\n",
        "\n",
        "        self.agconv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)  #antigen second a trous convolutional layer\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)  #antibody third a trous convolutional layer\n",
        "\n",
        "        self.agconv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)\n",
        "\n",
        "        #self.agconv4 = nn.Conv1d(256, 32, 1)\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # batch normalisation after the first convolutional layer for antibody\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.agbn1 = nn.BatchNorm1d(64)  # batch normalisation after the first convolutional layer for antigen\n",
        "        self.agbn2 = nn.BatchNorm1d(128)\n",
        "        self.agbn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.elu = nn.ReLU()            # activation function\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, 1, 1)  # dense prediction layer\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.aconv1 = nn.Conv1d(256, 1, 1)  # attentional mechanism\n",
        "        self.aconv2 = nn.Conv1d(32, 1, 1)\n",
        "\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool1d(2)\n",
        "        self.maxpool2 = nn.MaxPool1d(4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, ag_input, ag_unpacked_masks, ab_input, ab_unpacked_masks):\n",
        "        \"\"\"\n",
        "        Forward propagation step\n",
        "        :param ab_input: antibody amino acid sequences\n",
        "        :param ab_unpacked_masks: antibody amino acid masks\n",
        "        :param ag_input: antigen amino acid sequences\n",
        "        :param ag_unpacked_masks: antigen amino acid masks\n",
        "        :param dist: maskin\n",
        "        :return: antibody binding probabilities, attentional coefficients\n",
        "        \"\"\"\n",
        "        x=ab_input\n",
        "        agx = ag_input\n",
        "\n",
        "        ab_unpacked_masks = torch.transpose(ab_unpacked_masks, 1, 2)\n",
        "        ag_unpacked_masks = torch.transpose(ag_unpacked_masks, 1, 2)\n",
        "\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        agx = torch.transpose(agx, 1, 2)\n",
        "\n",
        "        x = self.conv1(x) \n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn1(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        agx = self.agconv1(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn1(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn2(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        agx = self.conv2(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn2(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn3(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        agx = self.conv3(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn3(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "        # MaxPool\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        old = x\n",
        "\n",
        "        oldag = agx\n",
        "\n",
        "        heads_no = 1\n",
        "\n",
        "\n",
        "        for i in range(heads_no):\n",
        "            #agconvi = nn.Conv1d(256, 128, 1)\n",
        "            aconvi1 = nn.Conv1d(256, 1, 1)\n",
        "            aconvi2 = nn.Conv1d(256, 1, 1)\n",
        "            if use_cuda:\n",
        "                aconvi1.cuda()\n",
        "                aconvi2.cuda()\n",
        "                #agconvi.cuda()\n",
        "            #agx = agconvi(oldag)\n",
        "            w_1 = aconvi1(x)\n",
        "            w_2 = aconvi2(agx)\n",
        "            #w = self.lrelu(w_1 + torch.transpose(w_2, 1, 2))\n",
        "\n",
        "            # print(\"w_1.shape\", w_1.shape)\n",
        "            # print(\"w_2.shape\", w_2.shape)\n",
        "\n",
        "            w= self.lrelu(torch.bmm(torch.transpose(w_1, 1, 2), w_2))\n",
        "\n",
        "            w = self.softmax(w)\n",
        "            w = self.dropout(w)\n",
        "            temp_loop_x = torch.bmm(torch.transpose(w,1,2), torch.transpose(x, 1, 2))\n",
        "            if i==0:\n",
        "                loop_x = temp_loop_x\n",
        "            else:\n",
        "                loop_x = torch.cat((loop_x, temp_loop_x), dim=2)\n",
        "\n",
        "        agx = torch.transpose(loop_x, 1, 2)\n",
        "        #x = x + old\n",
        "        agx = torch.cat((agx, oldag), dim=1)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "\n",
        "        agx = self.bn4(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = torch.transpose(agx, 1, 2)\n",
        "\n",
        "        agx = self.dropout2(agx)\n",
        "\n",
        "        agx = self.fc(agx)\n",
        "\n",
        "        #print(\"x after fc\", x, file=track_f)\n",
        "\n",
        "        return agx, w"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1gstW8hd2rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.set_printoptions(threshold=50000)\n",
        "import torch.optim as optim\n",
        "from torch import squeeze\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch import index_select\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4WWQDmeDON",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation of Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhxMrDtFSi1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off):\n",
        "    \"\"\"\n",
        "    For every CDR get the corresponding seq and check if dist is less than cutoff\n",
        "    and substitute 1 orelse 0 \n",
        "    \n",
        "    E.g. - CDR - TCRASGNIHNYLAWY\n",
        "           Seq.no - ['22','23','24','25','26','27','28','29','30','31','32','33','34','35','36']\n",
        "           \n",
        "           Output - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    \"\"\"\n",
        "    pdb_name = cdr_loop['PDB_Name'].to_numpy()\n",
        "    seq_no = cdr_loop['seq_no'].to_numpy()\n",
        "    chain_type = cdr_loop['Type'].to_numpy()\n",
        "\n",
        "    all_var = []\n",
        "    for i in range(len(pdb_name)):\n",
        "        if chain_type[i] == 'H1' or chain_type[i] == 'H2' or chain_type[i] == 'H3':\n",
        "            dist = pd.read_pickle(dist_matx_path_heavy+pdb_name[i]+\"heavy\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "\n",
        "        elif chain_type[i] == 'L1' or chain_type[i] == 'L2' or chain_type[i] == 'L3':\n",
        "            dist = pd.read_pickle(dist_matx_path_light+pdb_name[i]+\"light\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "    \n",
        "    return all_var"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRrj55XW7L-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def permute_training_cross_data(cdrs, cdr_masks, cdr_lengths, ag, ag_masks, ag_lengths,ag_lbls):\n",
        "    index = torch.randperm(cdrs.shape[0])\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    cdr_masks = torch.index_select(cdr_masks, 0, index)\n",
        "#     cdr_lbls = torch.index_select(cdr_lbls, 0, index)\n",
        "    cdr_lengths = [cdr_lengths[i] for i in index]\n",
        "\n",
        "    ag = torch.index_select(ag, 0, index)\n",
        "    ag_masks = torch.index_select(ag_masks, 0, index)\n",
        "#     print(\"permute\",type(ag_masks))\n",
        "    ag_lbls = torch.index_select(ag_lbls, 0, index)\n",
        "    ag_lengths = [ag_lengths[i] for i in index]\n",
        "\n",
        "    return cdrs, cdr_masks, cdr_lengths, ag, ag_masks, ag_lengths,  ag_lbls\n",
        "\n",
        "def flatten_with_lengths(matrix, lengths):\n",
        "    seqs = []\n",
        "    for i, example in enumerate(matrix):\n",
        "        seq = example[:lengths[i]]\n",
        "        seqs.append(seq)\n",
        "    return np.concatenate(seqs)\n",
        "\n",
        "def sort_cross_batch(cdrs, cdr_masks, ag, ag_masks, ag_lbls, ag_lengths):\n",
        "    order = np.argsort(ag_lengths)\n",
        "    order = order.tolist()\n",
        "    order.reverse()\n",
        "\n",
        "    ag_lengths.sort(reverse=True)\n",
        "\n",
        "    index = Variable(torch.LongTensor(order))\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    cdr_masks = torch.index_select(cdr_masks, 0, index)\n",
        "\n",
        "    ag = torch.index_select(ag, 0, index)\n",
        "    ag_masks = torch.index_select(ag_masks, 0, index)\n",
        "    ag_lbls = torch.index_select(ag_lbls, 0, index)\n",
        "\n",
        "\n",
        "    return cdrs, cdr_masks, ag, ag_masks, ag_lbls, ag_lengths"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29pFo-Xz9FB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def x_epitope_run(epi_train, lbls_train, masks_train, lengths_train, cdrs_train, cdr_masks_train, cdr_lengths_train,\n",
        "                  weights_template, weights_template_number, epi_test, lbls_test, masks_test, lengths_test,\n",
        "                  cdrs_test, cdr_masks_test, cdr_lengths_test):\n",
        "\n",
        "#     print(\"epitope run\", file=print_file)\n",
        "    print(\"epitope run\")\n",
        "    model = EpitopeX()\n",
        "\n",
        "    ignored_params = list(map(id, [model.conv1.weight,\n",
        "                                   #model.conv2.weight, model.conv3.weight, model.aconv1.weight,\n",
        "                                   #model.aconv2.weight\n",
        "                                ]))\n",
        "    base_params = filter(lambda p: id(p) not in ignored_params,\n",
        "                         model.parameters())\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': base_params},\n",
        "        {'params': model.conv1.weight, 'weight_decay': 0.01},\n",
        "        #{'params': model.conv2.weight, 'weight_decay': 0.01},\n",
        "        #{'params': model.conv3.weight, 'weight_decay': 0.01},\n",
        "        #{'params': model.aconv1.weight, 'weight_decay': 0.01},\n",
        "        #{'params': model.aconv2.weight, 'weight_decay': 0.01}\n",
        "    ], lr=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "\n",
        "    total_input = epi_train\n",
        "    total_lbls = lbls_train\n",
        "    total_masks = masks_train\n",
        "    total_lengths = lengths_train\n",
        "#     print(type(total_masks))\n",
        "\n",
        "    total_cdrs_train = cdrs_train\n",
        "    total_cdr_masks_train = cdr_masks_train\n",
        "    total_cdr_lengths_train = cdr_lengths_train\n",
        "\n",
        "    if use_cuda:\n",
        "        print(\"using cuda\")\n",
        "        model.cuda()\n",
        "        total_input = total_input.cuda()\n",
        "        total_lbls = total_lbls.cuda()\n",
        "        total_masks = total_masks.cuda()\n",
        "        epi_test = epi_test.cuda()\n",
        "        lbls_test = lbls_test.cuda()\n",
        "        masks_test = masks_test.cuda()\n",
        "\n",
        "        total_cdrs_train = total_cdrs_train.cuda()\n",
        "        total_cdr_masks_train = total_cdr_masks_train.cuda()\n",
        "        cdrs_test = cdrs_test.cuda()\n",
        "        cdr_masks_test = cdr_masks_test.cuda()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train(True)\n",
        "        scheduler.step()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        batches_done=0\n",
        "\n",
        "        #total_input, total_masks, total_lengths, total_lbls = \\\n",
        "        #    permute_training_data(total_input, total_masks, total_lengths, total_lbls)\n",
        "\n",
        "        total_cdrs_train, total_cdr_masks_train, total_cdr_lengths_train, total_input, total_masks, total_lengths, total_lbls = \\\n",
        "            permute_training_cross_data(cdrs=total_cdrs_train, cdr_masks=total_cdr_masks_train, cdr_lengths=total_cdr_lengths_train,\n",
        "                        ag=total_input, ag_masks=total_masks, ag_lengths=total_lengths, ag_lbls=total_lbls)\n",
        "        \n",
        "#         print(\"total_masks_output\",type(total_masks))\n",
        "\n",
        "        for j in range(0, epi_train.shape[0], batch_size):\n",
        "            batches_done +=1\n",
        "            interval = [x for x in range(j, min(epi_train.shape[0], j + batch_size))]\n",
        "            interval = torch.LongTensor(interval)\n",
        "            if use_cuda:\n",
        "                interval = interval.cuda()\n",
        "#             print(\"total_input\",type(total_input))\n",
        "#             print(\"total_masks\",type(total_masks))\n",
        "\n",
        "            input = Variable(index_select(total_input, 0, interval), requires_grad=True)\n",
        "            masks = Variable(index_select(total_masks, 0, interval))\n",
        "            lengths = total_lengths[j:j + batch_size]\n",
        "            lbls = Variable(index_select(total_lbls, 0, interval))\n",
        "\n",
        "            cdrs_train = Variable(index_select(total_cdrs_train, 0, interval))\n",
        "            cdr_masks_train = Variable(index_select(total_cdr_masks_train, 0, interval))\n",
        "            cdr_lengths_train = total_cdr_lengths_train[j:j+batch_size]\n",
        "\n",
        "            cdrs, cdr_masks, input, masks, lbls, lengths = sort_cross_batch(cdrs=cdrs_train, cdr_masks=cdr_masks_train,\n",
        "                                               ag=input, ag_masks=masks, ag_lbls=lbls,ag_lengths=list(lengths))\n",
        "\n",
        "            unpacked_masks = masks\n",
        "\n",
        "            packed_masks = pack_padded_sequence(masks, lengths, batch_first=True)\n",
        "            masks, _ = pad_packed_sequence(packed_masks, batch_first=True)\n",
        "\n",
        "            unpacked_lbls = lbls\n",
        "\n",
        "            packed_lbls = pack_padded_sequence(lbls, lengths, batch_first=True)\n",
        "            lbls, _ = pad_packed_sequence(packed_lbls, batch_first=True)\n",
        "\n",
        "\n",
        "            output, attn_coeff = model(input, unpacked_masks, cdrs, cdr_masks)\n",
        "\n",
        "            loss_weights = (unpacked_lbls * 1.5 + 1) * unpacked_masks\n",
        "            max_val = (-output).clamp(min=0)\n",
        "            loss = loss_weights * (output - output * unpacked_lbls + max_val + ((-max_val).exp() + (-output - max_val).exp()).log())\n",
        "            masks_added = masks.sum()\n",
        "            loss = loss.sum() / masks_added\n",
        "\n",
        "            #print(\"Epoch %d - Batch %d has loss %d \" % (epoch, j, loss.data), file=monitoring_file)\n",
        "            epoch_loss +=loss\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Epoch %d - loss is %f : \" % (epoch, epoch_loss.data/batches_done))\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        cdrs_test2, cdr_masks_test2, epi_test2, masks_test2, lbls_test2, lengths_test2 = \\\n",
        "            sort_cross_batch(cdrs=cdrs_test, cdr_masks=cdr_masks_test, ag=epi_test, ag_masks=masks_test,\n",
        "                                         ag_lbls=lbls_test,ag_lengths=list(lengths_test))\n",
        "\n",
        "        #epi_test2, masks_test2, lengths_test2, lbls_test2 = sort_batch(epi_test, masks_test, list(lengths_test),\n",
        "        #                                                            lbls_test)\n",
        "\n",
        "        unpacked_masks_test2 = masks_test2\n",
        "\n",
        "        probs_test2, attn_coeff_test2 = model(epi_test2, unpacked_masks_test2, cdrs_test2, cdr_masks_test2)\n",
        "\n",
        "        # K.mean(K.equal(lbls_test, K.round(y_pred)), axis=-1)\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        probs_test2 = sigmoid(probs_test2)\n",
        "\n",
        "        probs_test2 = probs_test2.data.cpu().numpy().astype('float32')\n",
        "        lbls_test2 = lbls_test2.data.cpu().numpy().astype('int32')\n",
        "\n",
        "        probs_test2 = flatten_with_lengths(probs_test2, lengths_test2)\n",
        "        lbls_test2 = flatten_with_lengths(lbls_test2, lengths_test2)\n",
        "\n",
        "        print(\"Roc\", roc_auc_score(lbls_test2, probs_test2))\n",
        "        # print(\"lbls_test2, probs_test2:\",(lbls_test2, probs_test2))\n",
        "\n",
        "    torch.save(model.state_dict(), weights_template.format(weights_template_number))\n",
        "\n",
        "    print(\"test\")\n",
        "    model.eval()\n",
        "\n",
        "    cdrs_test, cdr_masks_test, epi_test, masks_test, lbls_test, lengths_test = \\\n",
        "        sort_cross_batch(cdrs=cdrs_test, cdr_masks=cdr_masks_test, ag=epi_test, ag_masks=masks_test,\n",
        "                         ag_lbls=lbls_test,ag_lengths=list(lengths_test))\n",
        "\n",
        "    #epi_test, masks_test, lengths_test, lbls_test = sort_batch(epi_test, masks_test, list(lengths_test), lbls_test)\n",
        "\n",
        "    unpacked_masks_test = masks_test\n",
        "    packed_input = pack_padded_sequence(masks_test, list(lengths_test), batch_first=True)\n",
        "    masks_test, _ = pad_packed_sequence(packed_input, batch_first=True)\n",
        "\n",
        "    probs_test, attn_coeff_test = model(epi_test, unpacked_masks_test, cdrs_test, cdr_masks_test)\n",
        "\n",
        "    # K.mean(K.equal(lbls_test, K.round(y_pred)), axis=-1)\n",
        "\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    probs_test = sigmoid(probs_test)\n",
        "\n",
        "    probs_test1 = probs_test.data.cpu().numpy().astype('float32')\n",
        "    lbls_test1 = lbls_test.data.cpu().numpy().astype('int32')\n",
        "\n",
        "    probs_test1 = flatten_with_lengths(probs_test1, list(lengths_test))\n",
        "    lbls_test1 = flatten_with_lengths(lbls_test1, list(lengths_test))\n",
        "\n",
        "    print(\"Roc\", roc_auc_score(lbls_test1, probs_test1))\n",
        "    # print(\"lbls_test1, probs_test1:\",(lbls_test1, probs_test1))\n",
        "\n",
        "    return probs_test, lbls_test, probs_test1, lbls_test1  # get them in kfold, append, concatenate do roc on them"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2PEXo_79GUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kfold_cv_eval(ag, lbls, masks, lengths, cdrs, cdr_masks, cdr_lengths, seed,output_file,\n",
        "                  weights_template):\n",
        "#     ag, lbls, masks, lengths, cdrs, cdr_masks, cdr_lengths = \\\n",
        "#         dataset[\"ag\"], dataset[\"ag_lbls\"], dataset[\"ag_masks\"], dataset[\"ag_lengths\"], \\\n",
        "#         dataset[\"cdrs\"], dataset[\"cdr_masks\"], dataset[\"cdr_lengths\"]\n",
        "\n",
        "    print(\"ag\", ag.shape)\n",
        "    print(\"lbls\", lbls.shape)\n",
        "    print(\"masks\", masks.shape)\n",
        "    print(\"cdrs\", cdrs.shape)\n",
        "    print(\"cdr masks\", cdr_masks.shape)\n",
        "\n",
        "    kf = KFold(n_splits=NUM_SPLIT, random_state=seed, shuffle=True)\n",
        "\n",
        "    all_lbls2 = []\n",
        "    all_probs2 = []\n",
        "    all_masks = []\n",
        "\n",
        "    all_probs1 = []\n",
        "    all_lbls1 = []\n",
        "\n",
        "    for i, (train_idx, test_idx) in enumerate(kf.split(ag)):\n",
        "        print(\"train_idx, test_idx:\",(train_idx, test_idx))\n",
        "        print(\"Fold: \", i + 1)\n",
        "\n",
        "        lengths_train = [lengths[i] for i in train_idx]\n",
        "        lengths_test = [lengths[i] for i in test_idx]\n",
        "\n",
        "        cdr_lengths_train = [cdr_lengths[i] for i in train_idx]\n",
        "        cdr_lengths_test = [cdr_lengths[i] for i in test_idx]\n",
        "\n",
        "        print(\"len(train_idx)\",len(train_idx))\n",
        "\n",
        "        train_idx = torch.from_numpy(train_idx)\n",
        "        test_idx = torch.from_numpy(test_idx)\n",
        "\n",
        "        ag_train = index_select(ag, 0, train_idx)\n",
        "        lbls_train = index_select(lbls, 0, train_idx)\n",
        "        masks_train = index_select(masks, 0, train_idx)\n",
        "        cdrs_train = index_select(cdrs, 0, train_idx)\n",
        "        cdr_masks_train = index_select(cdr_masks, 0, train_idx)\n",
        "\n",
        "\n",
        "        ag_test = Variable(index_select(ag, 0, test_idx))\n",
        "        lbls_test = Variable(index_select(lbls, 0, test_idx))\n",
        "        masks_test = Variable(index_select(masks, 0, test_idx))\n",
        "        cdrs_test = Variable(index_select(cdrs, 0, test_idx))\n",
        "        cdr_masks_test = Variable(index_select(cdr_masks, 0, test_idx))\n",
        "        \n",
        "        \n",
        "\n",
        "        code = 2\n",
        "        if code == 1:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                epitope_run(ag_train, lbls_train, masks_train, lengths_train, weights_template, i,\n",
        "                                    ag_test, lbls_test, masks_test, lengths_test)\n",
        "\n",
        "        if code == 2:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                x_epitope_run(ag_train, lbls_train, masks_train, lengths_train,\n",
        "                              cdrs_train, cdr_masks_train, cdr_lengths_train,\n",
        "                              weights_template, i,\n",
        "                              ag_test, lbls_test, masks_test, lengths_test,\n",
        "                              cdrs_test, cdr_masks_test, cdr_lengths_test)\n",
        "\n",
        "\n",
        "        lbls_test2 = np.squeeze(lbls_test2)\n",
        "        all_lbls2 = np.concatenate((all_lbls2, lbls_test2))\n",
        "        all_lbls1.append(lbls_test1)\n",
        "\n",
        "        probs_test_pad = torch.zeros(probs_test1.data.shape[0], MAX_EXT_AG_LENGTH, probs_test1.data.shape[2])\n",
        "        probs_test_pad[:probs_test1.data.shape[0], :probs_test1.data.shape[1], :] = probs_test1.data\n",
        "        probs_test_pad = Variable(probs_test_pad)\n",
        "\n",
        "        probs_test2 = np.squeeze(probs_test2)\n",
        "        #print(probs_test)\n",
        "        all_probs2 = np.concatenate((all_probs2, probs_test2))\n",
        "        #print(all_probs)\n",
        "        #print(type(all_probs))\n",
        "\n",
        "        all_probs1.append(probs_test_pad)\n",
        "\n",
        "        all_masks.append(masks_test)\n",
        "\n",
        "    lbl_mat1 = torch.cat(all_lbls1)\n",
        "    prob_mat1 = torch.cat(all_probs1)\n",
        "    #print(\"end\", all_probs)\n",
        "    mask_mat = torch.cat(all_masks)\n",
        "\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump((lbl_mat1, prob_mat1, mask_mat, all_lbls2, all_probs2), f)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQAjQyLAHiam",
        "colab_type": "text"
      },
      "source": [
        "Extracting epitopes from all the pdbs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvc1m-yy6VdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_files = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/summary_file_cleaned.pkl\")\n",
        "summary_files = summary_files.drop(summary_files[summary_files.pdb.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "\n",
        "pdb_names = summary_files['pdb'].to_numpy()\n",
        "Hchain_id = summary_files['Hchain'].to_numpy()\n",
        "Lchain_id = summary_files['Lchain'].to_numpy()\n",
        "Achain_id = summary_files['antigen_chain_list'].to_numpy()\n",
        " \n",
        "epi_res = []\n",
        "epi_res_num = []\n",
        "ext_epi_res_num = []\n",
        "ext_epi_res = []\n",
        "A_pdb = []\n",
        "# for i in range(len(pdb_names)):\n",
        "#     epi_residues,epi_res_number,ext_epi_res_number, ext_epi_residues = ext_epitope_sorted(Hchain_id[i],Lchain_id[i],Achain_id[i],pdb_names[i])\n",
        "#     A_pdb.append(pdb_names[i])\n",
        "#     epi_res.append(epi_residues)\n",
        "#     epi_res_num.append(epi_res_number)\n",
        "#     ext_epi_res_num.append(ext_epi_res_number)\n",
        "#     ext_epi_res.append(ext_epi_residues)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzsQpvLTHZbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorted_Antigen_dataframe = pd.DataFrame(list(zip(A_pdb,epi_res,epi_res_num, ext_epi_res_num, ext_epi_res)),\n",
        "#                columns =['pdb','sorted_epi_residues','sorted_epi_res_number', 'sorted_ext_epi_res_number', 'sorted_ext_epi_residues'])\n",
        "# Sorted_Antigen_dataframe.to_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nXabrvlL9Gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d871f29d-58f0-451d-f50a-2d59eacd9eef"
      },
      "source": [
        " Sorted_epi = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")\n",
        " Sorted_epi.head()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pdb</th>\n",
              "      <th>sorted_epi_residues</th>\n",
              "      <th>sorted_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_residues</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4ydk</td>\n",
              "      <td>[Q, L, T, G, G, S, T, E, N, A, K, T, Q, P, S, ...</td>\n",
              "      <td>[105 , 122 , 123 , 124 , 198 , 199 , 257 , 275...</td>\n",
              "      <td>[52 , 54 , 65 , 66 , 69 , 93 , 94 , 95 , 96 , ...</td>\n",
              "      <td>[L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4ydj</td>\n",
              "      <td>[W, K, E, E, N, N, A, K, T, I, R, P, S, G, G, ...</td>\n",
              "      <td>[96 , 97 , 102 , 275 , 279 , 280 , 281 , 282 ,...</td>\n",
              "      <td>[47 , 48 , 49 , 50 , 51 , 52 , 69 , 92 , 93 , ...</td>\n",
              "      <td>[D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1u8i</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1u8l</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5y9j</td>\n",
              "      <td>[K, G, S, Y, T, Y, A, M, G, H, L, Q, K, V, G, ...</td>\n",
              "      <td>[160 , 161 , 162 , 163 , 205 , 206 , 207 , 208...</td>\n",
              "      <td>[156 , 157 , 158 , 159 , 160 , 161 , 162 , 163...</td>\n",
              "      <td>[P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pdb  ...                            sorted_ext_epi_residues\n",
              "0  4ydk  ...  [L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...\n",
              "1  4ydj  ...  [D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...\n",
              "2  1u8i  ...                              [E, L, D, K, W, A, N]\n",
              "3  1u8l  ...                              [D, L, D, R, W, A, S]\n",
              "4  5y9j  ...  [P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAVsKIatMEV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epi_res = Sorted_epi['sorted_epi_residues'].to_numpy()\n",
        "epi_res_num = Sorted_epi['sorted_epi_res_number'].to_numpy()\n",
        "ext_epi_res_num = Sorted_epi['sorted_ext_epi_res_number'].to_numpy()\n",
        "ext_epi_res = Sorted_epi['sorted_ext_epi_residues'].to_numpy()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sroRWQ-_GA8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Jw3NOAHqSr",
        "colab_type": "text"
      },
      "source": [
        "**Building input data Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVhRbNh6HGKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "097ed1c2-8335-4046-bfdc-54ab9318aeda"
      },
      "source": [
        "##Joining all the residue abbrevations for making a sequence and repeating each seq 6 times/pdb\n",
        "\n",
        "Anti_str = []\n",
        "for i in range(len(ext_epi_res)):\n",
        "#     print(i)\n",
        "    str1 = ''.join(ext_epi_res[i])\n",
        "#     print(repeat(str1, 6))\n",
        "#     print(str1,i,len(A_mat[i]))\n",
        "    Anti_str.append(str1)\n",
        "\n",
        "import itertools\n",
        "Anti_seq = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in Anti_str))\n",
        "\n",
        "MAX_EXT_AG_LENGTH = maxi_len(Anti_seq)\n",
        "MAX_EXT_AG_LENGTH"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzHLal6cH7AA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b90ec978-896a-4825-ce61-b02bcb1f92e8"
      },
      "source": [
        "## Generating Labels for Extended Epitope Antigen Matrix\n",
        "\n",
        "labels = []\n",
        "for i in range(len(ext_epi_res_num)):\n",
        "    label = []\n",
        "    for j in ext_epi_res_num[i]:\n",
        "#     print(i)\n",
        "        if j in epi_res_num[i]:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    labels.append(label)\n",
        "\n",
        "import itertools\n",
        "Anti_repeat_labels = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in labels))\n",
        "\n",
        "# Antigen_lbls = np.stack(labels)\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "label_mats = []\n",
        "for i in range(len(Anti_repeat_labels)):\n",
        "    label_mat = torch.tensor(Anti_repeat_labels[i])\n",
        "    label_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, 1))\n",
        "    label_mat_pad[:label_mat.shape[0], 0] = label_mat\n",
        "    label_mats.append(label_mat_pad)\n",
        "\n",
        "ext_epi_lbls = torch.stack(label_mats)\n",
        "ext_epi_lbls.shape"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8WpCw3J5Rf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "039bca14-e450-4f82-c058-6c7c6df786fd"
      },
      "source": [
        "NUM_FEATURES = 28\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "# cdr_mats = []\n",
        "ext_epi_mats = []\n",
        "\n",
        "ext_epi_lengths = []\n",
        "for i in range(len(Anti_seq)):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "#     print(i)\n",
        "    ext_epi_mat = seq_to_one_hot(Anti_seq[i])\n",
        "    ext_epi_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, NUM_FEATURES))\n",
        "    ext_epi_mat_pad[:ext_epi_mat.shape[0], :] = ext_epi_mat\n",
        "    ext_epi_mats.append(ext_epi_mat_pad)\n",
        "    ext_epi_lengths.append(ext_epi_mat.shape[0])\n",
        "\n",
        "ext_epi = torch.stack(ext_epi_mats)\n",
        "ext_epi.shape\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWDA1wRSMcbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e95e04a5-0f6a-4303-e472-05d5a900b8db"
      },
      "source": [
        "max(ext_epi_lengths)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TLMthRBKN1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "397e5f3f-7e61-400e-eab5-e59022391c30"
      },
      "source": [
        "ext_epi_masks = []\n",
        "for i in range(len(Anti_seq)):\n",
        "    ext_epi_mask = torch.zeros((MAX_EXT_AG_LENGTH, 1), dtype=int)\n",
        "    ext_epi_mask[:len(Anti_seq[i]), 0] = 1\n",
        "    ext_epi_masks.append(ext_epi_mask)\n",
        "ext_epi_masks = torch.stack(ext_epi_masks)\n",
        "ext_epi_masks.shape"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQscQTPXKSNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad2f1a51-b51e-4b36-8dac-35b74c98c443"
      },
      "source": [
        "NUM_FEATURES = 28+6\n",
        "Max_len_CDR = 38\n",
        "cdr_mats = []\n",
        "\n",
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "cdrs_seq = torch.stack(cdr_mats)\n",
        "cdrs_seq.shape"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 34])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_qQoviXMtkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45955abb-7d52-47f2-def6-08f557b2c205"
      },
      "source": [
        "all_cdrs_lengths = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "    all_cdrs_lengths.append(cdr_mat.shape[0])\n",
        "max(all_cdrs_lengths)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5mz0S1FNDiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28b71eed-1b73-4097-8bd2-c5ebf9227e06"
      },
      "source": [
        "Max_len_CDR = 38\n",
        "\n",
        "cdr_masks = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "    cdr_mask = torch.zeros((Max_len_CDR, 1), dtype=int)\n",
        "    cdr_mask[:len(cdr_loop['CDR'][i]), 0] = 1\n",
        "    cdr_masks.append(cdr_mask)\n",
        "cdr_mask = torch.stack(cdr_masks)\n",
        "cdr_mask.shape"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqWBz2YhNu6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\n",
        "                                                          \"5e08\",\"5gkr\",\"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dist_matx_path_light = \"/content/drive/My Drive/Peritia_Fast-Parapred/Light_chain_dist_clean/\"\n",
        "dist_matx_path_heavy = \"/content/drive/My Drive/Peritia_Fast-Parapred/Heavy_chain_dist_clean/\"\n",
        "cut_off = 5\n",
        "# all_variables = get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off)\n",
        "with open('/content/drive/My Drive/Peritia_Fast-Parapred/all_variables.pkl', 'rb') as f:\n",
        "    all_variables = pickle.load(f)\n",
        "# all_variables\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09SWlhCsR_ea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "905d1ede-b7b1-4e38-9859-1e1bd8d8bafa"
      },
      "source": [
        "cont_mats = []\n",
        "cdr_lengths = []\n",
        "Max_len_CDR = 38\n",
        "\"\"\"\n",
        "Pad the variables to Max CDR len with 0\n",
        "Here max len of CDR is 38\n",
        "\n",
        "E.g - Input = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    Output = [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[1.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.]\n",
        "\n",
        "\"\"\"\n",
        "for i in range(len(all_variables)):\n",
        "    cont_mat = torch.tensor(all_variables[i])\n",
        "    cont_mat_pad = torch.zeros((Max_len_CDR, 1))\n",
        "    cont_mat_pad[:cont_mat.shape[0], 0] = cont_mat\n",
        "    cont_mats.append(cont_mat_pad)\n",
        "    cdr_lengths.append(cont_mat.shape[0])\n",
        "\n",
        "\n",
        "cdr_lbls = torch.stack(cont_mats)\n",
        "\n",
        "cdr_lbls.shape\n",
        "# cdr_lengths"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeEDdixqXRdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def structure_ids_to_selection_mask(idx, num_structures):\n",
        "    mask = np.zeros((num_structures * 6, ), dtype=np.bool)\n",
        "    offset = idx * 6\n",
        "    for i in range(6):\n",
        "        mask[offset + i] = True\n",
        "    return mask\n"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8j0lP3lW8y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "AG_NUM_FEATURES = 28\n",
        "\n",
        "NUM_FEATURES = 34\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 16\n",
        "\n",
        "batch_size = 3"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25xwg7SgXHZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_ITERATIONS = 10\n",
        "NUM_SPLIT = 10\n",
        "output_file=\"crossval-data.p\",\n",
        "weights_template=\"weights-fold-{}.h5\"\n",
        "import os \n",
        "def run_cv(output_folder=\"cv-ag-seq\",\n",
        "           num_iters=NUM_ITERATIONS):\n",
        "#     cache_file = dataset.split(\"/\")[-1] + \".p\"\n",
        "#     dataset = open_dataset(dataset_cache=cache_file)\n",
        "\n",
        "    dir =  output_folder + \"/weights\"\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(output_folder + \"/weights\")\n",
        "    for i in range(num_iters):\n",
        "        #i=0\n",
        "        print(\"Crossvalidation run\", i+1)\n",
        "        output_file = \"{}/run-{}.p\".format(output_folder, i)\n",
        "        weights_template = output_folder + \"/weights/run-\" + \\\n",
        "                           str(i) + \"-fold-{}.pth.tar\"\n",
        "        kfold_cv_eval(ext_epi, ext_epi_lbls, ext_epi_masks, ext_epi_lengths, cdrs_seq,cdr_mask,all_cdrs_lengths ,\n",
        "                      i,output_file, weights_template)\n"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QfLYv9U-DhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adc989d7-93a8-44f4-9497-875a9bc5837b"
      },
      "source": [
        "import time\n",
        "import os\n",
        "start_time = time.time()\n",
        "run_cv(output_folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ag-seq\",\n",
        "           num_iters=NUM_ITERATIONS)\n",
        "end_time = time.time() - start_time\n",
        "hours, rem = divmod(end_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crossvalidation run 1\n",
            "ag torch.Size([3774, 288, 28])\n",
            "lbls torch.Size([3774, 288, 1])\n",
            "masks torch.Size([3774, 288, 1])\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "cdr masks torch.Size([3774, 38, 1])\n",
            "train_idx, test_idx: (array([   0,    3,    6, ..., 3770, 3771, 3772]), array([   1,    2,    4, ..., 3765, 3769, 3773]))\n",
            "Fold:  1\n",
            "len(train_idx) 2516\n",
            "epitope run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.204668 : \n",
            "Roc 0.6380423018721144\n",
            "Epoch 1 - loss is 0.991659 : \n",
            "Roc 0.6630710678319852\n",
            "Epoch 2 - loss is 0.972102 : \n",
            "Roc 0.6442031573740912\n",
            "Epoch 3 - loss is 0.957234 : \n",
            "Roc 0.649968109336591\n",
            "Epoch 4 - loss is 0.938730 : \n",
            "Roc 0.7237862208474369\n",
            "Epoch 5 - loss is 0.923659 : \n",
            "Roc 0.71290867010326\n",
            "Epoch 6 - loss is 0.907598 : \n",
            "Roc 0.7273007260420783\n",
            "Epoch 7 - loss is 0.892767 : \n",
            "Roc 0.7919702150453798\n",
            "Epoch 8 - loss is 0.871271 : \n",
            "Roc 0.6835464565547641\n",
            "Epoch 9 - loss is 0.839724 : \n",
            "Roc 0.7949645279431171\n",
            "Epoch 10 - loss is 0.819758 : \n",
            "Roc 0.8032576857464235\n",
            "Epoch 11 - loss is 0.805997 : \n",
            "Roc 0.792213119160152\n",
            "Epoch 12 - loss is 0.803086 : \n",
            "Roc 0.7781887932613049\n",
            "Epoch 13 - loss is 0.793016 : \n",
            "Roc 0.8232585363346732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-51564f4313b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m run_cv(output_folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ag-seq\",\n\u001b[0;32m----> 5\u001b[0;31m            num_iters=NUM_ITERATIONS)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-145-8bc782d3cd89>\u001b[0m in \u001b[0;36mrun_cv\u001b[0;34m(output_folder, num_iters)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mweights_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/weights/run-\"\u001b[0m \u001b[0;34m+\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-fold-{}.pth.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         kfold_cv_eval(ext_epi, ext_epi_lbls, ext_epi_masks, ext_epi_lengths, cdrs_seq,cdr_mask,all_cdrs_lengths ,\n\u001b[0;32m---> 20\u001b[0;31m                       i,output_file, weights_template)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-127-bf1022cace18>\u001b[0m in \u001b[0;36mkfold_cv_eval\u001b[0;34m(ag, lbls, masks, lengths, cdrs, cdr_masks, cdr_lengths, seed, output_file, weights_template)\u001b[0m\n\u001b[1;32m     60\u001b[0m                               \u001b[0mweights_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                               \u001b[0mag_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                               cdrs_test, cdr_masks_test, cdr_lengths_test)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-65dd37fc75b6>\u001b[0m in \u001b[0;36mx_epitope_run\u001b[0;34m(epi_train, lbls_train, masks_train, lengths_train, cdrs_train, cdr_masks_train, cdr_lengths_train, weights_template, weights_template_number, epi_test, lbls_test, masks_test, lengths_test, cdrs_test, cdr_masks_test, cdr_lengths_test)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d - loss is %f : \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatches_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYFcFDpxxKX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e39d6ec8-642c-4873-ed2d-f66012bb8c2f"
      },
      "source": [
        "ext_epi[0]\n",
        "Anti_seq[0]"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'LCVHWFNMWKNNMVEQMQEDVISLWDQSLQPCVKLTGGSVIKQFPIYFNGTGIKPVVSTQLLLGIIRSENLTNNAKTIIVHICRACWLVTLKHFNNKTIIFQPPSGGDLEITMHHFNCGEFFYCNTTQLFNNTILPCKIKQIINMWQGTGQAMYSITGILLTRDGGANTSNETFRPGGGNIKDNWRSELY'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6en-R7x_T9g1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dab94653-23b9-4ae3-f2c8-d1dc932e7e1f"
      },
      "source": [
        "model = EpitopeX()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ivhipOXgIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "00b09de1-4c49-444c-c723-b0a1f273f907"
      },
      "source": [
        "prediction = model([ext_epi[0]])"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-43513af2e4e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext_epi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 3 required positional arguments: 'ag_unpacked_masks', 'ab_input', and 'ab_unpacked_masks'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jvk5NvRXspU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}