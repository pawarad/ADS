{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_parapred_CDR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1J-RaOcInOtH0yu6lATObmolcw9QKi87x",
      "authorship_tag": "ABX9TyOykzWLsLpROYSu4QnwDVPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawarad/ADS/blob/master/Fast_parapred_CDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyEqeW2_mf7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9800be92-3dc7-428c-80dd-b846a0049e49"
      },
      "source": [
        "!pip install Biopython"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/66/134dbd5f885fc71493c61b6cf04c9ea08082da28da5ed07709b02857cbd0/biopython-1.77-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Biopython) (1.18.5)\n",
            "Installing collected packages: Biopython\n",
            "Successfully installed Biopython-1.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qor9NmtQmuar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bio.PDB import *\n",
        "from Bio.PDB.Model import Model\n",
        "from Bio.PDB.Structure import Structure\n",
        "import numpy as np\n",
        "import Bio.PDB\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKKKAurOmylb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residue_in_contact_with(res, ab_search, dist):\n",
        "    return any(len(ab_search.search(a.coord, dist)) > 0\n",
        "               for a in res.get_unpacked_list())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRnhRIKPm95N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aaMap(_aa):\n",
        "    aa_df = pd.DataFrame([['Alanine','Ala','A'],\n",
        "                        ['Arginine','Arg','R'],\n",
        "                        ['Asparagine','Asn','N'],\n",
        "                        ['Aspartate','Asp','D'],\n",
        "                        ['Cysteine','Cys','C'],\n",
        "                        ['Glutamate','Glu','E'],\n",
        "                        ['Glutamine','Gln','Q'],\n",
        "                        ['Glycine','Gly','G'],\n",
        "                        ['Histidine','His','H'],\n",
        "                        ['Histidine_D','HID','H'],\n",
        "                        ['Histidine_E','HIE','H'],\n",
        "                        ['Histidine_P','HIP','H'],\n",
        "                        ['Isoleucine','Ile','I'],\n",
        "                        ['Leucine','Leu','L'],\n",
        "                        ['Lysine','Lys','K'],\n",
        "                        ['Methionine','Met','M'],\n",
        "                        ['Phenylalanine','Phe','F'],\n",
        "                        ['Proline','Pro','P'],\n",
        "                        ['Serine','Ser','S'],\n",
        "                        ['Threonine','Thr','T'],\n",
        "                        ['Tryptophan','Trp','W'],\n",
        "                        ['Tyrosine','Tyr','Y'],\n",
        "                        ['Valine','Val','V']],columns=['Full','3','1'])\n",
        "    aa_df['3'] = aa_df['3'].str.upper()\n",
        "    _aa = _aa.upper()\n",
        "    if len(_aa)==1:\n",
        "        toret = aa_df.loc[aa_df['1']==_aa,'3'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        toret = aa_df.loc[aa_df['3']==_aa,'1'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    return toret\n",
        "\n",
        "def maxi_len(all_variables):\n",
        "    max_length = 0\n",
        "    for i in range(len(all_variables)):\n",
        "        length = len(all_variables[i])\n",
        "        max_length = max(max_length,length)\n",
        "    #     max_length = length\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def ext_epitope(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in epitope:\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in ext_epi:\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "\n",
        "def ext_epitope_sorted(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in sorted(epitope):\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in sorted(ext_epi):\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "aa_s = \"CSTPAGNDEQHRKMILVFYWX\"\n",
        "def one_to_number(res_str):\n",
        "    return [aa_s.index(r) for r in res_str]\n",
        "\n",
        "def aa_features():\n",
        "    # Meiler's features\n",
        "    prop1 = [[1.77, 0.13, 2.43,  1.54,  6.35, 0.17, 0.41],\n",
        "             [1.31, 0.06, 1.60, -0.04,  5.70, 0.20, 0.28],\n",
        "             [3.03, 0.11, 2.60,  0.26,  5.60, 0.21, 0.36],\n",
        "             [2.67, 0.00, 2.72,  0.72,  6.80, 0.13, 0.34],\n",
        "             [1.28, 0.05, 1.00,  0.31,  6.11, 0.42, 0.23],\n",
        "             [0.00, 0.00, 0.00,  0.00,  6.07, 0.13, 0.15],\n",
        "             [1.60, 0.13, 2.95, -0.60,  6.52, 0.21, 0.22],\n",
        "             [1.60, 0.11, 2.78, -0.77,  2.95, 0.25, 0.20],\n",
        "             [1.56, 0.15, 3.78, -0.64,  3.09, 0.42, 0.21],\n",
        "             [1.56, 0.18, 3.95, -0.22,  5.65, 0.36, 0.25],\n",
        "             [2.99, 0.23, 4.66,  0.13,  7.69, 0.27, 0.30],\n",
        "             [2.34, 0.29, 6.13, -1.01, 10.74, 0.36, 0.25],\n",
        "             [1.89, 0.22, 4.77, -0.99,  9.99, 0.32, 0.27],\n",
        "             [2.35, 0.22, 4.43,  1.23,  5.71, 0.38, 0.32],\n",
        "             [4.19, 0.19, 4.00,  1.80,  6.04, 0.30, 0.45],\n",
        "             [2.59, 0.19, 4.00,  1.70,  6.04, 0.39, 0.31],\n",
        "             [3.67, 0.14, 3.00,  1.22,  6.02, 0.27, 0.49],\n",
        "             [2.94, 0.29, 5.89,  1.79,  5.67, 0.30, 0.38],\n",
        "             [2.94, 0.30, 6.47,  0.96,  5.66, 0.25, 0.41],\n",
        "             [3.21, 0.41, 8.08,  2.25,  5.94, 0.32, 0.42],\n",
        "             [0.00, 0.00, 0.00,  0.00,  0.00, 0.00, 0.00]]\n",
        "    return np.array(prop1)\n",
        "\n",
        "NUM_FEATURES = len(aa_s) + 7\n",
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" Converts a class vector to binary class matrix. \"\"\"\n",
        "    new_y = torch.LongTensor(y)\n",
        "    n = new_y.size()[0]\n",
        "    categorical = torch.zeros(n, num_classes)\n",
        "    arangedTensor = torch.arange(0, n)\n",
        "    intaranged = arangedTensor.long()\n",
        "    categorical[intaranged, new_y] = 1\n",
        "    return categorical\n",
        "\n",
        "def seq_to_one_hot(res_seq_one):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[new_ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    return torch.cat((onehot, feats), axis=1)\n",
        "\n",
        "def cdrseq_to_one_hot(res_seq_one,i):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    if i%6 == 0:\n",
        "        ext_onehot = [1, 0, 0, 0, 0, 0]\n",
        "    if i%6 == 1:\n",
        "        ext_onehot = [0, 1, 0, 0, 0, 0]\n",
        "    if i%6 == 2:\n",
        "        ext_onehot = [0, 0, 1, 0, 0, 0]\n",
        "    if i%6 == 3:\n",
        "        ext_onehot = [0, 0, 0, 1, 0, 0]\n",
        "    if i%6 == 4:\n",
        "        ext_onehot = [0, 0, 0, 0, 1, 0]\n",
        "    if i%6 == 5:\n",
        "        ext_onehot = [0, 0, 0, 0, 0, 1]\n",
        "    \n",
        "    chain_encoding = torch.Tensor(ext_onehot)\n",
        "    chain_encoding = chain_encoding.expand(onehot.shape[0], 6)\n",
        "    concatenated = torch.cat((onehot, feats,chain_encoding), 1)\n",
        "\n",
        "    return concatenated\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_wuKcdVnEXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import index_select"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4KyNLg9nITU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 16\n",
        "\n",
        "batch_size = 3"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BscerYsUnNPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AG(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Declares the building blocks of the neural network.\n",
        "        \"\"\"\n",
        "        super(AG, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(NUM_FEATURES, 64, 3, padding=1)  # antibody first a trous convolutional layer\n",
        "\n",
        "        self.agconv1 = nn.Conv1d(AG_NUM_FEATURES, 64, 3, padding=1)   # antigen first a trous convolutional layer\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)  #antibody second a trous convolutional layer\n",
        "\n",
        "        self.agconv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)  #antigen second a trous convolutional layer\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)  #antibody third a trous convolutional layer\n",
        "\n",
        "        self.agconv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)\n",
        "\n",
        "        #self.agconv4 = nn.Conv1d(256, 32, 1)\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # batch normalisation after the first convolutional layer for antibody\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.agbn1 = nn.BatchNorm1d(64)  # batch normalisation after the first convolutional layer for antigen\n",
        "        self.agbn2 = nn.BatchNorm1d(128)\n",
        "        self.agbn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.elu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, 1, 1)  # dense prediction layer\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.aconv1 = nn.Conv1d(256, 1, 1)  # attentional mechanism\n",
        "        self.aconv2 = nn.Conv1d(32, 1, 1)\n",
        "\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool1d(2)\n",
        "        self.maxpool2 = nn.MaxPool1d(4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, ab_input, ab_unpacked_masks, ag_input, ag_unpacked_masks):\n",
        "        \"\"\"\n",
        "        Forward propagation step\n",
        "        :param ab_input: antibody amino acid sequences\n",
        "        :param ab_unpacked_masks: antibody amino acid masks\n",
        "        :param ag_input: antigen amino acid sequences\n",
        "        :param ag_unpacked_masks: antigen amino acid masks\n",
        "        :param dist: maskin\n",
        "        :return: antibody binding probabilities, attentional coefficients\n",
        "        \"\"\"\n",
        "        x=ab_input\n",
        "        agx = ag_input\n",
        "\n",
        "        ab_unpacked_masks = torch.transpose(ab_unpacked_masks, 1, 2)\n",
        "        ag_unpacked_masks = torch.transpose(ag_unpacked_masks, 1, 2)\n",
        "\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        agx = torch.transpose(agx, 1, 2)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn1(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        agx = self.agconv1(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn1(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn2(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        agx = self.conv2(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn2(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn3(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        agx = self.conv3(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn3(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "        # MaxPool\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        old = x\n",
        "\n",
        "        oldag = agx\n",
        "\n",
        "        heads_no = 1\n",
        "        #bias_mat = 1e9 * (ag_unpacked_masks - 1.0)\n",
        "        # dist_mat = 1e9 * (dist - 1.0)\n",
        "        #dist_mat = self.maxpool1(dist_mat)\n",
        "\n",
        "        for i in range(heads_no):\n",
        "            #agconvi = nn.Conv1d(256, 128, 1)\n",
        "            aconvi1 = nn.Conv1d(256, 1, 1)\n",
        "            aconvi2 = nn.Conv1d(256, 1, 1)\n",
        "            if use_cuda:\n",
        "                aconvi1.cuda()\n",
        "                aconvi2.cuda()\n",
        "                #agconvi.cuda()\n",
        "            #agx = agconvi(oldag)\n",
        "            w_1 = aconvi1(x)\n",
        "            w_2 = aconvi2(agx)\n",
        "            w = self.lrelu(w_2 + torch.transpose(w_1, 1, 2))\n",
        "            w = self.softmax(w)\n",
        "            w = self.dropout(w)\n",
        "            temp_loop_x = torch.bmm(w, torch.transpose(agx, 1, 2))\n",
        "            if i==0:\n",
        "                loop_x = temp_loop_x\n",
        "            else:\n",
        "                loop_x = torch.cat((loop_x, temp_loop_x), dim=2)\n",
        "\n",
        "        x = torch.transpose(loop_x, 1, 2)\n",
        "        #x = x + old\n",
        "        x = torch.cat((x, old), dim=1)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "\n",
        "        x = self.bn4(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        #print(\"x after fc\", x, file=track_f)\n",
        "\n",
        "        return x, w\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvXExyyIoAIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.set_printoptions(threshold=50000)\n",
        "import torch.optim as optim\n",
        "from torch import squeeze\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch import index_select\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XN9ppdOoFw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off):\n",
        "    \"\"\"\n",
        "    For every CDR get the corresponding seq and check if dist is less than cutoff\n",
        "    and substitute 1 orelse 0 \n",
        "    \n",
        "    E.g. - CDR - TCRASGNIHNYLAWY\n",
        "           Seq.no - ['22','23','24','25','26','27','28','29','30','31','32','33','34','35','36']\n",
        "           \n",
        "           Output - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    \"\"\"\n",
        "    pdb_name = cdr_loop['PDB_Name'].to_numpy()\n",
        "    seq_no = cdr_loop['seq_no'].to_numpy()\n",
        "    chain_type = cdr_loop['Type'].to_numpy()\n",
        "\n",
        "    all_var = []\n",
        "    for i in range(len(pdb_name)):\n",
        "        if chain_type[i] == 'H1' or chain_type[i] == 'H2' or chain_type[i] == 'H3':\n",
        "            dist = pd.read_pickle(dist_matx_path_heavy+pdb_name[i]+\"heavy\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "\n",
        "        elif chain_type[i] == 'L1' or chain_type[i] == 'L2' or chain_type[i] == 'L3':\n",
        "            dist = pd.read_pickle(dist_matx_path_light+pdb_name[i]+\"light\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "    \n",
        "    return all_var"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx3gYIBBIJie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def permute_training_ag_data(cdrs, masks, lengths, lbls, ag, ag_masks, ag_lengths):\n",
        "    index = torch.randperm(cdrs.shape[0])\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    lbls = torch.index_select(lbls, 0, index)\n",
        "    masks = torch.index_select(masks, 0, index)\n",
        "    lengths = [lengths[i] for i in index]\n",
        "\n",
        "    ag = torch.index_select(ag, 0, index)\n",
        "    ag_masks = torch.index_select(ag_masks, 0, index)\n",
        "    ag_lengths = [ag_lengths[i] for i in index]\n",
        "\n",
        "    # dist= torch.index_select(dist, 0, index)\n",
        "\n",
        "    return cdrs, masks, lengths, lbls, ag, ag_masks, ag_lengths\n",
        "\n",
        "\n",
        "def flatten_with_lengths(matrix, lengths):\n",
        "    seqs = []\n",
        "    for i, example in enumerate(matrix):\n",
        "        seq = example[:lengths[i]]\n",
        "        seqs.append(seq)\n",
        "    return np.concatenate(seqs)\n",
        "\n",
        "def sort_ag_batch(cdrs, masks, lengths, lbls, ag, ag_masks):\n",
        "    order = np.argsort(lengths)\n",
        "    order = order.tolist()\n",
        "    order.reverse()\n",
        "\n",
        "    lengths.sort(reverse=True)\n",
        "\n",
        "    index = Variable(torch.LongTensor(order))\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    lbls = torch.index_select(lbls, 0, index)\n",
        "    masks = torch.index_select(masks, 0, index)\n",
        "\n",
        "    ag = torch.index_select(ag, 0, index)\n",
        "    ag_masks = torch.index_select(ag_masks, 0, index)\n",
        "\n",
        "    # dist= torch.index_select(dist, 0, index)\n",
        "\n",
        "    return cdrs, masks, lengths, lbls, ag, ag_masks\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXJh24-qYiYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def antigen_run(cdrs_train, lbls_train, masks_train, lengths_train,\n",
        "               ag_train, ag_masks_train, ag_lengths_train,\n",
        "               weights_template, weights_template_number,\n",
        "               cdrs_test, lbls_test, masks_test, lengths_test,\n",
        "               ag_test, ag_masks_test, ag_lengths_test):\n",
        "    \"\"\"\n",
        "    :param cdrs_train: antibody amino acids used for training\n",
        "    :param lbls_train: ground truth values for antibody amino acids used for training\n",
        "    :param masks_train: amino acids' masks\n",
        "    :param lengths_train: amino acids' lengths\n",
        "    :param ag_train: antigen amino acids used for training\n",
        "    :param ag_masks_train: antigen amino acids' masks\n",
        "    :param ag_lengths_train: antigen amino acids' lengths\n",
        "    :param dist_mat_train:\n",
        "    :param weights_template: template for printing weights\n",
        "    :param weights_template_number: which file to print weights to\n",
        "    :param cdrs_test: antibody amino acids used for testing\n",
        "    :param lbls_test:\n",
        "    :param masks_test:\n",
        "    :param lengths_test:\n",
        "    :param ag_test:\n",
        "    :param ag_masks_test:\n",
        "    :param ag_lengths_test:\n",
        "    :param dist_test:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"dilated run\", file=print_file)\n",
        "    print(\"dilated run\")\n",
        "\n",
        "\n",
        "    model = AG()\n",
        "\n",
        "    ignored_params = list(map(id, [model.conv1.weight, model.conv2.weight, model.conv3.weight,\n",
        "                                   model.agconv1.weight, model.agconv2.weight, model.agconv3.weight,\n",
        "                                   model.aconv1.weight, model.aconv2.weight]))\n",
        "    base_params = filter(lambda p: id(p) not in ignored_params,\n",
        "                         model.parameters())\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': base_params},\n",
        "        {'params': model.conv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.conv2.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.conv3.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.agconv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.agconv2.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.agconv3.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.aconv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.aconv2.weight, 'weight_decay': 0.01}\n",
        "    ], lr=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "\n",
        "    total_input = cdrs_train\n",
        "    total_lbls = lbls_train\n",
        "    total_masks = masks_train\n",
        "    total_lengths = lengths_train\n",
        "    # total_dist_train = dist_mat_train\n",
        "\n",
        "    total_ag_input = ag_train\n",
        "    total_ag_masks = ag_masks_train\n",
        "    total_ag_lengths = ag_lengths_train\n",
        "\n",
        "    if use_cuda:\n",
        "        print(\"using cuda\")\n",
        "        model.cuda()\n",
        "        total_input = total_input.cuda()\n",
        "        total_lbls = total_lbls.cuda()\n",
        "        total_masks = total_masks.cuda()\n",
        "        cdrs_test = cdrs_test.cuda()\n",
        "        lbls_test = lbls_test.cuda()\n",
        "        masks_test = masks_test.cuda()\n",
        "\n",
        "        total_ag_input = total_ag_input.cuda()\n",
        "        total_ag_masks = total_ag_masks.cuda()\n",
        "        ag_test = ag_test.cuda()\n",
        "        ag_masks_test = ag_masks_test.cuda()\n",
        "\n",
        "        # total_dist_train = total_dist_train.cuda()\n",
        "        # dist_test = dist_test.cuda()\n",
        "\n",
        "    times = []\n",
        "\n",
        "    for epoch in range(epochs):  # training iterations\n",
        "        model.train(True)\n",
        "        scheduler.step()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        batches_done=0\n",
        "\n",
        "        total_input, total_masks, total_lengths, total_lbls,\\\n",
        "        total_ag_input, total_ag_masks, total_ag_lengths = \\\n",
        "            permute_training_ag_data(total_input, total_masks, total_lengths, total_lbls,\n",
        "                                     total_ag_input, total_ag_masks, total_ag_lengths)\n",
        "\n",
        "        total_time = 0\n",
        "\n",
        "        for j in range(0, cdrs_train.shape[0], batch_size):\n",
        "            batches_done +=1\n",
        "            interval = [x for x in range(j, min(cdrs_train.shape[0], j + batch_size))]\n",
        "            interval = torch.LongTensor(interval)\n",
        "            if use_cuda:\n",
        "                interval = interval.cuda()\n",
        "\n",
        "            input = Variable(index_select(total_input, 0, interval), requires_grad=True)\n",
        "            lbls = Variable(index_select(total_lbls, 0, interval))\n",
        "            masks = Variable(index_select(total_masks, 0, interval))\n",
        "            lengths = total_lengths[j:j + batch_size]\n",
        "\n",
        "            ag_input = Variable(index_select(total_ag_input, 0, interval), requires_grad=True)\n",
        "            ag_masks = Variable(index_select(total_ag_masks, 0, interval))\n",
        "\n",
        "            # dist = Variable(index_select(total_dist_train, 0, interval))\n",
        "\n",
        "            input, masks, lengths, lbls, ag, ag_masks = \\\n",
        "                sort_ag_batch(input, masks, list(lengths), lbls, ag_input, ag_masks)\n",
        "\n",
        "            output, _ = model(input, masks, ag_input, ag_masks )\n",
        "\n",
        "            loss_weights = (lbls * 1.5 + 1) * masks\n",
        "            max_val = (-output).clamp(min=0)\n",
        "            loss = loss_weights * \\\n",
        "                   (output - output * lbls + max_val + ((-max_val).exp() + (-output - max_val).exp()).log())\n",
        "            masks_added = masks.sum()\n",
        "            loss = loss.sum() / masks_added\n",
        "            #print(\"Epoch %d - Batch %d has loss %d \" % (epoch, j, loss.data), file=monitoring_file)\n",
        "            epoch_loss +=loss\n",
        "            model.zero_grad()\n",
        "\n",
        "            start_time =time.time()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_time += time.time() - start_time\n",
        "            # print(\"Total time\", total_time)\n",
        "\n",
        "        print(\"Epoch %d - loss is %f : \" % (epoch, epoch_loss.data/batches_done))\n",
        "        # print(\"--- %s seconds ---\" % (total_time))\n",
        "        times.append(total_time)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        cdrs_test2, masks_test2, lengths_test2, lbls_test2, ag_test2, ag_masks_test2 = \\\n",
        "            sort_ag_batch(cdrs_test, masks_test, list(lengths_test), lbls_test, ag_test, ag_masks_test)\n",
        "\n",
        "\n",
        "        probs_test2, _= model(cdrs_test2, masks_test2, ag_test2, ag_masks_test2)\n",
        "\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        probs_test2 = sigmoid(probs_test2)\n",
        "\n",
        "        probs_test2 = probs_test2.data.cpu().numpy().astype('float32')\n",
        "        lbls_test2 = lbls_test2.data.cpu().numpy().astype('int32')\n",
        "\n",
        "        probs_test2 = flatten_with_lengths(probs_test2, lengths_test2)\n",
        "        lbls_test2 = flatten_with_lengths(lbls_test2, lengths_test2)\n",
        "\n",
        "        print(\"Roc\", roc_auc_score(lbls_test2, probs_test2))\n",
        "\n",
        "    print(\"Saving\")\n",
        "\n",
        "    torch.save(model.state_dict(), weights_template.format(weights_template_number))\n",
        "\n",
        "    times_mean = np.mean(times)\n",
        "    times_std = 2 * np.std(times)\n",
        "\n",
        "    print(\"Time mean\", times_mean)\n",
        "    print(\"Time std\", times_std)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    cdrs_test, masks_test, lengths_test, lbls_test, ag_test, ag_masks_test = \\\n",
        "        sort_ag_batch(cdrs_test, masks_test, list(lengths_test), lbls_test, ag_test, ag_masks_test)\n",
        "\n",
        "    probs_test, _ = model(cdrs_test, masks_test, ag_test, ag_masks_test)\n",
        "\n",
        "\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    probs_test = sigmoid(probs_test)\n",
        "\n",
        "    #print(\"probs\", probs_test, file=track_f)\n",
        "\n",
        "    probs_test1 = probs_test.data.cpu().numpy().astype('float32')\n",
        "    lbls_test1 = lbls_test.data.cpu().numpy().astype('int32')\n",
        "\n",
        "    probs_test1 = flatten_with_lengths(probs_test1, list(lengths_test))\n",
        "    lbls_test1 = flatten_with_lengths(lbls_test1, list(lengths_test))\n",
        "\n",
        "    print(\"Roc\", roc_auc_score(lbls_test1, probs_test1))\n",
        "\n",
        "    return probs_test, lbls_test, probs_test1, lbls_test1  # get them in kfold, append, concatenate do roc on them"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qshUopP3ZvYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def kfold_cv_eval(cdrs, lbls, masks, lengths, ag, ag_masks, ag_lengths, output_file=\"crossval-data.p\",\n",
        "                  weights_template=\"weights-fold-{}.h5\", seed=0):\n",
        "    \"\"\"\n",
        "    Performs 10-fold cross-vallidation\n",
        "    :param dataset: contains antibody amino acids, ground truth values, antigen atoms\n",
        "    :param output_file: where to print weights\n",
        "    :param weights_template:\n",
        "    :param seed: cv\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # cdrs, lbls, masks, lengths, ag, ag_masks, ag_lengths, dist_mat = \\\n",
        "    #     dataset[\"cdrs\"], dataset[\"lbls\"], dataset[\"masks\"], dataset[\"lengths\"],\\\n",
        "    #     dataset[\"ag\"], dataset[\"ag_masks\"], dataset[\"ag_lengths\"], dataset[\"dist_mat\"]\n",
        "\n",
        "\n",
        "    print(\"cdrs\", cdrs.shape)\n",
        "    print(\"ag\", ag.shape)\n",
        "    #print(\"lbls\", lbls, file=data_file)\n",
        "    #print(\"masks\", masks, file=data_file)\n",
        "    #print(\"lengths\", lengths, file=data_file)\n",
        "\n",
        "    kf = KFold(n_splits=NUM_SPLIT, random_state=seed, shuffle=True)\n",
        "\n",
        "    all_lbls2 = []\n",
        "    all_probs2 = []\n",
        "    all_masks = []\n",
        "\n",
        "    all_probs1 = []\n",
        "    all_lbls1 = []\n",
        "\n",
        "    for i, (train_idx, test_idx) in enumerate(kf.split(cdrs)):\n",
        "        print(\"Fold: \", i + 1)\n",
        "        #print(train_idx, )\n",
        "        #print(test_idx)\n",
        "\n",
        "        lengths_train = [lengths[i] for i in train_idx]\n",
        "        lengths_test = [lengths[i] for i in test_idx]\n",
        "\n",
        "        ag_lengths_train = [ag_lengths[i] for i in train_idx]\n",
        "        ag_lengths_test = [ag_lengths[i] for i in test_idx]\n",
        "\n",
        "        #print(\"train_idx\", train_idx)\n",
        "\n",
        "        print(\"len(train_idx\",len(train_idx))\n",
        "\n",
        "        train_idx = torch.from_numpy(train_idx)\n",
        "        test_idx = torch.from_numpy(test_idx)\n",
        "\n",
        "        cdrs_train = index_select(cdrs, 0, train_idx)\n",
        "        lbls_train = index_select(lbls, 0, train_idx)\n",
        "        mask_train = index_select(masks, 0, train_idx)\n",
        "        ag_train = index_select(ag, 0, train_idx)\n",
        "        ag_masks_train = index_select(ag_masks, 0, train_idx)\n",
        "        # dist_mat_train = index_select(dist_mat, 0, train_idx)\n",
        "\n",
        "        cdrs_test = Variable(index_select(cdrs, 0, test_idx))\n",
        "        lbls_test = Variable(index_select(lbls, 0, test_idx))\n",
        "        mask_test = Variable(index_select(masks, 0, test_idx))\n",
        "        ag_test = Variable(index_select(ag, 0, test_idx))\n",
        "        ag_masks_test = Variable(index_select(ag_masks, 0, test_idx))\n",
        "        # dist_mat_test = Variable(index_select(dist_mat, 0, test_idx))\n",
        "\n",
        "        code = 4\n",
        "        if code ==1:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                simple_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                    cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "        if code == 2:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                attention_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                              cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 3:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                atrous_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                     cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 4:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                antigen_run(cdrs_train, lbls_train, mask_train, lengths_train,\n",
        "                            ag_train, ag_masks_train, ag_lengths_train, weights_template, i,\n",
        "                           cdrs_test, lbls_test, mask_test, lengths_test,\n",
        "                            ag_test, ag_masks_test, ag_lengths_test)\n",
        "\n",
        "        if code == 5:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                atrous_self_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                           cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 6:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                rnn_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                    cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 7:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                xself_run(cdrs_train, lbls_train, mask_train, lengths_train,\n",
        "                            ag_train, ag_masks_train, ag_lengths_train, dist_mat_train, weights_template, i,\n",
        "                            cdrs_test, lbls_test, mask_test, lengths_test,\n",
        "                            ag_test, ag_masks_test, ag_lengths_test, dist_mat_test)\n",
        "\n",
        "        # print(\"test\", file=track_f)\n",
        "        print(\"test\")\n",
        "\n",
        "        lbls_test2 = np.squeeze(lbls_test2)\n",
        "        all_lbls2 = np.concatenate((all_lbls2, lbls_test2))\n",
        "        all_lbls1.append(lbls_test1)\n",
        "\n",
        "        probs_test_pad = torch.zeros(probs_test1.data.shape[0], MAX_CDR_LENGTH, probs_test1.data.shape[2])\n",
        "        probs_test_pad[:probs_test1.data.shape[0], :probs_test1.data.shape[1], :] = probs_test1.data\n",
        "        probs_test_pad = Variable(probs_test_pad)\n",
        "\n",
        "        probs_test2 = np.squeeze(probs_test2)\n",
        "        #print(probs_test)\n",
        "        all_probs2 = np.concatenate((all_probs2, probs_test2))\n",
        "        #print(all_probs)\n",
        "        #print(type(all_probs))\n",
        "\n",
        "        all_probs1.append(probs_test_pad)\n",
        "\n",
        "        all_masks.append(mask_test)\n",
        "\n",
        "    lbl_mat1 = torch.cat(all_lbls1)\n",
        "    prob_mat1 = torch.cat(all_probs1)\n",
        "    #print(\"end\", all_probs)\n",
        "    mask_mat = torch.cat(all_masks)\n",
        "\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump((lbl_mat1, prob_mat1, mask_mat, all_lbls2, all_probs2), f)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pHuIuYsaQwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UteOMmqfbf1x",
        "colab_type": "text"
      },
      "source": [
        "**Building the input Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVMC3S2Kbemr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_files = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/summary_file_cleaned.pkl\")\n",
        "summary_files = summary_files.drop(summary_files[summary_files.pdb.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "\n",
        "pdb_names = summary_files['pdb'].to_numpy()\n",
        "Hchain_id = summary_files['Hchain'].to_numpy()\n",
        "Lchain_id = summary_files['Lchain'].to_numpy()\n",
        "Achain_id = summary_files['antigen_chain_list'].to_numpy()\n",
        " \n",
        "epi_res = []\n",
        "epi_res_num = []\n",
        "ext_epi_res_num = []\n",
        "ext_epi_res = []\n",
        "A_pdb = []\n",
        "# for i in range(len(pdb_names)):\n",
        "#     epi_residues,epi_res_number,ext_epi_res_number, ext_epi_residues = ext_epitope_sorted(Hchain_id[i],Lchain_id[i],Achain_id[i],pdb_names[i])\n",
        "#     A_pdb.append(pdb_names[i])\n",
        "#     epi_res.append(epi_residues)\n",
        "#     epi_res_num.append(epi_res_number)\n",
        "#     ext_epi_res_num.append(ext_epi_res_number)\n",
        "#     ext_epi_res.append(ext_epi_residues)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8GDVKcgbdYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorted_Antigen_dataframe = pd.DataFrame(list(zip(A_pdb,epi_res,epi_res_num, ext_epi_res_num, ext_epi_res)),\n",
        "#                columns =['pdb','sorted_epi_residues','sorted_epi_res_number', 'sorted_ext_epi_res_number', 'sorted_ext_epi_residues'])\n",
        "# Sorted_Antigen_dataframe.to_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT8WZ1Gubs3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2bbd61fe-326d-47ee-9304-fd03a5d2adb0"
      },
      "source": [
        " Sorted_epi = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")\n",
        " Sorted_epi.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pdb</th>\n",
              "      <th>sorted_epi_residues</th>\n",
              "      <th>sorted_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_residues</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4ydk</td>\n",
              "      <td>[Q, L, T, G, G, S, T, E, N, A, K, T, Q, P, S, ...</td>\n",
              "      <td>[105 , 122 , 123 , 124 , 198 , 199 , 257 , 275...</td>\n",
              "      <td>[52 , 54 , 65 , 66 , 69 , 93 , 94 , 95 , 96 , ...</td>\n",
              "      <td>[L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4ydj</td>\n",
              "      <td>[W, K, E, E, N, N, A, K, T, I, R, P, S, G, G, ...</td>\n",
              "      <td>[96 , 97 , 102 , 275 , 279 , 280 , 281 , 282 ,...</td>\n",
              "      <td>[47 , 48 , 49 , 50 , 51 , 52 , 69 , 92 , 93 , ...</td>\n",
              "      <td>[D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1u8i</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1u8l</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5y9j</td>\n",
              "      <td>[K, G, S, Y, T, Y, A, M, G, H, L, Q, K, V, G, ...</td>\n",
              "      <td>[160 , 161 , 162 , 163 , 205 , 206 , 207 , 208...</td>\n",
              "      <td>[156 , 157 , 158 , 159 , 160 , 161 , 162 , 163...</td>\n",
              "      <td>[P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pdb  ...                            sorted_ext_epi_residues\n",
              "0  4ydk  ...  [L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...\n",
              "1  4ydj  ...  [D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...\n",
              "2  1u8i  ...                              [E, L, D, K, W, A, N]\n",
              "3  1u8l  ...                              [D, L, D, R, W, A, S]\n",
              "4  5y9j  ...  [P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSn9eF0JbuX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epi_res = Sorted_epi['sorted_epi_residues'].to_numpy()\n",
        "epi_res_num = Sorted_epi['sorted_epi_res_number'].to_numpy()\n",
        "ext_epi_res_num = Sorted_epi['sorted_ext_epi_res_number'].to_numpy()\n",
        "ext_epi_res = Sorted_epi['sorted_ext_epi_residues'].to_numpy()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMudhicybwSw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74ee6914-821c-41be-802e-fb828c11f343"
      },
      "source": [
        "##Joining all the residue abbrevations for making a sequence and repeating each seq 6 times/pdb\n",
        "\n",
        "Anti_str = []\n",
        "for i in range(len(ext_epi_res)):\n",
        "#     print(i)\n",
        "    str1 = ''.join(ext_epi_res[i])\n",
        "#     print(repeat(str1, 6))\n",
        "#     print(str1,i,len(A_mat[i]))\n",
        "    Anti_str.append(str1)\n",
        "\n",
        "import itertools\n",
        "Anti_seq = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in Anti_str))\n",
        "\n",
        "MAX_EXT_AG_LENGTH = maxi_len(Anti_seq)\n",
        "MAX_EXT_AG_LENGTH"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltTHrKblbyfs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e58e692f-2768-44e8-cf7f-52477941cbaa"
      },
      "source": [
        "## Generating Labels for Extended Epitope Antigen Matrix\n",
        "\n",
        "labels = []\n",
        "for i in range(len(ext_epi_res_num)):\n",
        "    label = []\n",
        "    for j in ext_epi_res_num[i]:\n",
        "#     print(i)\n",
        "        if j in epi_res_num[i]:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    labels.append(label)\n",
        "\n",
        "import itertools\n",
        "Anti_repeat_labels = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in labels))\n",
        "\n",
        "# Antigen_lbls = np.stack(labels)\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "label_mats = []\n",
        "for i in range(len(Anti_repeat_labels)):\n",
        "    label_mat = torch.tensor(Anti_repeat_labels[i])\n",
        "    label_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, 1))\n",
        "    label_mat_pad[:label_mat.shape[0], 0] = label_mat\n",
        "    label_mats.append(label_mat_pad)\n",
        "\n",
        "ext_epi_lbls = torch.stack(label_mats)\n",
        "ext_epi_lbls.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga1lhG8mb0bR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea3b9710-535f-4b0e-8050-44cf769140f1"
      },
      "source": [
        "NUM_FEATURES = 28\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "# cdr_mats = []\n",
        "ext_epi_mats = []\n",
        "\n",
        "ext_epi_lengths = []\n",
        "for i in range(len(Anti_seq)):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "#     print(i)\n",
        "    ext_epi_mat = seq_to_one_hot(Anti_seq[i])\n",
        "    ext_epi_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, NUM_FEATURES))\n",
        "    ext_epi_mat_pad[:ext_epi_mat.shape[0], :] = ext_epi_mat\n",
        "    ext_epi_mats.append(ext_epi_mat_pad)\n",
        "    ext_epi_lengths.append(ext_epi_mat.shape[0])\n",
        "\n",
        "ext_epi = torch.stack(ext_epi_mats)\n",
        "ext_epi.shape\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8Vl9iOb2uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb1ac69d-7275-41e5-d181-6ea796e0e721"
      },
      "source": [
        "max(ext_epi_lengths)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MyLXjpLb4kc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "321cebc2-1d51-4e97-a470-ff850d587fb7"
      },
      "source": [
        "ext_epi_masks = []\n",
        "for i in range(len(Anti_seq)):\n",
        "    ext_epi_mask = torch.zeros((MAX_EXT_AG_LENGTH, 1), dtype=int)\n",
        "    ext_epi_mask[:len(Anti_seq[i]), 0] = 1\n",
        "    ext_epi_masks.append(ext_epi_mask)\n",
        "ext_epi_masks = torch.stack(ext_epi_masks)\n",
        "ext_epi_masks.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYEaye-Qb6O1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ed81113-4a59-43d3-dc18-dd9e932fe8c4"
      },
      "source": [
        "NUM_FEATURES = 28+6\n",
        "Max_len_CDR = 38\n",
        "cdr_mats = []\n",
        "\n",
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "cdrs_seq = torch.stack(cdr_mats)\n",
        "cdrs_seq.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 34])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk3SbBsab8N6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf92d594-ffab-4402-8f6e-d40ea7a5acfc"
      },
      "source": [
        "all_cdrs_lengths = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "    all_cdrs_lengths.append(cdr_mat.shape[0])\n",
        "max(all_cdrs_lengths)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1il5jhBDb-YB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b401ee91-b6ef-4619-8eb3-4cd376ba91d0"
      },
      "source": [
        "Max_len_CDR = 38\n",
        "\n",
        "cdr_masks = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "    cdr_mask = torch.zeros((Max_len_CDR, 1), dtype=int)\n",
        "    cdr_mask[:len(cdr_loop['CDR'][i]), 0] = 1\n",
        "    cdr_masks.append(cdr_mask)\n",
        "cdr_mask = torch.stack(cdr_masks)\n",
        "cdr_mask.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idRfuf80cAYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\n",
        "                                                          \"5e08\",\"5gkr\",\"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dist_matx_path_light = \"/content/drive/My Drive/Peritia_Fast-Parapred/Light_chain_dist_clean/\"\n",
        "dist_matx_path_heavy = \"/content/drive/My Drive/Peritia_Fast-Parapred/Heavy_chain_dist_clean/\"\n",
        "cut_off = 5\n",
        "# all_variables = get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off)\n",
        "with open('/content/drive/My Drive/Peritia_Fast-Parapred/all_variables.pkl', 'rb') as f:\n",
        "    all_variables = pickle.load(f)\n",
        "# all_variables\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvRy5PsIcCNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b710b708-4ba1-4c1c-cc7e-031575743631"
      },
      "source": [
        "cont_mats = []\n",
        "cdr_lengths = []\n",
        "Max_len_CDR = 38\n",
        "\"\"\"\n",
        "Pad the variables to Max CDR len with 0\n",
        "Here max len of CDR is 38\n",
        "\n",
        "E.g - Input = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    Output = [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[1.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.]\n",
        "\n",
        "\"\"\"\n",
        "for i in range(len(all_variables)):\n",
        "    cont_mat = torch.tensor(all_variables[i])\n",
        "    cont_mat_pad = torch.zeros((Max_len_CDR, 1))\n",
        "    cont_mat_pad[:cont_mat.shape[0], 0] = cont_mat\n",
        "    cont_mats.append(cont_mat_pad)\n",
        "    cdr_lengths.append(cont_mat.shape[0])\n",
        "\n",
        "\n",
        "cdr_lbls = torch.stack(cont_mats)\n",
        "\n",
        "cdr_lbls.shape\n",
        "# cdr_lengths"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQZKgRN3cDt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def structure_ids_to_selection_mask(idx, num_structures):\n",
        "    mask = np.zeros((num_structures * 6, ), dtype=np.bool)\n",
        "    offset = idx * 6\n",
        "    for i in range(6):\n",
        "        mask[offset + i] = True\n",
        "    return mask\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxWl7F4_cFew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "AG_NUM_FEATURES = 28\n",
        "\n",
        "NUM_FEATURES = 34\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 16\n",
        "\n",
        "batch_size = 3"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIrxKKWccHKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_ITERATIONS = 10\n",
        "NUM_SPLIT = 10\n",
        "output_file=\"crossval-data.p\",\n",
        "# weights_template=\"weights-fold-{}.h5\"\n",
        "import os \n",
        "def run_cv(output_folder=\"cv-ab-seq\",\n",
        "           num_iters=NUM_ITERATIONS):\n",
        "#     cache_file = dataset.split(\"/\")[-1] + \".p\"\n",
        "#     dataset = open_dataset(dataset_cache=cache_file)\n",
        "\n",
        "    dir =  output_folder + \"/weights\"\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(output_folder + \"/weights\")\n",
        "    for i in range(num_iters):\n",
        "        #i=0\n",
        "        print(\"Crossvalidation run\", i+1)\n",
        "        output_file = \"{}/run-{}.p\".format(output_folder, i)\n",
        "        weights_template = output_folder + \"/weights/run-\" + \\\n",
        "                           str(i) + \"-fold-{}.pth.tar\"\n",
        "        kfold_cv_eval(cdrs_seq, cdr_lbls, cdr_mask, all_cdrs_lengths, ext_epi, ext_epi_masks, ext_epi_lengths, output_file,\n",
        "                  weights_template, seed=0)\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4clQIB_tcrtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1797450a-0c0f-49e8-8b12-98358ecd715d"
      },
      "source": [
        "import time\n",
        "import os\n",
        "start_time = time.time()\n",
        "run_cv(output_folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq\",\n",
        "           num_iters=NUM_ITERATIONS)\n",
        "end_time = time.time() - start_time\n",
        "hours, rem = divmod(end_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crossvalidation run 1\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.035646 : \n",
            "Roc 0.7493498851848668\n",
            "Epoch 1 - loss is 0.897764 : \n",
            "Roc 0.7676263523821353\n",
            "Epoch 2 - loss is 0.905175 : \n",
            "Roc 0.7435882277065956\n",
            "Epoch 3 - loss is 0.907558 : \n",
            "Roc 0.798775132730116\n",
            "Epoch 4 - loss is 0.913956 : \n",
            "Roc 0.771019051986891\n",
            "Epoch 5 - loss is 0.907963 : \n",
            "Roc 0.7860799788523583\n",
            "Epoch 6 - loss is 0.901644 : \n",
            "Roc 0.7958515070879635\n",
            "Epoch 7 - loss is 0.903412 : \n",
            "Roc 0.7640510194819464\n",
            "Epoch 8 - loss is 0.907618 : \n",
            "Roc 0.7655402521792581\n",
            "Epoch 9 - loss is 0.876123 : \n",
            "Roc 0.7981744831726558\n",
            "Epoch 10 - loss is 0.861363 : \n",
            "Roc 0.8050146226069564\n",
            "Epoch 11 - loss is 0.854567 : \n",
            "Roc 0.8108393806352572\n",
            "Epoch 12 - loss is 0.844756 : \n",
            "Roc 0.8253344926317666\n",
            "Epoch 13 - loss is 0.834102 : \n",
            "Roc 0.8214016562998634\n",
            "Epoch 14 - loss is 0.837347 : \n",
            "Roc 0.8330728693910181\n",
            "Epoch 15 - loss is 0.821929 : \n",
            "Roc 0.8260018257357882\n",
            "Saving\n",
            "Time mean 5.798071280121803\n",
            "Time std 0.15881810646845942\n",
            "Roc 0.8260340727045732\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.028017 : \n",
            "Roc 0.7519562840170984\n",
            "Epoch 1 - loss is 0.892830 : \n",
            "Roc 0.7732410392223563\n",
            "Epoch 2 - loss is 0.902652 : \n",
            "Roc 0.7511111717285821\n",
            "Epoch 3 - loss is 0.901502 : \n",
            "Roc 0.7659734947433955\n",
            "Epoch 4 - loss is 0.905287 : \n",
            "Roc 0.756577474743909\n",
            "Epoch 5 - loss is 0.892923 : \n",
            "Roc 0.7762836239762778\n",
            "Epoch 6 - loss is 0.894646 : \n",
            "Roc 0.7542009913095941\n",
            "Epoch 7 - loss is 0.892447 : \n",
            "Roc 0.7683137748389002\n",
            "Epoch 8 - loss is 0.896376 : \n",
            "Roc 0.7683206945906396\n",
            "Epoch 9 - loss is 0.858994 : \n",
            "Roc 0.7845364809311699\n",
            "Epoch 10 - loss is 0.840907 : \n",
            "Roc 0.8002777127031399\n",
            "Epoch 11 - loss is 0.821603 : \n",
            "Roc 0.8131194682100586\n",
            "Epoch 12 - loss is 0.807288 : \n",
            "Roc 0.8230888247413417\n",
            "Epoch 13 - loss is 0.800884 : \n",
            "Roc 0.8253984172421761\n",
            "Epoch 14 - loss is 0.794041 : \n",
            "Roc 0.8165628971335781\n",
            "Epoch 15 - loss is 0.786598 : \n",
            "Roc 0.8262275840158146\n",
            "Saving\n",
            "Time mean 5.777051463723183\n",
            "Time std 0.08222042805215858\n",
            "Roc 0.8265339585312315\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.020208 : \n",
            "Roc 0.7824583851248343\n",
            "Epoch 1 - loss is 0.904734 : \n",
            "Roc 0.7428178647481839\n",
            "Epoch 2 - loss is 0.896435 : \n",
            "Roc 0.7665814603738423\n",
            "Epoch 3 - loss is 0.894957 : \n",
            "Roc 0.7691361272779225\n",
            "Epoch 4 - loss is 0.897287 : \n",
            "Roc 0.7345745030964563\n",
            "Epoch 5 - loss is 0.899455 : \n",
            "Roc 0.7457245616698379\n",
            "Epoch 6 - loss is 0.905489 : \n",
            "Roc 0.7657057131780027\n",
            "Epoch 7 - loss is 0.898192 : \n",
            "Roc 0.7764628411913738\n",
            "Epoch 8 - loss is 0.902365 : \n",
            "Roc 0.7363196246129191\n",
            "Epoch 9 - loss is 0.863233 : \n",
            "Roc 0.792756951475216\n",
            "Epoch 10 - loss is 0.836084 : \n",
            "Roc 0.8122014498196002\n",
            "Epoch 11 - loss is 0.815941 : \n",
            "Roc 0.8093709420479263\n",
            "Epoch 12 - loss is 0.818536 : \n",
            "Roc 0.8083023915973926\n",
            "Epoch 13 - loss is 0.813507 : \n",
            "Roc 0.817920677297764\n",
            "Epoch 14 - loss is 0.805348 : \n",
            "Roc 0.822807436061038\n",
            "Epoch 15 - loss is 0.804428 : \n",
            "Roc 0.8231454838001513\n",
            "Saving\n",
            "Time mean 5.767199665307999\n",
            "Time std 0.11045893937543191\n",
            "Roc 0.8231455789176918\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.023914 : \n",
            "Roc 0.7334394868154269\n",
            "Epoch 1 - loss is 0.887273 : \n",
            "Roc 0.7301386265024815\n",
            "Epoch 2 - loss is 0.895910 : \n",
            "Roc 0.7144855575445821\n",
            "Epoch 3 - loss is 0.901512 : \n",
            "Roc 0.7487190236631266\n",
            "Epoch 4 - loss is 0.896791 : \n",
            "Roc 0.7612259125347195\n",
            "Epoch 5 - loss is 0.898184 : \n",
            "Roc 0.720682102456094\n",
            "Epoch 6 - loss is 0.912287 : \n",
            "Roc 0.6674853712207512\n",
            "Epoch 7 - loss is 0.902588 : \n",
            "Roc 0.7644665320732497\n",
            "Epoch 8 - loss is 0.895096 : \n",
            "Roc 0.6972580718706834\n",
            "Epoch 9 - loss is 0.850675 : \n",
            "Roc 0.7877837409265405\n",
            "Epoch 10 - loss is 0.823770 : \n",
            "Roc 0.8112204486080352\n",
            "Epoch 11 - loss is 0.808321 : \n",
            "Roc 0.8180014174762318\n",
            "Epoch 12 - loss is 0.798311 : \n",
            "Roc 0.8234448052994585\n",
            "Epoch 13 - loss is 0.792372 : \n",
            "Roc 0.8262722890865336\n",
            "Epoch 14 - loss is 0.787281 : \n",
            "Roc 0.8300154719535463\n",
            "Epoch 15 - loss is 0.781504 : \n",
            "Roc 0.8355020012122841\n",
            "Saving\n",
            "Time mean 5.707932636141777\n",
            "Time std 0.0670686399816364\n",
            "Roc 0.8355207711762537\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.029276 : \n",
            "Roc 0.6976871907799451\n",
            "Epoch 1 - loss is 0.897264 : \n",
            "Roc 0.7471912175887366\n",
            "Epoch 2 - loss is 0.903429 : \n",
            "Roc 0.7639936025895993\n",
            "Epoch 3 - loss is 0.899183 : \n",
            "Roc 0.7529306973908046\n",
            "Epoch 4 - loss is 0.897274 : \n",
            "Roc 0.7453761692555039\n",
            "Epoch 5 - loss is 0.896412 : \n",
            "Roc 0.7642076376217906\n",
            "Epoch 6 - loss is 0.892178 : \n",
            "Roc 0.7621071441201128\n",
            "Epoch 7 - loss is 0.898141 : \n",
            "Roc 0.7569072944037704\n",
            "Epoch 8 - loss is 0.899340 : \n",
            "Roc 0.7579055709870484\n",
            "Epoch 9 - loss is 0.852657 : \n",
            "Roc 0.8047059768232592\n",
            "Epoch 10 - loss is 0.818668 : \n",
            "Roc 0.8237523594163008\n",
            "Epoch 11 - loss is 0.794359 : \n",
            "Roc 0.8425490507531669\n",
            "Epoch 12 - loss is 0.784921 : \n",
            "Roc 0.8369398871231406\n",
            "Epoch 13 - loss is 0.783702 : \n",
            "Roc 0.8455915523860267\n",
            "Epoch 14 - loss is 0.774522 : \n",
            "Roc 0.843732544324594\n",
            "Epoch 15 - loss is 0.773213 : \n",
            "Roc 0.8417294460429504\n",
            "Saving\n",
            "Time mean 5.740725293755531\n",
            "Time std 0.13338339618345083\n",
            "Roc 0.8421648427286995\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.015918 : \n",
            "Roc 0.755179454959554\n",
            "Epoch 1 - loss is 0.901163 : \n",
            "Roc 0.7177038538232708\n",
            "Epoch 2 - loss is 0.907331 : \n",
            "Roc 0.7542421938002463\n",
            "Epoch 3 - loss is 0.904213 : \n",
            "Roc 0.7414835275345607\n",
            "Epoch 4 - loss is 0.899455 : \n",
            "Roc 0.7636154758233359\n",
            "Epoch 5 - loss is 0.900900 : \n",
            "Roc 0.7756669342001674\n",
            "Epoch 6 - loss is 0.908676 : \n",
            "Roc 0.7625865573035688\n",
            "Epoch 7 - loss is 0.899981 : \n",
            "Roc 0.7414138028131243\n",
            "Epoch 8 - loss is 0.907691 : \n",
            "Roc 0.7743160177223385\n",
            "Epoch 9 - loss is 0.860338 : \n",
            "Roc 0.8134158792306528\n",
            "Epoch 10 - loss is 0.838001 : \n",
            "Roc 0.8240283074706973\n",
            "Epoch 11 - loss is 0.812993 : \n",
            "Roc 0.8229106048619584\n",
            "Epoch 12 - loss is 0.807164 : \n",
            "Roc 0.828049546693535\n",
            "Epoch 13 - loss is 0.794469 : \n",
            "Roc 0.8286003528377387\n",
            "Epoch 14 - loss is 0.792001 : \n",
            "Roc 0.8316582799064463\n",
            "Epoch 15 - loss is 0.791384 : \n",
            "Roc 0.8347604553560654\n",
            "Saving\n",
            "Time mean 5.728047341108322\n",
            "Time std 0.05697837666228095\n",
            "Roc 0.8338525973416492\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.047849 : \n",
            "Roc 0.8008951476613698\n",
            "Epoch 1 - loss is 0.892839 : \n",
            "Roc 0.7630200164541008\n",
            "Epoch 2 - loss is 0.888783 : \n",
            "Roc 0.7531644761638439\n",
            "Epoch 3 - loss is 0.899818 : \n",
            "Roc 0.7479686683120768\n",
            "Epoch 4 - loss is 0.899181 : \n",
            "Roc 0.7419949381350128\n",
            "Epoch 5 - loss is 0.898917 : \n",
            "Roc 0.758117727875111\n",
            "Epoch 6 - loss is 0.906163 : \n",
            "Roc 0.7650462822243027\n",
            "Epoch 7 - loss is 0.907255 : \n",
            "Roc 0.7570248067858981\n",
            "Epoch 8 - loss is 0.895906 : \n",
            "Roc 0.7507325317025131\n",
            "Epoch 9 - loss is 0.857813 : \n",
            "Roc 0.8070708053188083\n",
            "Epoch 10 - loss is 0.819730 : \n",
            "Roc 0.828013613134101\n",
            "Epoch 11 - loss is 0.794303 : \n",
            "Roc 0.8292438205935635\n",
            "Epoch 12 - loss is 0.789308 : \n",
            "Roc 0.8379351116730902\n",
            "Epoch 13 - loss is 0.787365 : \n",
            "Roc 0.8344985957276074\n",
            "Epoch 14 - loss is 0.775035 : \n",
            "Roc 0.8452048049216404\n",
            "Epoch 15 - loss is 0.767118 : \n",
            "Roc 0.840650038298338\n",
            "Saving\n",
            "Time mean 5.754470348358154\n",
            "Time std 0.12805984623832123\n",
            "Roc 0.8406996842920076\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.029345 : \n",
            "Roc 0.733411719592928\n",
            "Epoch 1 - loss is 0.889288 : \n",
            "Roc 0.721251677392691\n",
            "Epoch 2 - loss is 0.901444 : \n",
            "Roc 0.7673806141877102\n",
            "Epoch 3 - loss is 0.896809 : \n",
            "Roc 0.7509575709430647\n",
            "Epoch 4 - loss is 0.893504 : \n",
            "Roc 0.7488343126546155\n",
            "Epoch 5 - loss is 0.894498 : \n",
            "Roc 0.7181790903949326\n",
            "Epoch 6 - loss is 0.895107 : \n",
            "Roc 0.755350289047288\n",
            "Epoch 7 - loss is 0.909237 : \n",
            "Roc 0.7563725222765991\n",
            "Epoch 8 - loss is 0.903385 : \n",
            "Roc 0.7679845540539426\n",
            "Epoch 9 - loss is 0.850148 : \n",
            "Roc 0.8077088827338154\n",
            "Epoch 10 - loss is 0.816400 : \n",
            "Roc 0.8229082486312986\n",
            "Epoch 11 - loss is 0.790011 : \n",
            "Roc 0.8333089664121124\n",
            "Epoch 12 - loss is 0.775257 : \n",
            "Roc 0.8289704115582807\n",
            "Epoch 13 - loss is 0.776797 : \n",
            "Roc 0.82714377556319\n",
            "Epoch 14 - loss is 0.769778 : \n",
            "Roc 0.8440981485195574\n",
            "Epoch 15 - loss is 0.767621 : \n",
            "Roc 0.847724731371865\n",
            "Saving\n",
            "Time mean 5.728200286626816\n",
            "Time std 0.09239063910970195\n",
            "Roc 0.8477364078696846\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.021463 : \n",
            "Roc 0.7346748954170829\n",
            "Epoch 1 - loss is 0.893396 : \n",
            "Roc 0.7708099517669831\n",
            "Epoch 2 - loss is 0.891486 : \n",
            "Roc 0.7896475594717782\n",
            "Epoch 3 - loss is 0.900851 : \n",
            "Roc 0.7643644441496004\n",
            "Epoch 4 - loss is 0.901006 : \n",
            "Roc 0.7492675293456543\n",
            "Epoch 5 - loss is 0.899045 : \n",
            "Roc 0.7698963146228771\n",
            "Epoch 6 - loss is 0.901251 : \n",
            "Roc 0.7696468570492009\n",
            "Epoch 7 - loss is 0.901964 : \n",
            "Roc 0.767297448645105\n",
            "Epoch 8 - loss is 0.899889 : \n",
            "Roc 0.5\n",
            "Epoch 9 - loss is 0.853155 : \n",
            "Roc 0.8103136121690809\n",
            "Epoch 10 - loss is 0.821675 : \n",
            "Roc 0.8309647383866136\n",
            "Epoch 11 - loss is 0.810616 : \n",
            "Roc 0.8376349236700799\n",
            "Epoch 12 - loss is 0.793075 : \n",
            "Roc 0.8304858227709789\n",
            "Epoch 13 - loss is 0.784908 : \n",
            "Roc 0.8313611193494005\n",
            "Epoch 14 - loss is 0.777807 : \n",
            "Roc 0.8433246441058941\n",
            "Epoch 15 - loss is 0.777945 : \n",
            "Roc 0.8472780149537963\n",
            "Saving\n",
            "Time mean 5.785571858286858\n",
            "Time std 0.13094687500218571\n",
            "Roc 0.8474358649163338\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.028173 : \n",
            "Roc 0.7633248268622946\n",
            "Epoch 1 - loss is 0.892629 : \n",
            "Roc 0.7381973234803167\n",
            "Epoch 2 - loss is 0.900808 : \n",
            "Roc 0.6716397045260838\n",
            "Epoch 3 - loss is 0.907394 : \n",
            "Roc 0.7459827430641865\n",
            "Epoch 4 - loss is 0.912687 : \n",
            "Roc 0.7055309965728458\n",
            "Epoch 5 - loss is 0.902465 : \n",
            "Roc 0.7592369949464983\n",
            "Epoch 6 - loss is 0.901102 : \n",
            "Roc 0.7647063477093942\n",
            "Epoch 7 - loss is 0.894926 : \n",
            "Roc 0.7445280449148333\n",
            "Epoch 8 - loss is 0.889530 : \n",
            "Roc 0.7603554730584697\n",
            "Epoch 9 - loss is 0.860846 : \n",
            "Roc 0.795687931178777\n",
            "Epoch 10 - loss is 0.827875 : \n",
            "Roc 0.8138574493660952\n",
            "Epoch 11 - loss is 0.812171 : \n",
            "Roc 0.8272300548652809\n",
            "Epoch 12 - loss is 0.798582 : \n",
            "Roc 0.8320775566983567\n",
            "Epoch 13 - loss is 0.791838 : \n",
            "Roc 0.8277580140075721\n",
            "Epoch 14 - loss is 0.788765 : \n",
            "Roc 0.8287345094164511\n",
            "Epoch 15 - loss is 0.804569 : \n",
            "Roc 0.8343385583800815\n",
            "Saving\n",
            "Time mean 5.816391542553902\n",
            "Time std 0.10703371496841374\n",
            "Roc 0.8343379338227365\n",
            "test\n",
            "Crossvalidation run 2\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.028618 : \n",
            "Roc 0.7860288216241006\n",
            "Epoch 1 - loss is 0.898779 : \n",
            "Roc 0.7576637309981751\n",
            "Epoch 2 - loss is 0.898296 : \n",
            "Roc 0.7345963993273522\n",
            "Epoch 3 - loss is 0.898993 : \n",
            "Roc 0.743807288133434\n",
            "Epoch 4 - loss is 0.908558 : \n",
            "Roc 0.7690015263565225\n",
            "Epoch 5 - loss is 0.902283 : \n",
            "Roc 0.7821120092489083\n",
            "Epoch 6 - loss is 0.899819 : \n",
            "Roc 0.7851519827506586\n",
            "Epoch 7 - loss is 0.903523 : \n",
            "Roc 0.7846920653347473\n",
            "Epoch 8 - loss is 0.906371 : \n",
            "Roc 0.7925185240920686\n",
            "Epoch 9 - loss is 0.859642 : \n",
            "Roc 0.8175318209903084\n",
            "Epoch 10 - loss is 0.828640 : \n",
            "Roc 0.8318288729007621\n",
            "Epoch 11 - loss is 0.814470 : \n",
            "Roc 0.8457201504540055\n",
            "Epoch 12 - loss is 0.799658 : \n",
            "Roc 0.8398955914810675\n",
            "Epoch 13 - loss is 0.796047 : \n",
            "Roc 0.8383109117372597\n",
            "Epoch 14 - loss is 0.797593 : \n",
            "Roc 0.8521582975829902\n",
            "Epoch 15 - loss is 0.788995 : \n",
            "Roc 0.8417710911100283\n",
            "Saving\n",
            "Time mean 5.825040400028229\n",
            "Time std 0.12957219403631048\n",
            "Roc 0.841551831627827\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.007933 : \n",
            "Roc 0.7619760845356987\n",
            "Epoch 1 - loss is 0.888245 : \n",
            "Roc 0.7769847251996098\n",
            "Epoch 2 - loss is 0.891879 : \n",
            "Roc 0.7758469776128982\n",
            "Epoch 3 - loss is 0.889178 : \n",
            "Roc 0.7435032556930503\n",
            "Epoch 4 - loss is 0.897460 : \n",
            "Roc 0.7739213811904702\n",
            "Epoch 5 - loss is 0.901538 : \n",
            "Roc 0.7530783869477035\n",
            "Epoch 6 - loss is 0.898273 : \n",
            "Roc 0.7588232851852327\n",
            "Epoch 7 - loss is 0.894631 : \n",
            "Roc 0.7581095478935072\n",
            "Epoch 8 - loss is 0.898979 : \n",
            "Roc 0.7477641178981284\n",
            "Epoch 9 - loss is 0.864332 : \n",
            "Roc 0.7861911038869348\n",
            "Epoch 10 - loss is 0.847494 : \n",
            "Roc 0.7952821533786039\n",
            "Epoch 11 - loss is 0.833292 : \n",
            "Roc 0.803536213768581\n",
            "Epoch 12 - loss is 0.825042 : \n",
            "Roc 0.8154059948717619\n",
            "Epoch 13 - loss is 0.818689 : \n",
            "Roc 0.8121885108854715\n",
            "Epoch 14 - loss is 0.807773 : \n",
            "Roc 0.8229376933519037\n",
            "Epoch 15 - loss is 0.802527 : \n",
            "Roc 0.8106881281931144\n",
            "Saving\n",
            "Time mean 5.806047573685646\n",
            "Time std 0.11362120630804597\n",
            "Roc 0.811955245051475\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.027116 : \n",
            "Roc 0.7561219551448506\n",
            "Epoch 1 - loss is 0.893344 : \n",
            "Roc 0.679931568636627\n",
            "Epoch 2 - loss is 0.912229 : \n",
            "Roc 0.758167457854369\n",
            "Epoch 3 - loss is 0.899558 : \n",
            "Roc 0.767521411909553\n",
            "Epoch 4 - loss is 0.897349 : \n",
            "Roc 0.7687492842405073\n",
            "Epoch 5 - loss is 0.899025 : \n",
            "Roc 0.7721679988829396\n",
            "Epoch 6 - loss is 0.911848 : \n",
            "Roc 0.7471823330963004\n",
            "Epoch 7 - loss is 0.899538 : \n",
            "Roc 0.7524189341737159\n",
            "Epoch 8 - loss is 0.906823 : \n",
            "Roc 0.7411417985661601\n",
            "Epoch 9 - loss is 0.865825 : \n",
            "Roc 0.7890061814987253\n",
            "Epoch 10 - loss is 0.832062 : \n",
            "Roc 0.8129467908673464\n",
            "Epoch 11 - loss is 0.813929 : \n",
            "Roc 0.8279262816232531\n",
            "Epoch 12 - loss is 0.783743 : \n",
            "Roc 0.8419292005903375\n",
            "Epoch 13 - loss is 0.778058 : \n",
            "Roc 0.8388484385694779\n",
            "Epoch 14 - loss is 0.771180 : \n",
            "Roc 0.8293344016935489\n",
            "Epoch 15 - loss is 0.768892 : \n",
            "Roc 0.8398515481521136\n",
            "Saving\n",
            "Time mean 5.798399329185486\n",
            "Time std 0.12202501382186491\n",
            "Roc 0.8411855716583211\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.019792 : \n",
            "Roc 0.7388014070987022\n",
            "Epoch 1 - loss is 0.892828 : \n",
            "Roc 0.7545549101046479\n",
            "Epoch 2 - loss is 0.890671 : \n",
            "Roc 0.7557767266597956\n",
            "Epoch 3 - loss is 0.901621 : \n",
            "Roc 0.748018540400431\n",
            "Epoch 4 - loss is 0.913197 : \n",
            "Roc 0.7709903229534449\n",
            "Epoch 5 - loss is 0.917868 : \n",
            "Roc 0.7459920721961606\n",
            "Epoch 6 - loss is 0.902290 : \n",
            "Roc 0.7464170005951356\n",
            "Epoch 7 - loss is 0.898627 : \n",
            "Roc 0.7653441015928721\n",
            "Epoch 8 - loss is 0.897730 : \n",
            "Roc 0.7304405182789967\n",
            "Epoch 9 - loss is 0.851525 : \n",
            "Roc 0.8023645634086726\n",
            "Epoch 10 - loss is 0.823040 : \n",
            "Roc 0.8108398409093232\n",
            "Epoch 11 - loss is 0.810652 : \n",
            "Roc 0.8264938925878493\n",
            "Epoch 12 - loss is 0.792782 : \n",
            "Roc 0.8284625768192714\n",
            "Epoch 13 - loss is 0.788277 : \n",
            "Roc 0.828010525331\n",
            "Epoch 14 - loss is 0.777298 : \n",
            "Roc 0.8338562389787882\n",
            "Epoch 15 - loss is 0.778252 : \n",
            "Roc 0.8399851727111847\n",
            "Saving\n",
            "Time mean 5.814117476344109\n",
            "Time std 0.14224573564554138\n",
            "Roc 0.8397710378866318\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.043141 : \n",
            "Roc 0.7471686515949273\n",
            "Epoch 1 - loss is 0.892778 : \n",
            "Roc 0.7682279012981016\n",
            "Epoch 2 - loss is 0.895546 : \n",
            "Roc 0.6977781385731767\n",
            "Epoch 3 - loss is 0.901920 : \n",
            "Roc 0.7352989388512366\n",
            "Epoch 4 - loss is 0.900975 : \n",
            "Roc 0.7542535921447677\n",
            "Epoch 5 - loss is 0.895735 : \n",
            "Roc 0.7512031777608321\n",
            "Epoch 6 - loss is 0.911177 : \n",
            "Roc 0.6773684182762277\n",
            "Epoch 7 - loss is 0.910099 : \n",
            "Roc 0.7377422205445889\n",
            "Epoch 8 - loss is 0.899888 : \n",
            "Roc 0.750257750539679\n",
            "Epoch 9 - loss is 0.854240 : \n",
            "Roc 0.7911485035815455\n",
            "Epoch 10 - loss is 0.840644 : \n",
            "Roc 0.8078568147640603\n",
            "Epoch 11 - loss is 0.826694 : \n",
            "Roc 0.8171372018284903\n",
            "Epoch 12 - loss is 0.822452 : \n",
            "Roc 0.8241150760180926\n",
            "Epoch 13 - loss is 0.817135 : \n",
            "Roc 0.8317514669361301\n",
            "Epoch 14 - loss is 0.812858 : \n",
            "Roc 0.8246377083422474\n",
            "Epoch 15 - loss is 0.805560 : \n",
            "Roc 0.8281520443911197\n",
            "Saving\n",
            "Time mean 5.810201525688171\n",
            "Time std 0.12078025006816931\n",
            "Roc 0.8286619772382389\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.032269 : \n",
            "Roc 0.7663783179102505\n",
            "Epoch 1 - loss is 0.897545 : \n",
            "Roc 0.7493339756691371\n",
            "Epoch 2 - loss is 0.895800 : \n",
            "Roc 0.6173462272987609\n",
            "Epoch 3 - loss is 0.895703 : \n",
            "Roc 0.7570296502462394\n",
            "Epoch 4 - loss is 0.907958 : \n",
            "Roc 0.7371180703874894\n",
            "Epoch 5 - loss is 0.908387 : \n",
            "Roc 0.7735291244375571\n",
            "Epoch 6 - loss is 0.904730 : \n",
            "Roc 0.7631101631443551\n",
            "Epoch 7 - loss is 0.903749 : \n",
            "Roc 0.7447950686999213\n",
            "Epoch 8 - loss is 0.897013 : \n",
            "Roc 0.7256597510214481\n",
            "Epoch 9 - loss is 0.865954 : \n",
            "Roc 0.8004151877299814\n",
            "Epoch 10 - loss is 0.854733 : \n",
            "Roc 0.8058767021739172\n",
            "Epoch 11 - loss is 0.840592 : \n",
            "Roc 0.8119106680739312\n",
            "Epoch 12 - loss is 0.835619 : \n",
            "Roc 0.8148948478411195\n",
            "Epoch 13 - loss is 0.828785 : \n",
            "Roc 0.8249387514294525\n",
            "Epoch 14 - loss is 0.821095 : \n",
            "Roc 0.8250295468084657\n",
            "Epoch 15 - loss is 0.819325 : \n",
            "Roc 0.8237948062744586\n",
            "Saving\n",
            "Time mean 5.846902817487717\n",
            "Time std 0.10251826114070006\n",
            "Roc 0.8237628171852284\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.033280 : \n",
            "Roc 0.7243168103360933\n",
            "Epoch 1 - loss is 0.897197 : \n",
            "Roc 0.7482335144905511\n",
            "Epoch 2 - loss is 0.902181 : \n",
            "Roc 0.7546369358087434\n",
            "Epoch 3 - loss is 0.903880 : \n",
            "Roc 0.7377181890762602\n",
            "Epoch 4 - loss is 0.907404 : \n",
            "Roc 0.7655826615926435\n",
            "Epoch 5 - loss is 0.906883 : \n",
            "Roc 0.7292040024802733\n",
            "Epoch 6 - loss is 0.909937 : \n",
            "Roc 0.7632539605344745\n",
            "Epoch 7 - loss is 0.905339 : \n",
            "Roc 0.7546114035834276\n",
            "Epoch 8 - loss is 0.902151 : \n",
            "Roc 0.7441317422298954\n",
            "Epoch 9 - loss is 0.873640 : \n",
            "Roc 0.7884691607192793\n",
            "Epoch 10 - loss is 0.838137 : \n",
            "Roc 0.8141435356863509\n",
            "Epoch 11 - loss is 0.815416 : \n",
            "Roc 0.8235413196513027\n",
            "Epoch 12 - loss is 0.809162 : \n",
            "Roc 0.8294770554454562\n",
            "Epoch 13 - loss is 0.799337 : \n",
            "Roc 0.8340678914029349\n",
            "Epoch 14 - loss is 0.795530 : \n",
            "Roc 0.8361307330990853\n",
            "Epoch 15 - loss is 0.795260 : \n",
            "Roc 0.8325832735555043\n",
            "Saving\n",
            "Time mean 5.805086404085159\n",
            "Time std 0.08760172462638052\n",
            "Roc 0.8326564254074011\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.033860 : \n",
            "Roc 0.7574927792144994\n",
            "Epoch 1 - loss is 0.893809 : \n",
            "Roc 0.7224631876328815\n",
            "Epoch 2 - loss is 0.904940 : \n",
            "Roc 0.7533818670072411\n",
            "Epoch 3 - loss is 0.904141 : \n",
            "Roc 0.7583732263988542\n",
            "Epoch 4 - loss is 0.902305 : \n",
            "Roc 0.7815743294452231\n",
            "Epoch 5 - loss is 0.895320 : \n",
            "Roc 0.7418090839620654\n",
            "Epoch 6 - loss is 0.903591 : \n",
            "Roc 0.7680681538534602\n",
            "Epoch 7 - loss is 0.905045 : \n",
            "Roc 0.754002291340984\n",
            "Epoch 8 - loss is 0.899290 : \n",
            "Roc 0.7643115595758467\n",
            "Epoch 9 - loss is 0.853787 : \n",
            "Roc 0.8095657402526755\n",
            "Epoch 10 - loss is 0.821237 : \n",
            "Roc 0.8245005696953474\n",
            "Epoch 11 - loss is 0.811136 : \n",
            "Roc 0.8277610599983594\n",
            "Epoch 12 - loss is 0.799183 : \n",
            "Roc 0.8358601340030213\n",
            "Epoch 13 - loss is 0.795106 : \n",
            "Roc 0.8367546126091262\n",
            "Epoch 14 - loss is 0.794264 : \n",
            "Roc 0.8256250458719556\n",
            "Epoch 15 - loss is 0.791442 : \n",
            "Roc 0.8378729071104181\n",
            "Saving\n",
            "Time mean 5.841137245297432\n",
            "Time std 0.13207560524840642\n",
            "Roc 0.8370739010453114\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.031620 : \n",
            "Roc 0.763694020822927\n",
            "Epoch 1 - loss is 0.895558 : \n",
            "Roc 0.7791406835352148\n",
            "Epoch 2 - loss is 0.899765 : \n",
            "Roc 0.7763024085289711\n",
            "Epoch 3 - loss is 0.900499 : \n",
            "Roc 0.7659947474400599\n",
            "Epoch 4 - loss is 0.902968 : \n",
            "Roc 0.7683233758429071\n",
            "Epoch 5 - loss is 0.902589 : \n",
            "Roc 0.7532716307130369\n",
            "Epoch 6 - loss is 0.898640 : \n",
            "Roc 0.7648878465284716\n",
            "Epoch 7 - loss is 0.910051 : \n",
            "Roc 0.7665831239073426\n",
            "Epoch 8 - loss is 0.900939 : \n",
            "Roc 0.7870241087037961\n",
            "Epoch 9 - loss is 0.851665 : \n",
            "Roc 0.8121583494630371\n",
            "Epoch 10 - loss is 0.811521 : \n",
            "Roc 0.8242641147914587\n",
            "Epoch 11 - loss is 0.802088 : \n",
            "Roc 0.8363924161775724\n",
            "Epoch 12 - loss is 0.791818 : \n",
            "Roc 0.8435051276848152\n",
            "Epoch 13 - loss is 0.792425 : \n",
            "Roc 0.8301359773039461\n",
            "Epoch 14 - loss is 0.783576 : \n",
            "Roc 0.8301886589972527\n",
            "Epoch 15 - loss is 0.782326 : \n",
            "Roc 0.8391305959665334\n",
            "Saving\n",
            "Time mean 5.8258844912052155\n",
            "Time std 0.08601240825682491\n",
            "Roc 0.8394625881930569\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.039910 : \n",
            "Roc 0.7284582885213439\n",
            "Epoch 1 - loss is 0.899601 : \n",
            "Roc 0.6806511301781529\n",
            "Epoch 2 - loss is 0.896607 : \n",
            "Roc 0.7225572625401746\n",
            "Epoch 3 - loss is 0.899730 : \n",
            "Roc 0.7035139886272271\n",
            "Epoch 4 - loss is 0.902183 : \n",
            "Roc 0.7432682086571142\n",
            "Epoch 5 - loss is 0.913116 : \n",
            "Roc 0.732836331416775\n",
            "Epoch 6 - loss is 0.904359 : \n",
            "Roc 0.7404239745080674\n",
            "Epoch 7 - loss is 0.903916 : \n",
            "Roc 0.7603323644367055\n",
            "Epoch 8 - loss is 0.902695 : \n",
            "Roc 0.7374094443896221\n",
            "Epoch 9 - loss is 0.871525 : \n",
            "Roc 0.7762986524967095\n",
            "Epoch 10 - loss is 0.838686 : \n",
            "Roc 0.7962130798130159\n",
            "Epoch 11 - loss is 0.822773 : \n",
            "Roc 0.8036701904192071\n",
            "Epoch 12 - loss is 0.822885 : \n",
            "Roc 0.8207320561552\n",
            "Epoch 13 - loss is 0.807990 : \n",
            "Roc 0.8227265800363992\n",
            "Epoch 14 - loss is 0.800678 : \n",
            "Roc 0.8323565256457818\n",
            "Epoch 15 - loss is 0.788090 : \n",
            "Roc 0.8329247687368244\n",
            "Saving\n",
            "Time mean 5.869618371129036\n",
            "Time std 0.1461802650886849\n",
            "Roc 0.8332360064804071\n",
            "test\n",
            "Crossvalidation run 3\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.019608 : \n",
            "Roc 0.7668595911243601\n",
            "Epoch 1 - loss is 0.911181 : \n",
            "Roc 0.7522460411869433\n",
            "Epoch 2 - loss is 0.911815 : \n",
            "Roc 0.7509335696518601\n",
            "Epoch 3 - loss is 0.912477 : \n",
            "Roc 0.7775352288181208\n",
            "Epoch 4 - loss is 0.910215 : \n",
            "Roc 0.7682208312233465\n",
            "Epoch 5 - loss is 0.902352 : \n",
            "Roc 0.7747177992120593\n",
            "Epoch 6 - loss is 0.909286 : \n",
            "Roc 0.767104130637646\n",
            "Epoch 7 - loss is 0.907060 : \n",
            "Roc 0.7766559017526427\n",
            "Epoch 8 - loss is 0.897968 : \n",
            "Roc 0.7712000928394214\n",
            "Epoch 9 - loss is 0.868480 : \n",
            "Roc 0.8083652220024651\n",
            "Epoch 10 - loss is 0.848661 : \n",
            "Roc 0.8215669717787268\n",
            "Epoch 11 - loss is 0.841204 : \n",
            "Roc 0.8288000464993328\n",
            "Epoch 12 - loss is 0.826853 : \n",
            "Roc 0.8346577481901887\n",
            "Epoch 13 - loss is 0.815212 : \n",
            "Roc 0.845713880210075\n",
            "Epoch 14 - loss is 0.802469 : \n",
            "Roc 0.8443144214814179\n",
            "Epoch 15 - loss is 0.796803 : \n",
            "Roc 0.8517449591219908\n",
            "Saving\n",
            "Time mean 5.85205614566803\n",
            "Time std 0.09278272610855515\n",
            "Roc 0.8517423714022734\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.012200 : \n",
            "Roc 0.7583484297579008\n",
            "Epoch 1 - loss is 0.892150 : \n",
            "Roc 0.745743750962748\n",
            "Epoch 2 - loss is 0.897295 : \n",
            "Roc 0.7512282057906088\n",
            "Epoch 3 - loss is 0.902797 : \n",
            "Roc 0.7521683894508485\n",
            "Epoch 4 - loss is 0.910408 : \n",
            "Roc 0.7167993320132988\n",
            "Epoch 5 - loss is 0.900339 : \n",
            "Roc 0.7616932772906986\n",
            "Epoch 6 - loss is 0.895069 : \n",
            "Roc 0.7793418533863059\n",
            "Epoch 7 - loss is 0.905135 : \n",
            "Roc 0.7562389083399657\n",
            "Epoch 8 - loss is 0.896835 : \n",
            "Roc 0.7356103261148622\n",
            "Epoch 9 - loss is 0.860169 : \n",
            "Roc 0.8051807078123797\n",
            "Epoch 10 - loss is 0.828619 : \n",
            "Roc 0.8118905604156504\n",
            "Epoch 11 - loss is 0.810290 : \n",
            "Roc 0.8189607415406537\n",
            "Epoch 12 - loss is 0.807839 : \n",
            "Roc 0.8233657150971734\n",
            "Epoch 13 - loss is 0.796570 : \n",
            "Roc 0.8266012506097404\n",
            "Epoch 14 - loss is 0.788646 : \n",
            "Roc 0.8269654902954994\n",
            "Epoch 15 - loss is 0.782465 : \n",
            "Roc 0.8247417428307361\n",
            "Saving\n",
            "Time mean 5.849505245685577\n",
            "Time std 0.13952252067278859\n",
            "Roc 0.8249009974069985\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.013559 : \n",
            "Roc 0.7835354961540174\n",
            "Epoch 1 - loss is 0.899859 : \n",
            "Roc 0.7191071544750711\n",
            "Epoch 2 - loss is 0.904828 : \n",
            "Roc 0.7595276386842276\n",
            "Epoch 3 - loss is 0.896936 : \n",
            "Roc 0.7455495453952267\n",
            "Epoch 4 - loss is 0.902041 : \n",
            "Roc 0.7486543721538454\n",
            "Epoch 5 - loss is 0.899169 : \n",
            "Roc 0.7685765507868693\n",
            "Epoch 6 - loss is 0.893013 : \n",
            "Roc 0.7553427522526688\n",
            "Epoch 7 - loss is 0.891150 : \n",
            "Roc 0.7608127717745927\n",
            "Epoch 8 - loss is 0.898933 : \n",
            "Roc 0.745316602538421\n",
            "Epoch 9 - loss is 0.854804 : \n",
            "Roc 0.7818053080914209\n",
            "Epoch 10 - loss is 0.829291 : \n",
            "Roc 0.8151831944807476\n",
            "Epoch 11 - loss is 0.810728 : \n",
            "Roc 0.82339450152131\n",
            "Epoch 12 - loss is 0.801661 : \n",
            "Roc 0.8260621680635629\n",
            "Epoch 13 - loss is 0.794793 : \n",
            "Roc 0.8254947919341848\n",
            "Epoch 14 - loss is 0.792131 : \n",
            "Roc 0.8259310960926856\n",
            "Epoch 15 - loss is 0.785599 : \n",
            "Roc 0.8282594783677981\n",
            "Saving\n",
            "Time mean 5.838159590959549\n",
            "Time std 0.10286426453394547\n",
            "Roc 0.8286124595607777\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.039882 : \n",
            "Roc 0.7004219409282701\n",
            "Epoch 1 - loss is 0.898103 : \n",
            "Roc 0.7715221713566027\n",
            "Epoch 2 - loss is 0.894230 : \n",
            "Roc 0.7415604935301605\n",
            "Epoch 3 - loss is 0.900116 : \n",
            "Roc 0.7468967648050793\n",
            "Epoch 4 - loss is 0.901785 : \n",
            "Roc 0.7660600135694065\n",
            "Epoch 5 - loss is 0.895484 : \n",
            "Roc 0.7442448931939913\n",
            "Epoch 6 - loss is 0.910241 : \n",
            "Roc 0.7145673199007215\n",
            "Epoch 7 - loss is 0.907022 : \n",
            "Roc 0.6847061743943886\n",
            "Epoch 8 - loss is 0.895739 : \n",
            "Roc 0.7247899729476666\n",
            "Epoch 9 - loss is 0.850735 : \n",
            "Roc 0.7981621551928137\n",
            "Epoch 10 - loss is 0.820016 : \n",
            "Roc 0.8170424786887205\n",
            "Epoch 11 - loss is 0.797822 : \n",
            "Roc 0.8281922303748724\n",
            "Epoch 12 - loss is 0.797169 : \n",
            "Roc 0.8271173305534132\n",
            "Epoch 13 - loss is 0.790096 : \n",
            "Roc 0.8224965781667779\n",
            "Epoch 14 - loss is 0.781254 : \n",
            "Roc 0.8311289926956341\n",
            "Epoch 15 - loss is 0.786851 : \n",
            "Roc 0.834218273257656\n",
            "Saving\n",
            "Time mean 5.862050339579582\n",
            "Time std 0.13513303580882816\n",
            "Roc 0.8342084460513892\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.036977 : \n",
            "Roc 0.7666964420559008\n",
            "Epoch 1 - loss is 0.893885 : \n",
            "Roc 0.7121026602278364\n",
            "Epoch 2 - loss is 0.901601 : \n",
            "Roc 0.7132036071985326\n",
            "Epoch 3 - loss is 0.906195 : \n",
            "Roc 0.758192188414912\n",
            "Epoch 4 - loss is 0.903078 : \n",
            "Roc 0.755126143905394\n",
            "Epoch 5 - loss is 0.910726 : \n",
            "Roc 0.7342277868074315\n",
            "Epoch 6 - loss is 0.910900 : \n",
            "Roc 0.7080357015513485\n",
            "Epoch 7 - loss is 0.904333 : \n",
            "Roc 0.752969675016475\n",
            "Epoch 8 - loss is 0.899605 : \n",
            "Roc 0.7395328468065994\n",
            "Epoch 9 - loss is 0.869043 : \n",
            "Roc 0.7865572518800599\n",
            "Epoch 10 - loss is 0.850500 : \n",
            "Roc 0.7956048477615999\n",
            "Epoch 11 - loss is 0.835453 : \n",
            "Roc 0.8112912417958936\n",
            "Epoch 12 - loss is 0.822105 : \n",
            "Roc 0.8222662275382885\n",
            "Epoch 13 - loss is 0.807768 : \n",
            "Roc 0.8154931079938128\n",
            "Epoch 14 - loss is 0.808595 : \n",
            "Roc 0.8290836975641034\n",
            "Epoch 15 - loss is 0.806320 : \n",
            "Roc 0.8275890668053059\n",
            "Saving\n",
            "Time mean 5.877741381525993\n",
            "Time std 0.1621633972771816\n",
            "Roc 0.827249990768457\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.048669 : \n",
            "Roc 0.7348485690150234\n",
            "Epoch 1 - loss is 0.897379 : \n",
            "Roc 0.766904222368227\n",
            "Epoch 2 - loss is 0.900939 : \n",
            "Roc 0.7637972581327949\n",
            "Epoch 3 - loss is 0.915518 : \n",
            "Roc 0.7645563764598613\n",
            "Epoch 4 - loss is 0.903365 : \n",
            "Roc 0.7622121700287136\n",
            "Epoch 5 - loss is 0.898857 : \n",
            "Roc 0.765797246639709\n",
            "Epoch 6 - loss is 0.903323 : \n",
            "Roc 0.7435916468251308\n",
            "Epoch 7 - loss is 0.897842 : \n",
            "Roc 0.7526234884197581\n",
            "Epoch 8 - loss is 0.896827 : \n",
            "Roc 0.7709551520631047\n",
            "Epoch 9 - loss is 0.860929 : \n",
            "Roc 0.7928480441640983\n",
            "Epoch 10 - loss is 0.837443 : \n",
            "Roc 0.7993145331987364\n",
            "Epoch 11 - loss is 0.829695 : \n",
            "Roc 0.8110413118974511\n",
            "Epoch 12 - loss is 0.825201 : \n",
            "Roc 0.8261190913566248\n",
            "Epoch 13 - loss is 0.812410 : \n",
            "Roc 0.825641553635359\n",
            "Epoch 14 - loss is 0.811001 : \n",
            "Roc 0.8358418589683423\n",
            "Epoch 15 - loss is 0.803155 : \n",
            "Roc 0.8360401147009978\n",
            "Saving\n",
            "Time mean 5.856456413865089\n",
            "Time std 0.09228050527816942\n",
            "Roc 0.8366082562498444\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.020550 : \n",
            "Roc 0.7673801505185474\n",
            "Epoch 1 - loss is 0.899202 : \n",
            "Roc 0.7691628469647047\n",
            "Epoch 2 - loss is 0.892313 : \n",
            "Roc 0.7466825331209701\n",
            "Epoch 3 - loss is 0.900050 : \n",
            "Roc 0.7471363380304522\n",
            "Epoch 4 - loss is 0.898863 : \n",
            "Roc 0.7699049025925342\n",
            "Epoch 5 - loss is 0.904927 : \n",
            "Roc 0.7460547645969353\n",
            "Epoch 6 - loss is 0.896690 : \n",
            "Roc 0.77178344620198\n",
            "Epoch 7 - loss is 0.896995 : \n",
            "Roc 0.7441987136621722\n",
            "Epoch 8 - loss is 0.893392 : \n",
            "Roc 0.7268648656315984\n",
            "Epoch 9 - loss is 0.873638 : \n",
            "Roc 0.7935800637900359\n",
            "Epoch 10 - loss is 0.825035 : \n",
            "Roc 0.8230232789051134\n",
            "Epoch 11 - loss is 0.816505 : \n",
            "Roc 0.8298119126068402\n",
            "Epoch 12 - loss is 0.804292 : \n",
            "Roc 0.8252734582386008\n",
            "Epoch 13 - loss is 0.790861 : \n",
            "Roc 0.835686350796565\n",
            "Epoch 14 - loss is 0.785872 : \n",
            "Roc 0.8355981025098583\n",
            "Epoch 15 - loss is 0.783854 : \n",
            "Roc 0.8364730878186969\n",
            "Saving\n",
            "Time mean 5.882857441902161\n",
            "Time std 0.13276032395249773\n",
            "Roc 0.8366705572914767\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.028561 : \n",
            "Roc 0.7802991381370903\n",
            "Epoch 1 - loss is 0.890627 : \n",
            "Roc 0.6983839923260878\n",
            "Epoch 2 - loss is 0.895737 : \n",
            "Roc 0.7783422944730328\n",
            "Epoch 3 - loss is 0.899763 : \n",
            "Roc 0.7736407869684789\n",
            "Epoch 4 - loss is 0.896162 : \n",
            "Roc 0.7753295617918303\n",
            "Epoch 5 - loss is 0.907106 : \n",
            "Roc 0.7600486566532685\n",
            "Epoch 6 - loss is 0.898895 : \n",
            "Roc 0.7681332086270283\n",
            "Epoch 7 - loss is 0.894330 : \n",
            "Roc 0.7523209737767446\n",
            "Epoch 8 - loss is 0.898481 : \n",
            "Roc 0.7691079508904163\n",
            "Epoch 9 - loss is 0.857329 : \n",
            "Roc 0.793421461128252\n",
            "Epoch 10 - loss is 0.827468 : \n",
            "Roc 0.8118363775304149\n",
            "Epoch 11 - loss is 0.809873 : \n",
            "Roc 0.808458042909069\n",
            "Epoch 12 - loss is 0.796539 : \n",
            "Roc 0.8315195185907508\n",
            "Epoch 13 - loss is 0.785893 : \n",
            "Roc 0.8388793427250317\n",
            "Epoch 14 - loss is 0.782258 : \n",
            "Roc 0.8352703236803694\n",
            "Epoch 15 - loss is 0.779582 : \n",
            "Roc 0.8394012527410334\n",
            "Saving\n",
            "Time mean 5.887590453028679\n",
            "Time std 0.09963101474727007\n",
            "Roc 0.8393434589829161\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.033612 : \n",
            "Roc 0.7786464512050448\n",
            "Epoch 1 - loss is 0.898858 : \n",
            "Roc 0.759674310064935\n",
            "Epoch 2 - loss is 0.898691 : \n",
            "Roc 0.752871835196054\n",
            "Epoch 3 - loss is 0.899662 : \n",
            "Roc 0.7850393746878122\n",
            "Epoch 4 - loss is 0.893390 : \n",
            "Roc 0.7791991211913086\n",
            "Epoch 5 - loss is 0.904506 : \n",
            "Roc 0.7773451158216783\n",
            "Epoch 6 - loss is 0.904176 : \n",
            "Roc 0.750554035807942\n",
            "Epoch 7 - loss is 0.896285 : \n",
            "Roc 0.790734948645105\n",
            "Epoch 8 - loss is 0.897335 : \n",
            "Roc 0.7772051190996504\n",
            "Epoch 9 - loss is 0.857637 : \n",
            "Roc 0.8019475641545954\n",
            "Epoch 10 - loss is 0.837356 : \n",
            "Roc 0.8122584446803195\n",
            "Epoch 11 - loss is 0.820152 : \n",
            "Roc 0.8176994880119881\n",
            "Epoch 12 - loss is 0.815163 : \n",
            "Roc 0.8286605972152847\n",
            "Epoch 13 - loss is 0.801181 : \n",
            "Roc 0.8342542223401598\n",
            "Epoch 14 - loss is 0.785541 : \n",
            "Roc 0.8347813319493007\n",
            "Epoch 15 - loss is 0.782118 : \n",
            "Roc 0.8347609421828172\n",
            "Saving\n",
            "Time mean 5.901743873953819\n",
            "Time std 0.12811615332371046\n",
            "Roc 0.8357526262799699\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.008503 : \n",
            "Roc 0.7283206777196662\n",
            "Epoch 1 - loss is 0.891401 : \n",
            "Roc 0.7154845669798197\n",
            "Epoch 2 - loss is 0.902150 : \n",
            "Roc 0.7500801515259393\n",
            "Epoch 3 - loss is 0.903719 : \n",
            "Roc 0.7595278304834783\n",
            "Epoch 4 - loss is 0.900326 : \n",
            "Roc 0.7476903869382575\n",
            "Epoch 5 - loss is 0.902451 : \n",
            "Roc 0.7179614573171265\n",
            "Epoch 6 - loss is 0.903261 : \n",
            "Roc 0.7172349930320219\n",
            "Epoch 7 - loss is 0.900696 : \n",
            "Roc 0.7231334166909205\n",
            "Epoch 8 - loss is 0.894844 : \n",
            "Roc 0.7418180905949575\n",
            "Epoch 9 - loss is 0.859383 : \n",
            "Roc 0.7875142451121102\n",
            "Epoch 10 - loss is 0.838867 : \n",
            "Roc 0.8036894476040107\n",
            "Epoch 11 - loss is 0.811970 : \n",
            "Roc 0.8118038007229044\n",
            "Epoch 12 - loss is 0.805380 : \n",
            "Roc 0.8287432532192809\n",
            "Epoch 13 - loss is 0.804564 : \n",
            "Roc 0.8239488387605285\n",
            "Epoch 14 - loss is 0.795636 : \n",
            "Roc 0.8298858808819248\n",
            "Epoch 15 - loss is 0.789828 : \n",
            "Roc 0.8317360279235426\n",
            "Saving\n",
            "Time mean 5.8788586258888245\n",
            "Time std 0.09047337487908812\n",
            "Roc 0.8317978591006957\n",
            "test\n",
            "Crossvalidation run 4\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.026754 : \n",
            "Roc 0.7211951960774947\n",
            "Epoch 1 - loss is 0.896786 : \n",
            "Roc 0.7868057347053821\n",
            "Epoch 2 - loss is 0.899546 : \n",
            "Roc 0.7531032731071428\n",
            "Epoch 3 - loss is 0.904027 : \n",
            "Roc 0.7606946474809146\n",
            "Epoch 4 - loss is 0.904194 : \n",
            "Roc 0.7765886210399926\n",
            "Epoch 5 - loss is 0.904138 : \n",
            "Roc 0.7715932271810894\n",
            "Epoch 6 - loss is 0.900028 : \n",
            "Roc 0.7605811859240788\n",
            "Epoch 7 - loss is 0.900621 : \n",
            "Roc 0.777848541959278\n",
            "Epoch 8 - loss is 0.905019 : \n",
            "Roc 0.7456007769528924\n",
            "Epoch 9 - loss is 0.861624 : \n",
            "Roc 0.801708014446642\n",
            "Epoch 10 - loss is 0.840857 : \n",
            "Roc 0.8196720642518862\n",
            "Epoch 11 - loss is 0.828243 : \n",
            "Roc 0.8313540263326358\n",
            "Epoch 12 - loss is 0.815814 : \n",
            "Roc 0.8203869715876337\n",
            "Epoch 13 - loss is 0.807500 : \n",
            "Roc 0.8486607953296834\n",
            "Epoch 14 - loss is 0.799599 : \n",
            "Roc 0.8425393452830249\n",
            "Epoch 15 - loss is 0.793523 : \n",
            "Roc 0.8495888909590646\n",
            "Saving\n",
            "Time mean 5.902409419417381\n",
            "Time std 0.12532628534709753\n",
            "Roc 0.8505512241108595\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.039949 : \n",
            "Roc 0.7698853607737928\n",
            "Epoch 1 - loss is 0.892525 : \n",
            "Roc 0.7309759978883726\n",
            "Epoch 2 - loss is 0.898792 : \n",
            "Roc 0.7673724880298323\n",
            "Epoch 3 - loss is 0.912200 : \n",
            "Roc 0.7368529731264921\n",
            "Epoch 4 - loss is 0.907494 : \n",
            "Roc 0.75588489785243\n",
            "Epoch 5 - loss is 0.907500 : \n",
            "Roc 0.7495850154681523\n",
            "Epoch 6 - loss is 0.913757 : \n",
            "Roc 0.7730425727195708\n",
            "Epoch 7 - loss is 0.898592 : \n",
            "Roc 0.7489748738800031\n",
            "Epoch 8 - loss is 0.905119 : \n",
            "Roc 0.7716859002336268\n",
            "Epoch 9 - loss is 0.859561 : \n",
            "Roc 0.795269116165182\n",
            "Epoch 10 - loss is 0.842166 : \n",
            "Roc 0.8030083069112475\n",
            "Epoch 11 - loss is 0.833629 : \n",
            "Roc 0.8074167904867654\n",
            "Epoch 12 - loss is 0.822507 : \n",
            "Roc 0.8151943908692972\n",
            "Epoch 13 - loss is 0.825102 : \n",
            "Roc 0.8125146417935354\n",
            "Epoch 14 - loss is 0.813399 : \n",
            "Roc 0.8153002931567868\n",
            "Epoch 15 - loss is 0.821447 : \n",
            "Roc 0.814610223421735\n",
            "Saving\n",
            "Time mean 5.874905496835709\n",
            "Time std 0.08355086897532417\n",
            "Roc 0.814596083059485\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.006932 : \n",
            "Roc 0.7490790719723992\n",
            "Epoch 1 - loss is 0.899837 : \n",
            "Roc 0.6960136619225842\n",
            "Epoch 2 - loss is 0.902918 : \n",
            "Roc 0.724213349404393\n",
            "Epoch 3 - loss is 0.899725 : \n",
            "Roc 0.7543923377874975\n",
            "Epoch 4 - loss is 0.895810 : \n",
            "Roc 0.7578466263900951\n",
            "Epoch 5 - loss is 0.897280 : \n",
            "Roc 0.7111793928228869\n",
            "Epoch 6 - loss is 0.907150 : \n",
            "Roc 0.7341646416142282\n",
            "Epoch 7 - loss is 0.896087 : \n",
            "Roc 0.7678826683285602\n",
            "Epoch 8 - loss is 0.904079 : \n",
            "Roc 0.7570528705142092\n",
            "Epoch 9 - loss is 0.851568 : \n",
            "Roc 0.8035594505554293\n",
            "Epoch 10 - loss is 0.831042 : \n",
            "Roc 0.8233336262953582\n",
            "Epoch 11 - loss is 0.825029 : \n",
            "Roc 0.8124720592224635\n",
            "Epoch 12 - loss is 0.814851 : \n",
            "Roc 0.8253362309940887\n",
            "Epoch 13 - loss is 0.814964 : \n",
            "Roc 0.816820452706227\n",
            "Epoch 14 - loss is 0.801485 : \n",
            "Roc 0.8358061991525408\n",
            "Epoch 15 - loss is 0.800008 : \n",
            "Roc 0.8235027452824554\n",
            "Saving\n",
            "Time mean 5.891459554433823\n",
            "Time std 0.1447436605856575\n",
            "Roc 0.8235200566748353\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.002814 : \n",
            "Roc 0.745381802686994\n",
            "Epoch 1 - loss is 0.892797 : \n",
            "Roc 0.7216681643329017\n",
            "Epoch 2 - loss is 0.903168 : \n",
            "Roc 0.7432558831553038\n",
            "Epoch 3 - loss is 0.896757 : \n",
            "Roc 0.7490345752563523\n",
            "Epoch 4 - loss is 0.901044 : \n",
            "Roc 0.7375088346584338\n",
            "Epoch 5 - loss is 0.905648 : \n",
            "Roc 0.7450013915324074\n",
            "Epoch 6 - loss is 0.899328 : \n",
            "Roc 0.7504952911958451\n",
            "Epoch 7 - loss is 0.903171 : \n",
            "Roc 0.765300960157361\n",
            "Epoch 8 - loss is 0.905943 : \n",
            "Roc 0.7680067831308535\n",
            "Epoch 9 - loss is 0.870971 : \n",
            "Roc 0.7763823144878965\n",
            "Epoch 10 - loss is 0.857935 : \n",
            "Roc 0.7890462421156323\n",
            "Epoch 11 - loss is 0.846451 : \n",
            "Roc 0.8130502744149078\n",
            "Epoch 12 - loss is 0.819112 : \n",
            "Roc 0.8259081910943497\n",
            "Epoch 13 - loss is 0.802626 : \n",
            "Roc 0.8255093047919817\n",
            "Epoch 14 - loss is 0.796574 : \n",
            "Roc 0.8323288945808067\n",
            "Epoch 15 - loss is 0.787561 : \n",
            "Roc 0.8343319740341626\n",
            "Saving\n",
            "Time mean 5.86809304356575\n",
            "Time std 0.057969498471341134\n",
            "Roc 0.8344158001036182\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.017907 : \n",
            "Roc 0.7418331128508744\n",
            "Epoch 1 - loss is 0.896400 : \n",
            "Roc 0.7720768197227177\n",
            "Epoch 2 - loss is 0.889087 : \n",
            "Roc 0.7395891152586952\n",
            "Epoch 3 - loss is 0.904340 : \n",
            "Roc 0.6766943691104954\n",
            "Epoch 4 - loss is 0.913716 : \n",
            "Roc 0.7207447520388033\n",
            "Epoch 5 - loss is 0.905176 : \n",
            "Roc 0.6704919445263399\n",
            "Epoch 6 - loss is 0.905713 : \n",
            "Roc 0.7105243106088891\n",
            "Epoch 7 - loss is 0.899387 : \n",
            "Roc 0.7560493958858774\n",
            "Epoch 8 - loss is 0.901114 : \n",
            "Roc 0.7572196038505595\n",
            "Epoch 9 - loss is 0.858886 : \n",
            "Roc 0.8037122915893709\n",
            "Epoch 10 - loss is 0.829166 : \n",
            "Roc 0.8143210439517178\n",
            "Epoch 11 - loss is 0.819235 : \n",
            "Roc 0.823832854563049\n",
            "Epoch 12 - loss is 0.808128 : \n",
            "Roc 0.8185172442291133\n",
            "Epoch 13 - loss is 0.801298 : \n",
            "Roc 0.8323540082773238\n",
            "Epoch 14 - loss is 0.801691 : \n",
            "Roc 0.830849804066607\n",
            "Epoch 15 - loss is 0.792685 : \n",
            "Roc 0.8304585624856275\n",
            "Saving\n",
            "Time mean 5.903517350554466\n",
            "Time std 0.1103879806840712\n",
            "Roc 0.8302034983737832\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.024259 : \n",
            "Roc 0.7251598975582939\n",
            "Epoch 1 - loss is 0.896887 : \n",
            "Roc 0.687421870959462\n",
            "Epoch 2 - loss is 0.897955 : \n",
            "Roc 0.7656574140939714\n",
            "Epoch 3 - loss is 0.914497 : \n",
            "Roc 0.7113961609261894\n",
            "Epoch 4 - loss is 0.915572 : \n",
            "Roc 0.7689067010437638\n",
            "Epoch 5 - loss is 0.902461 : \n",
            "Roc 0.7563130563371918\n",
            "Epoch 6 - loss is 0.908097 : \n",
            "Roc 0.7351540935498886\n",
            "Epoch 7 - loss is 0.895042 : \n",
            "Roc 0.7802164722738878\n",
            "Epoch 8 - loss is 0.901083 : \n",
            "Roc 0.7808932234849718\n",
            "Epoch 9 - loss is 0.862334 : \n",
            "Roc 0.7894556682942153\n",
            "Epoch 10 - loss is 0.847202 : \n",
            "Roc 0.8042886450226319\n",
            "Epoch 11 - loss is 0.828783 : \n",
            "Roc 0.8186688899402934\n",
            "Epoch 12 - loss is 0.820139 : \n",
            "Roc 0.816390385650499\n",
            "Epoch 13 - loss is 0.809130 : \n",
            "Roc 0.8238181755492258\n",
            "Epoch 14 - loss is 0.807230 : \n",
            "Roc 0.8318171718197195\n",
            "Epoch 15 - loss is 0.798008 : \n",
            "Roc 0.8336846067353315\n",
            "Saving\n",
            "Time mean 5.905072882771492\n",
            "Time std 0.12797937573645435\n",
            "Roc 0.8337763598715073\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.045009 : \n",
            "Roc 0.7117150360490705\n",
            "Epoch 1 - loss is 0.893346 : \n",
            "Roc 0.7413278580894601\n",
            "Epoch 2 - loss is 0.892747 : \n",
            "Roc 0.747847491560181\n",
            "Epoch 3 - loss is 0.892705 : \n",
            "Roc 0.7750520776341758\n",
            "Epoch 4 - loss is 0.893578 : \n",
            "Roc 0.7610708539516184\n",
            "Epoch 5 - loss is 0.903645 : \n",
            "Roc 0.7489222766639514\n",
            "Epoch 6 - loss is 0.901483 : \n",
            "Roc 0.7627199114882854\n",
            "Epoch 7 - loss is 0.896486 : \n",
            "Roc 0.7663283646812321\n",
            "Epoch 8 - loss is 0.896094 : \n",
            "Roc 0.7482158850968806\n",
            "Epoch 9 - loss is 0.856633 : \n",
            "Roc 0.7943773379210284\n",
            "Epoch 10 - loss is 0.824759 : \n",
            "Roc 0.8189274844273688\n",
            "Epoch 11 - loss is 0.810274 : \n",
            "Roc 0.8204061244918884\n",
            "Epoch 12 - loss is 0.802680 : \n",
            "Roc 0.8283789671201676\n",
            "Epoch 13 - loss is 0.792154 : \n",
            "Roc 0.8353639557927757\n",
            "Epoch 14 - loss is 0.779675 : \n",
            "Roc 0.8414175248331286\n",
            "Epoch 15 - loss is 0.772850 : \n",
            "Roc 0.836875929595902\n",
            "Saving\n",
            "Time mean 5.883465647697449\n",
            "Time std 0.10707134954496933\n",
            "Roc 0.8369149371623565\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.038316 : \n",
            "Roc 0.727202962651299\n",
            "Epoch 1 - loss is 0.889420 : \n",
            "Roc 0.744590543488969\n",
            "Epoch 2 - loss is 0.902182 : \n",
            "Roc 0.7356488973264941\n",
            "Epoch 3 - loss is 0.905306 : \n",
            "Roc 0.7152750767018349\n",
            "Epoch 4 - loss is 0.911689 : \n",
            "Roc 0.7424588467231037\n",
            "Epoch 5 - loss is 0.903824 : \n",
            "Roc 0.7385059106628209\n",
            "Epoch 6 - loss is 0.897760 : \n",
            "Roc 0.7535402356415382\n",
            "Epoch 7 - loss is 0.895063 : \n",
            "Roc 0.7476379131763096\n",
            "Epoch 8 - loss is 0.899200 : \n",
            "Roc 0.7657226496191106\n",
            "Epoch 9 - loss is 0.852363 : \n",
            "Roc 0.8102753573302699\n",
            "Epoch 10 - loss is 0.822647 : \n",
            "Roc 0.8205478298787254\n",
            "Epoch 11 - loss is 0.809155 : \n",
            "Roc 0.8236577031720433\n",
            "Epoch 12 - loss is 0.804223 : \n",
            "Roc 0.8289107534853855\n",
            "Epoch 13 - loss is 0.800711 : \n",
            "Roc 0.8276304598420711\n",
            "Epoch 14 - loss is 0.796212 : \n",
            "Roc 0.8289969044524939\n",
            "Epoch 15 - loss is 0.791946 : \n",
            "Roc 0.8222500473928441\n",
            "Saving\n",
            "Time mean 5.885174781084061\n",
            "Time std 0.17876404349467181\n",
            "Roc 0.8222418051590887\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.039049 : \n",
            "Roc 0.6447573910464536\n",
            "Epoch 1 - loss is 0.902226 : \n",
            "Roc 0.7598094288523976\n",
            "Epoch 2 - loss is 0.907983 : \n",
            "Roc 0.7581667356081419\n",
            "Epoch 3 - loss is 0.898329 : \n",
            "Roc 0.7929062148788711\n",
            "Epoch 4 - loss is 0.896847 : \n",
            "Roc 0.667573734858891\n",
            "Epoch 5 - loss is 0.902641 : \n",
            "Roc 0.7739998282967033\n",
            "Epoch 6 - loss is 0.899257 : \n",
            "Roc 0.7796923779345654\n",
            "Epoch 7 - loss is 0.895273 : \n",
            "Roc 0.7503636988011988\n",
            "Epoch 8 - loss is 0.901354 : \n",
            "Roc 0.7440379932567432\n",
            "Epoch 9 - loss is 0.855020 : \n",
            "Roc 0.7984282319243258\n",
            "Epoch 10 - loss is 0.828286 : \n",
            "Roc 0.8216568587662337\n",
            "Epoch 11 - loss is 0.805272 : \n",
            "Roc 0.8347755759865134\n",
            "Epoch 12 - loss is 0.789325 : \n",
            "Roc 0.8400541840971528\n",
            "Epoch 13 - loss is 0.780819 : \n",
            "Roc 0.8430265047452548\n",
            "Epoch 14 - loss is 0.775380 : \n",
            "Roc 0.843867265546953\n",
            "Epoch 15 - loss is 0.764584 : \n",
            "Roc 0.8458854621940559\n",
            "Saving\n",
            "Time mean 5.897229105234146\n",
            "Time std 0.11188736210886832\n",
            "Roc 0.8452458674138361\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.038897 : \n",
            "Roc 0.7714814416948654\n",
            "Epoch 1 - loss is 0.897736 : \n",
            "Roc 0.6807344044908171\n",
            "Epoch 2 - loss is 0.893610 : \n",
            "Roc 0.7325681881299962\n",
            "Epoch 3 - loss is 0.905708 : \n",
            "Roc 0.7586590712166086\n",
            "Epoch 4 - loss is 0.902663 : \n",
            "Roc 0.7619532989327148\n",
            "Epoch 5 - loss is 0.901827 : \n",
            "Roc 0.7388408257314294\n",
            "Epoch 6 - loss is 0.900892 : \n",
            "Roc 0.7574239049740163\n",
            "Epoch 7 - loss is 0.898341 : \n",
            "Roc 0.7267499159970372\n",
            "Epoch 8 - loss is 0.899361 : \n",
            "Roc 0.7394012618556598\n",
            "Epoch 9 - loss is 0.855826 : \n",
            "Roc 0.776154275657128\n",
            "Epoch 10 - loss is 0.833878 : \n",
            "Roc 0.8040626206176371\n",
            "Epoch 11 - loss is 0.818300 : \n",
            "Roc 0.8226528822696914\n",
            "Epoch 12 - loss is 0.804958 : \n",
            "Roc 0.8019756622493808\n",
            "Epoch 13 - loss is 0.802674 : \n",
            "Roc 0.8311833987661245\n",
            "Epoch 14 - loss is 0.793922 : \n",
            "Roc 0.8384833291071621\n",
            "Epoch 15 - loss is 0.792954 : \n",
            "Roc 0.8306291041224534\n",
            "Saving\n",
            "Time mean 5.900272071361542\n",
            "Time std 0.12721189429458352\n",
            "Roc 0.8294280803480534\n",
            "test\n",
            "Crossvalidation run 5\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.054861 : \n",
            "Roc 0.7841961188981569\n",
            "Epoch 1 - loss is 0.897964 : \n",
            "Roc 0.7593636637970846\n",
            "Epoch 2 - loss is 0.901796 : \n",
            "Roc 0.7408861506833173\n",
            "Epoch 3 - loss is 0.912241 : \n",
            "Roc 0.7777021367398871\n",
            "Epoch 4 - loss is 0.901849 : \n",
            "Roc 0.7843710885621197\n",
            "Epoch 5 - loss is 0.894945 : \n",
            "Roc 0.7598766294672005\n",
            "Epoch 6 - loss is 0.896755 : \n",
            "Roc 0.7577261348544349\n",
            "Epoch 7 - loss is 0.904115 : \n",
            "Roc 0.7540408238662603\n",
            "Epoch 8 - loss is 0.902687 : \n",
            "Roc 0.7858953550032963\n",
            "Epoch 9 - loss is 0.872416 : \n",
            "Roc 0.8061238586165493\n",
            "Epoch 10 - loss is 0.847669 : \n",
            "Roc 0.8153896867027832\n",
            "Epoch 11 - loss is 0.840391 : \n",
            "Roc 0.8199190919572079\n",
            "Epoch 12 - loss is 0.836861 : \n",
            "Roc 0.8239963429548733\n",
            "Epoch 13 - loss is 0.829749 : \n",
            "Roc 0.8319367609074376\n",
            "Epoch 14 - loss is 0.819231 : \n",
            "Roc 0.8361396158390741\n",
            "Epoch 15 - loss is 0.813584 : \n",
            "Roc 0.8358396394072289\n",
            "Saving\n",
            "Time mean 5.916366174817085\n",
            "Time std 0.061507057025069936\n",
            "Roc 0.835757927180771\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.011093 : \n",
            "Roc 0.7366476871582245\n",
            "Epoch 1 - loss is 0.887881 : \n",
            "Roc 0.7524856951682883\n",
            "Epoch 2 - loss is 0.896800 : \n",
            "Roc 0.7540615934122359\n",
            "Epoch 3 - loss is 0.908005 : \n",
            "Roc 0.7457940946638084\n",
            "Epoch 4 - loss is 0.917847 : \n",
            "Roc 0.7410693082334214\n",
            "Epoch 5 - loss is 0.909096 : \n",
            "Roc 0.7505031361518317\n",
            "Epoch 6 - loss is 0.913387 : \n",
            "Roc 0.7498958025788812\n",
            "Epoch 7 - loss is 0.899062 : \n",
            "Roc 0.7526430443056661\n",
            "Epoch 8 - loss is 0.888090 : \n",
            "Roc 0.7714027921298554\n",
            "Epoch 9 - loss is 0.845906 : \n",
            "Roc 0.8077292824638649\n",
            "Epoch 10 - loss is 0.821723 : \n",
            "Roc 0.816463814311571\n",
            "Epoch 11 - loss is 0.808241 : \n",
            "Roc 0.8168177245128494\n",
            "Epoch 12 - loss is 0.804039 : \n",
            "Roc 0.8228154444045083\n",
            "Epoch 13 - loss is 0.786770 : \n",
            "Roc 0.8215694879463943\n",
            "Epoch 14 - loss is 0.784819 : \n",
            "Roc 0.8290546536032452\n",
            "Epoch 15 - loss is 0.784872 : \n",
            "Roc 0.8160637724320299\n",
            "Saving\n",
            "Time mean 5.926130294799805\n",
            "Time std 0.1278254596873059\n",
            "Roc 0.8160637724320299\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.037611 : \n",
            "Roc 0.754398996015336\n",
            "Epoch 1 - loss is 0.892026 : \n",
            "Roc 0.7228099852111248\n",
            "Epoch 2 - loss is 0.896943 : \n",
            "Roc 0.7496797392409696\n",
            "Epoch 3 - loss is 0.904290 : \n",
            "Roc 0.7548340636458097\n",
            "Epoch 4 - loss is 0.901871 : \n",
            "Roc 0.7700278542205745\n",
            "Epoch 5 - loss is 0.900294 : \n",
            "Roc 0.7644022224023114\n",
            "Epoch 6 - loss is 0.901440 : \n",
            "Roc 0.7185528094487481\n",
            "Epoch 7 - loss is 0.893694 : \n",
            "Roc 0.7606728538724444\n",
            "Epoch 8 - loss is 0.898289 : \n",
            "Roc 0.7651254010631097\n",
            "Epoch 9 - loss is 0.872442 : \n",
            "Roc 0.785754493257498\n",
            "Epoch 10 - loss is 0.843260 : \n",
            "Roc 0.8081054031709145\n",
            "Epoch 11 - loss is 0.815543 : \n",
            "Roc 0.8282774555829623\n",
            "Epoch 12 - loss is 0.798400 : \n",
            "Roc 0.8311535246565592\n",
            "Epoch 13 - loss is 0.788289 : \n",
            "Roc 0.832921854852916\n",
            "Epoch 14 - loss is 0.777864 : \n",
            "Roc 0.8341775966232513\n",
            "Epoch 15 - loss is 0.773388 : \n",
            "Roc 0.8334996939117546\n",
            "Saving\n",
            "Time mean 5.916650146245956\n",
            "Time std 0.0881079753091896\n",
            "Roc 0.8330810816157959\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.031733 : \n",
            "Roc 0.7269383967817079\n",
            "Epoch 1 - loss is 0.889578 : \n",
            "Roc 0.732321347286394\n",
            "Epoch 2 - loss is 0.895933 : \n",
            "Roc 0.7408262046779074\n",
            "Epoch 3 - loss is 0.900344 : \n",
            "Roc 0.7628029825964109\n",
            "Epoch 4 - loss is 0.895759 : \n",
            "Roc 0.7688923126875522\n",
            "Epoch 5 - loss is 0.898224 : \n",
            "Roc 0.7637514062732168\n",
            "Epoch 6 - loss is 0.904935 : \n",
            "Roc 0.723857076256762\n",
            "Epoch 7 - loss is 0.906264 : \n",
            "Roc 0.7589347976656846\n",
            "Epoch 8 - loss is 0.899642 : \n",
            "Roc 0.7358026351063971\n",
            "Epoch 9 - loss is 0.848488 : \n",
            "Roc 0.7983740297599253\n",
            "Epoch 10 - loss is 0.828086 : \n",
            "Roc 0.8146062159831254\n",
            "Epoch 11 - loss is 0.810013 : \n",
            "Roc 0.8270957106996263\n",
            "Epoch 12 - loss is 0.798589 : \n",
            "Roc 0.8302420873300581\n",
            "Epoch 13 - loss is 0.789048 : \n",
            "Roc 0.8354305574227248\n",
            "Epoch 14 - loss is 0.783088 : \n",
            "Roc 0.8369404093935513\n",
            "Epoch 15 - loss is 0.786105 : \n",
            "Roc 0.8327170692283442\n",
            "Saving\n",
            "Time mean 5.922871217131615\n",
            "Time std 0.10250423225532486\n",
            "Roc 0.8336596948534529\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.027405 : \n",
            "Roc 0.7563216531358623\n",
            "Epoch 1 - loss is 0.893746 : \n",
            "Roc 0.7480058401964238\n",
            "Epoch 2 - loss is 0.899682 : \n",
            "Roc 0.7519731568223252\n",
            "Epoch 3 - loss is 0.902288 : \n",
            "Roc 0.735054522762445\n",
            "Epoch 4 - loss is 0.904763 : \n",
            "Roc 0.7409709160484463\n",
            "Epoch 5 - loss is 0.899412 : \n",
            "Roc 0.723659086641889\n",
            "Epoch 6 - loss is 0.899290 : \n",
            "Roc 0.7400600704801438\n",
            "Epoch 7 - loss is 0.900638 : \n",
            "Roc 0.7130724118319269\n",
            "Epoch 8 - loss is 0.899451 : \n",
            "Roc 0.7429808525100321\n",
            "Epoch 9 - loss is 0.857229 : \n",
            "Roc 0.784794173714388\n",
            "Epoch 10 - loss is 0.846470 : \n",
            "Roc 0.7918419927159708\n",
            "Epoch 11 - loss is 0.825920 : \n",
            "Roc 0.8128062894062329\n",
            "Epoch 12 - loss is 0.816454 : \n",
            "Roc 0.8280009206143968\n",
            "Epoch 13 - loss is 0.801235 : \n",
            "Roc 0.8326122961025694\n",
            "Epoch 14 - loss is 0.794196 : \n",
            "Roc 0.8339959529697336\n",
            "Epoch 15 - loss is 0.791045 : \n",
            "Roc 0.8307033693275587\n",
            "Saving\n",
            "Time mean 5.903596639633179\n",
            "Time std 0.059806498556032685\n",
            "Roc 0.8312622440054073\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.034484 : \n",
            "Roc 0.7604220261161223\n",
            "Epoch 1 - loss is 0.901658 : \n",
            "Roc 0.734701840617715\n",
            "Epoch 2 - loss is 0.894919 : \n",
            "Roc 0.7425897370573487\n",
            "Epoch 3 - loss is 0.906658 : \n",
            "Roc 0.7089065669577609\n",
            "Epoch 4 - loss is 0.906291 : \n",
            "Roc 0.7631040334985145\n",
            "Epoch 5 - loss is 0.901324 : \n",
            "Roc 0.7710295697946378\n",
            "Epoch 6 - loss is 0.902547 : \n",
            "Roc 0.7618580872057051\n",
            "Epoch 7 - loss is 0.902562 : \n",
            "Roc 0.7714557717319888\n",
            "Epoch 8 - loss is 0.900684 : \n",
            "Roc 0.7491360072636303\n",
            "Epoch 9 - loss is 0.870793 : \n",
            "Roc 0.7896554564383309\n",
            "Epoch 10 - loss is 0.848370 : \n",
            "Roc 0.8004588614565954\n",
            "Epoch 11 - loss is 0.837230 : \n",
            "Roc 0.8158851687472345\n",
            "Epoch 12 - loss is 0.829436 : \n",
            "Roc 0.8196014580895044\n",
            "Epoch 13 - loss is 0.828673 : \n",
            "Roc 0.8166909298481189\n",
            "Epoch 14 - loss is 0.820408 : \n",
            "Roc 0.8252764566049807\n",
            "Epoch 15 - loss is 0.820933 : \n",
            "Roc 0.8172517924425298\n",
            "Saving\n",
            "Time mean 5.932018607854843\n",
            "Time std 0.11077083288265119\n",
            "Roc 0.819438064717567\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.047478 : \n",
            "Roc 0.7613571796212314\n",
            "Epoch 1 - loss is 0.900113 : \n",
            "Roc 0.6772131981341212\n",
            "Epoch 2 - loss is 0.896281 : \n",
            "Roc 0.6656894511382103\n",
            "Epoch 3 - loss is 0.909208 : \n",
            "Roc 0.7530655894499224\n",
            "Epoch 4 - loss is 0.901750 : \n",
            "Roc 0.7573994820605721\n",
            "Epoch 5 - loss is 0.902983 : \n",
            "Roc 0.7655270378160625\n",
            "Epoch 6 - loss is 0.900180 : \n",
            "Roc 0.7670660636198211\n",
            "Epoch 7 - loss is 0.901733 : \n",
            "Roc 0.7520982018018456\n",
            "Epoch 8 - loss is 0.901350 : \n",
            "Roc 0.7722444447146266\n",
            "Epoch 9 - loss is 0.859310 : \n",
            "Roc 0.7785408738505433\n",
            "Epoch 10 - loss is 0.839193 : \n",
            "Roc 0.802881493999927\n",
            "Epoch 11 - loss is 0.813566 : \n",
            "Roc 0.8172848099470308\n",
            "Epoch 12 - loss is 0.799705 : \n",
            "Roc 0.8293030918309037\n",
            "Epoch 13 - loss is 0.788917 : \n",
            "Roc 0.8357781452256765\n",
            "Epoch 14 - loss is 0.779655 : \n",
            "Roc 0.8367963946876761\n",
            "Epoch 15 - loss is 0.781701 : \n",
            "Roc 0.834367489776978\n",
            "Saving\n",
            "Time mean 5.930186942219734\n",
            "Time std 0.12433148687598894\n",
            "Roc 0.834835884529498\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.020470 : \n",
            "Roc 0.747835824908266\n",
            "Epoch 1 - loss is 0.890744 : \n",
            "Roc 0.7689099410366298\n",
            "Epoch 2 - loss is 0.893763 : \n",
            "Roc 0.7707541408393656\n",
            "Epoch 3 - loss is 0.893042 : \n",
            "Roc 0.7706604344913148\n",
            "Epoch 4 - loss is 0.905880 : \n",
            "Roc 0.7355766796593053\n",
            "Epoch 5 - loss is 0.894192 : \n",
            "Roc 0.7243131766236122\n",
            "Epoch 6 - loss is 0.903036 : \n",
            "Roc 0.7512983480601118\n",
            "Epoch 7 - loss is 0.897345 : \n",
            "Roc 0.7593875117599014\n",
            "Epoch 8 - loss is 0.901808 : \n",
            "Roc 0.7584166943697301\n",
            "Epoch 9 - loss is 0.859234 : \n",
            "Roc 0.7916487921398919\n",
            "Epoch 10 - loss is 0.838664 : \n",
            "Roc 0.8107284839431474\n",
            "Epoch 11 - loss is 0.814216 : \n",
            "Roc 0.8310788534503364\n",
            "Epoch 12 - loss is 0.793231 : \n",
            "Roc 0.8396513652867728\n",
            "Epoch 13 - loss is 0.791926 : \n",
            "Roc 0.8326334957315041\n",
            "Epoch 14 - loss is 0.778186 : \n",
            "Roc 0.8318272286509465\n",
            "Epoch 15 - loss is 0.771653 : \n",
            "Roc 0.8445929769103633\n",
            "Saving\n",
            "Time mean 5.909352585673332\n",
            "Time std 0.07024565873468586\n",
            "Roc 0.8436899617050121\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.027797 : \n",
            "Roc 0.7493147477522477\n",
            "Epoch 1 - loss is 0.896871 : \n",
            "Roc 0.7769214184253246\n",
            "Epoch 2 - loss is 0.904322 : \n",
            "Roc 0.7766496199113387\n",
            "Epoch 3 - loss is 0.897157 : \n",
            "Roc 0.7512642630806694\n",
            "Epoch 4 - loss is 0.913732 : \n",
            "Roc 0.7585562874625374\n",
            "Epoch 5 - loss is 0.901665 : \n",
            "Roc 0.7700166044892608\n",
            "Epoch 6 - loss is 0.902171 : \n",
            "Roc 0.7706495652784715\n",
            "Epoch 7 - loss is 0.916336 : \n",
            "Roc 0.765898066776973\n",
            "Epoch 8 - loss is 0.902340 : \n",
            "Roc 0.7601342992944555\n",
            "Epoch 9 - loss is 0.870279 : \n",
            "Roc 0.7982395534153347\n",
            "Epoch 10 - loss is 0.850619 : \n",
            "Roc 0.803883811500999\n",
            "Epoch 11 - loss is 0.836298 : \n",
            "Roc 0.8152465698364135\n",
            "Epoch 12 - loss is 0.821263 : \n",
            "Roc 0.8209214613511487\n",
            "Epoch 13 - loss is 0.812558 : \n",
            "Roc 0.8300736373001998\n",
            "Epoch 14 - loss is 0.804139 : \n",
            "Roc 0.8360246199113387\n",
            "Epoch 15 - loss is 0.798454 : \n",
            "Roc 0.8349655227584916\n",
            "Saving\n",
            "Time mean 5.914318174123764\n",
            "Time std 0.13392128862888605\n",
            "Roc 0.8341969553883616\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.042404 : \n",
            "Roc 0.7714014983547077\n",
            "Epoch 1 - loss is 0.895103 : \n",
            "Roc 0.7152058062181761\n",
            "Epoch 2 - loss is 0.906321 : \n",
            "Roc 0.7320945654767184\n",
            "Epoch 3 - loss is 0.914418 : \n",
            "Roc 0.6974210153553669\n",
            "Epoch 4 - loss is 0.911324 : \n",
            "Roc 0.7439051530561047\n",
            "Epoch 5 - loss is 0.898156 : \n",
            "Roc 0.7000812132734259\n",
            "Epoch 6 - loss is 0.917409 : \n",
            "Roc 0.731709005409083\n",
            "Epoch 7 - loss is 0.907392 : \n",
            "Roc 0.7535664306256274\n",
            "Epoch 8 - loss is 0.895994 : \n",
            "Roc 0.751332180816846\n",
            "Epoch 9 - loss is 0.865439 : \n",
            "Roc 0.779533859543715\n",
            "Epoch 10 - loss is 0.836436 : \n",
            "Roc 0.8049355436001401\n",
            "Epoch 11 - loss is 0.816921 : \n",
            "Roc 0.825423002280467\n",
            "Epoch 12 - loss is 0.802335 : \n",
            "Roc 0.8254795247201879\n",
            "Epoch 13 - loss is 0.805847 : \n",
            "Roc 0.8207082188832\n",
            "Epoch 14 - loss is 0.796742 : \n",
            "Roc 0.8187387813886908\n",
            "Epoch 15 - loss is 0.798697 : \n",
            "Roc 0.8343635406738807\n",
            "Saving\n",
            "Time mean 5.909761369228363\n",
            "Time std 0.08625046100456663\n",
            "Roc 0.8354695276389527\n",
            "test\n",
            "Crossvalidation run 6\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.031699 : \n",
            "Roc 0.741978666042429\n",
            "Epoch 1 - loss is 0.896253 : \n",
            "Roc 0.778342099731514\n",
            "Epoch 2 - loss is 0.897616 : \n",
            "Roc 0.7559638977269469\n",
            "Epoch 3 - loss is 0.901607 : \n",
            "Roc 0.7740192144160671\n",
            "Epoch 4 - loss is 0.897839 : \n",
            "Roc 0.7814173060324923\n",
            "Epoch 5 - loss is 0.909792 : \n",
            "Roc 0.6744881490399163\n",
            "Epoch 6 - loss is 0.911790 : \n",
            "Roc 0.6729097395400389\n",
            "Epoch 7 - loss is 0.898264 : \n",
            "Roc 0.7781054229050618\n",
            "Epoch 8 - loss is 0.901433 : \n",
            "Roc 0.7260443837723698\n",
            "Epoch 9 - loss is 0.865535 : \n",
            "Roc 0.8302021924753888\n",
            "Epoch 10 - loss is 0.824287 : \n",
            "Roc 0.8325993166827504\n",
            "Epoch 11 - loss is 0.817233 : \n",
            "Roc 0.847283929384712\n",
            "Epoch 12 - loss is 0.807185 : \n",
            "Roc 0.8465141822964937\n",
            "Epoch 13 - loss is 0.796739 : \n",
            "Roc 0.8476927891000468\n",
            "Epoch 14 - loss is 0.789532 : \n",
            "Roc 0.8533727343518597\n",
            "Epoch 15 - loss is 0.790122 : \n",
            "Roc 0.8560228579254292\n",
            "Saving\n",
            "Time mean 5.900989010930061\n",
            "Time std 0.14412500203395776\n",
            "Roc 0.8561006885723113\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.028984 : \n",
            "Roc 0.7560869746604708\n",
            "Epoch 1 - loss is 0.890834 : \n",
            "Roc 0.7594669223832509\n",
            "Epoch 2 - loss is 0.907890 : \n",
            "Roc 0.7603668912543965\n",
            "Epoch 3 - loss is 0.899200 : \n",
            "Roc 0.6841569576198301\n",
            "Epoch 4 - loss is 0.904871 : \n",
            "Roc 0.7418201514402711\n",
            "Epoch 5 - loss is 0.899897 : \n",
            "Roc 0.7696075678416472\n",
            "Epoch 6 - loss is 0.897594 : \n",
            "Roc 0.7839134024158559\n",
            "Epoch 7 - loss is 0.904829 : \n",
            "Roc 0.7514233628469101\n",
            "Epoch 8 - loss is 0.904333 : \n",
            "Roc 0.758161496174681\n",
            "Epoch 9 - loss is 0.847092 : \n",
            "Roc 0.8052489024672024\n",
            "Epoch 10 - loss is 0.814515 : \n",
            "Roc 0.8217332554042258\n",
            "Epoch 11 - loss is 0.797364 : \n",
            "Roc 0.8219689281083925\n",
            "Epoch 12 - loss is 0.783333 : \n",
            "Roc 0.8323990999910144\n",
            "Epoch 13 - loss is 0.771776 : \n",
            "Roc 0.8358329014338528\n",
            "Epoch 14 - loss is 0.770217 : \n",
            "Roc 0.8315493745346717\n",
            "Epoch 15 - loss is 0.759564 : \n",
            "Roc 0.840622071641293\n",
            "Saving\n",
            "Time mean 5.880660146474838\n",
            "Time std 0.12044300778054053\n",
            "Roc 0.842564917299941\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.044777 : \n",
            "Roc 0.7693106679648309\n",
            "Epoch 1 - loss is 0.888807 : \n",
            "Roc 0.7106079266392652\n",
            "Epoch 2 - loss is 0.906025 : \n",
            "Roc 0.7330880061727478\n",
            "Epoch 3 - loss is 0.905005 : \n",
            "Roc 0.757961338143998\n",
            "Epoch 4 - loss is 0.903406 : \n",
            "Roc 0.7669364390351734\n",
            "Epoch 5 - loss is 0.904414 : \n",
            "Roc 0.7199081392840389\n",
            "Epoch 6 - loss is 0.900085 : \n",
            "Roc 0.7490965735998601\n",
            "Epoch 7 - loss is 0.897573 : \n",
            "Roc 0.7301657099744437\n",
            "Epoch 8 - loss is 0.901766 : \n",
            "Roc 0.7523830748609286\n",
            "Epoch 9 - loss is 0.863720 : \n",
            "Roc 0.7889239999436903\n",
            "Epoch 10 - loss is 0.849234 : \n",
            "Roc 0.7978800773267558\n",
            "Epoch 11 - loss is 0.835281 : \n",
            "Roc 0.8006916567078601\n",
            "Epoch 12 - loss is 0.830941 : \n",
            "Roc 0.8052306657428852\n",
            "Epoch 13 - loss is 0.826197 : \n",
            "Roc 0.8076672917791432\n",
            "Epoch 14 - loss is 0.811016 : \n",
            "Roc 0.8054867221620444\n",
            "Epoch 15 - loss is 0.809082 : \n",
            "Roc 0.8121189829119435\n",
            "Saving\n",
            "Time mean 5.926444038748741\n",
            "Time std 0.22501709690143942\n",
            "Roc 0.8121674928576239\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.023762 : \n",
            "Roc 0.7611646890396775\n",
            "Epoch 1 - loss is 0.896312 : \n",
            "Roc 0.7134757138286087\n",
            "Epoch 2 - loss is 0.905624 : \n",
            "Roc 0.729005254803735\n",
            "Epoch 3 - loss is 0.895769 : \n",
            "Roc 0.7469318479314516\n",
            "Epoch 4 - loss is 0.910430 : \n",
            "Roc 0.7543616089573806\n",
            "Epoch 5 - loss is 0.900453 : \n",
            "Roc 0.7654282224785158\n",
            "Epoch 6 - loss is 0.900700 : \n",
            "Roc 0.757178086273437\n",
            "Epoch 7 - loss is 0.925143 : \n",
            "Roc 0.7687179780483797\n",
            "Epoch 8 - loss is 0.893936 : \n",
            "Roc 0.7539168296017466\n",
            "Epoch 9 - loss is 0.849774 : \n",
            "Roc 0.8029703124029564\n",
            "Epoch 10 - loss is 0.825017 : \n",
            "Roc 0.8164822296594519\n",
            "Epoch 11 - loss is 0.809348 : \n",
            "Roc 0.8260959890061079\n",
            "Epoch 12 - loss is 0.798946 : \n",
            "Roc 0.8270508986390499\n",
            "Epoch 13 - loss is 0.789408 : \n",
            "Roc 0.8374472966927914\n",
            "Epoch 14 - loss is 0.790702 : \n",
            "Roc 0.8344740754367799\n",
            "Epoch 15 - loss is 0.785109 : \n",
            "Roc 0.8304318506830694\n",
            "Saving\n",
            "Time mean 5.8911270797252655\n",
            "Time std 0.08331447290569188\n",
            "Roc 0.8302205657483339\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.044460 : \n",
            "Roc 0.7600306819365179\n",
            "Epoch 1 - loss is 0.891203 : \n",
            "Roc 0.7660168246579299\n",
            "Epoch 2 - loss is 0.895236 : \n",
            "Roc 0.7597073337135362\n",
            "Epoch 3 - loss is 0.898778 : \n",
            "Roc 0.7821995728677433\n",
            "Epoch 4 - loss is 0.891603 : \n",
            "Roc 0.7704559756998439\n",
            "Epoch 5 - loss is 0.895493 : \n",
            "Roc 0.7599091577101164\n",
            "Epoch 6 - loss is 0.894794 : \n",
            "Roc 0.7549041961221268\n",
            "Epoch 7 - loss is 0.892611 : \n",
            "Roc 0.7453748993078002\n",
            "Epoch 8 - loss is 0.901790 : \n",
            "Roc 0.7562239648509735\n",
            "Epoch 9 - loss is 0.858379 : \n",
            "Roc 0.7762735084415378\n",
            "Epoch 10 - loss is 0.835132 : \n",
            "Roc 0.7981467946226897\n",
            "Epoch 11 - loss is 0.825592 : \n",
            "Roc 0.8192453150163892\n",
            "Epoch 12 - loss is 0.799403 : \n",
            "Roc 0.8287009548639095\n",
            "Epoch 13 - loss is 0.795356 : \n",
            "Roc 0.8344755047602525\n",
            "Epoch 14 - loss is 0.784349 : \n",
            "Roc 0.8398242314227654\n",
            "Epoch 15 - loss is 0.778172 : \n",
            "Roc 0.8378257244905116\n",
            "Saving\n",
            "Time mean 5.92410534620285\n",
            "Time std 0.09558124440035179\n",
            "Roc 0.837597915410151\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.024792 : \n",
            "Roc 0.7262637126881754\n",
            "Epoch 1 - loss is 0.889119 : \n",
            "Roc 0.7614698124519924\n",
            "Epoch 2 - loss is 0.903655 : \n",
            "Roc 0.7333712285917331\n",
            "Epoch 3 - loss is 0.901472 : \n",
            "Roc 0.7594057499909012\n",
            "Epoch 4 - loss is 0.898890 : \n",
            "Roc 0.761271939822202\n",
            "Epoch 5 - loss is 0.904079 : \n",
            "Roc 0.7562878673238158\n",
            "Epoch 6 - loss is 0.899306 : \n",
            "Roc 0.7480247216278806\n",
            "Epoch 7 - loss is 0.897389 : \n",
            "Roc 0.7530301521109926\n",
            "Epoch 8 - loss is 0.892501 : \n",
            "Roc 0.7727149350736419\n",
            "Epoch 9 - loss is 0.860675 : \n",
            "Roc 0.78954770875754\n",
            "Epoch 10 - loss is 0.842200 : \n",
            "Roc 0.808050619381557\n",
            "Epoch 11 - loss is 0.819738 : \n",
            "Roc 0.8240692037015399\n",
            "Epoch 12 - loss is 0.803703 : \n",
            "Roc 0.8231473624325499\n",
            "Epoch 13 - loss is 0.795870 : \n",
            "Roc 0.8398985352061955\n",
            "Epoch 14 - loss is 0.787816 : \n",
            "Roc 0.8370199002783241\n",
            "Epoch 15 - loss is 0.787342 : \n",
            "Roc 0.8368905072856587\n",
            "Saving\n",
            "Time mean 5.906691089272499\n",
            "Time std 0.0948284372171783\n",
            "Roc 0.8371329156235097\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.022843 : \n",
            "Roc 0.43562363473517407\n",
            "Epoch 1 - loss is 0.888304 : \n",
            "Roc 0.742581166133732\n",
            "Epoch 2 - loss is 0.897603 : \n",
            "Roc 0.7639236748572426\n",
            "Epoch 3 - loss is 0.897583 : \n",
            "Roc 0.7604620319598616\n",
            "Epoch 4 - loss is 0.896053 : \n",
            "Roc 0.7467714906361577\n",
            "Epoch 5 - loss is 0.896526 : \n",
            "Roc 0.7584422505643433\n",
            "Epoch 6 - loss is 0.900095 : \n",
            "Roc 0.7440864529254663\n",
            "Epoch 7 - loss is 0.892512 : \n",
            "Roc 0.771308567074777\n",
            "Epoch 8 - loss is 0.896222 : \n",
            "Roc 0.7544432151150774\n",
            "Epoch 9 - loss is 0.861512 : \n",
            "Roc 0.7922684976919678\n",
            "Epoch 10 - loss is 0.837960 : \n",
            "Roc 0.7966598378095783\n",
            "Epoch 11 - loss is 0.826301 : \n",
            "Roc 0.8066292599302118\n",
            "Epoch 12 - loss is 0.820135 : \n",
            "Roc 0.8082864229352331\n",
            "Epoch 13 - loss is 0.818655 : \n",
            "Roc 0.8172336441780448\n",
            "Epoch 14 - loss is 0.806732 : \n",
            "Roc 0.822133906389946\n",
            "Epoch 15 - loss is 0.800866 : \n",
            "Roc 0.8152887370464484\n",
            "Saving\n",
            "Time mean 5.939735561609268\n",
            "Time std 0.10786876302635053\n",
            "Roc 0.8152923845072079\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.011620 : \n",
            "Roc 0.7255501985397117\n",
            "Epoch 1 - loss is 0.897465 : \n",
            "Roc 0.7619660553414973\n",
            "Epoch 2 - loss is 0.902505 : \n",
            "Roc 0.6791547628532729\n",
            "Epoch 3 - loss is 0.909240 : \n",
            "Roc 0.7249956139541802\n",
            "Epoch 4 - loss is 0.897529 : \n",
            "Roc 0.7613919445117123\n",
            "Epoch 5 - loss is 0.904371 : \n",
            "Roc 0.769151615104953\n",
            "Epoch 6 - loss is 0.900130 : \n",
            "Roc 0.7558780863731148\n",
            "Epoch 7 - loss is 0.900582 : \n",
            "Roc 0.7617874736101337\n",
            "Epoch 8 - loss is 0.902249 : \n",
            "Roc 0.772980132684264\n",
            "Epoch 9 - loss is 0.848424 : \n",
            "Roc 0.8105088872866684\n",
            "Epoch 10 - loss is 0.819855 : \n",
            "Roc 0.8236847847972391\n",
            "Epoch 11 - loss is 0.806829 : \n",
            "Roc 0.8243739925341061\n",
            "Epoch 12 - loss is 0.796390 : \n",
            "Roc 0.8333218203718974\n",
            "Epoch 13 - loss is 0.786046 : \n",
            "Roc 0.8312279986325742\n",
            "Epoch 14 - loss is 0.784831 : \n",
            "Roc 0.8388039851592693\n",
            "Epoch 15 - loss is 0.779677 : \n",
            "Roc 0.8467133896657304\n",
            "Saving\n",
            "Time mean 5.92400036752224\n",
            "Time std 0.09645348549877085\n",
            "Roc 0.8459783590340573\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.027078 : \n",
            "Roc 0.7649005291583417\n",
            "Epoch 1 - loss is 0.886778 : \n",
            "Roc 0.7637293370691809\n",
            "Epoch 2 - loss is 0.908883 : \n",
            "Roc 0.7870085968718782\n",
            "Epoch 3 - loss is 0.899524 : \n",
            "Roc 0.750356479458042\n",
            "Epoch 4 - loss is 0.905336 : \n",
            "Roc 0.7586312125374626\n",
            "Epoch 5 - loss is 0.901933 : \n",
            "Roc 0.7707190270666833\n",
            "Epoch 6 - loss is 0.897303 : \n",
            "Roc 0.722416060501998\n",
            "Epoch 7 - loss is 0.906954 : \n",
            "Roc 0.7523754565746753\n",
            "Epoch 8 - loss is 0.897604 : \n",
            "Roc 0.7738734897914585\n",
            "Epoch 9 - loss is 0.852544 : \n",
            "Roc 0.8119176721715784\n",
            "Epoch 10 - loss is 0.824994 : \n",
            "Roc 0.8301943174013486\n",
            "Epoch 11 - loss is 0.815114 : \n",
            "Roc 0.8354143902972028\n",
            "Epoch 12 - loss is 0.805107 : \n",
            "Roc 0.8291574636301199\n",
            "Epoch 13 - loss is 0.796355 : \n",
            "Roc 0.8371394230769231\n",
            "Epoch 14 - loss is 0.786124 : \n",
            "Roc 0.8391711803821178\n",
            "Epoch 15 - loss is 0.778763 : \n",
            "Roc 0.8431825010926574\n",
            "Saving\n",
            "Time mean 5.913808166980743\n",
            "Time std 0.06395111446731075\n",
            "Roc 0.842180573332917\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.021193 : \n",
            "Roc 0.7450064558410894\n",
            "Epoch 1 - loss is 0.895099 : \n",
            "Roc 0.7117651198046717\n",
            "Epoch 2 - loss is 0.893723 : \n",
            "Roc 0.7506081106682306\n",
            "Epoch 3 - loss is 0.900365 : \n",
            "Roc 0.7275428956393822\n",
            "Epoch 4 - loss is 0.902810 : \n",
            "Roc 0.7408109918761744\n",
            "Epoch 5 - loss is 0.898806 : \n",
            "Roc 0.725247855582356\n",
            "Epoch 6 - loss is 0.901143 : \n",
            "Roc 0.7360569654590642\n",
            "Epoch 7 - loss is 0.897412 : \n",
            "Roc 0.7525895188451851\n",
            "Epoch 8 - loss is 0.894814 : \n",
            "Roc 0.773922836356399\n",
            "Epoch 9 - loss is 0.844670 : \n",
            "Roc 0.8178938593938214\n",
            "Epoch 10 - loss is 0.816458 : \n",
            "Roc 0.8267826636204922\n",
            "Epoch 11 - loss is 0.803738 : \n",
            "Roc 0.8303024606310279\n",
            "Epoch 12 - loss is 0.792020 : \n",
            "Roc 0.8190454390450768\n",
            "Epoch 13 - loss is 0.786828 : \n",
            "Roc 0.8417794304953031\n",
            "Epoch 14 - loss is 0.776801 : \n",
            "Roc 0.8356534597770497\n",
            "Epoch 15 - loss is 0.771064 : \n",
            "Roc 0.8469785581136038\n",
            "Saving\n",
            "Time mean 5.923289954662323\n",
            "Time std 0.14161414581782628\n",
            "Roc 0.8467132253348774\n",
            "test\n",
            "Crossvalidation run 7\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.022460 : \n",
            "Roc 0.7554078365707697\n",
            "Epoch 1 - loss is 0.894158 : \n",
            "Roc 0.6924350800839536\n",
            "Epoch 2 - loss is 0.903842 : \n",
            "Roc 0.7539962354649773\n",
            "Epoch 3 - loss is 0.904681 : \n",
            "Roc 0.7609622774162933\n",
            "Epoch 4 - loss is 0.908671 : \n",
            "Roc 0.7477088727733667\n",
            "Epoch 5 - loss is 0.907865 : \n",
            "Roc 0.7587118570113669\n",
            "Epoch 6 - loss is 0.904278 : \n",
            "Roc 0.7721364492663615\n",
            "Epoch 7 - loss is 0.905454 : \n",
            "Roc 0.7803893841386317\n",
            "Epoch 8 - loss is 0.903355 : \n",
            "Roc 0.7740158304748983\n",
            "Epoch 9 - loss is 0.866168 : \n",
            "Roc 0.8126200303838106\n",
            "Epoch 10 - loss is 0.849111 : \n",
            "Roc 0.8263423099339774\n",
            "Epoch 11 - loss is 0.832755 : \n",
            "Roc 0.8464041046808266\n",
            "Epoch 12 - loss is 0.810128 : \n",
            "Roc 0.8464329677084428\n",
            "Epoch 13 - loss is 0.809154 : \n",
            "Roc 0.839678820190902\n",
            "Epoch 14 - loss is 0.796566 : \n",
            "Roc 0.8426433517101244\n",
            "Epoch 15 - loss is 0.789770 : \n",
            "Roc 0.8487002082915317\n",
            "Saving\n",
            "Time mean 5.960559621453285\n",
            "Time std 0.3667757685442798\n",
            "Roc 0.8486406907380337\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.026960 : \n",
            "Roc 0.7366734607262971\n",
            "Epoch 1 - loss is 0.899711 : \n",
            "Roc 0.7460421025775975\n",
            "Epoch 2 - loss is 0.897733 : \n",
            "Roc 0.7595340138892456\n",
            "Epoch 3 - loss is 0.908849 : \n",
            "Roc 0.7393472929128905\n",
            "Epoch 4 - loss is 0.903379 : \n",
            "Roc 0.7599813908821339\n",
            "Epoch 5 - loss is 0.898914 : \n",
            "Roc 0.7649904888513774\n",
            "Epoch 6 - loss is 0.891938 : \n",
            "Roc 0.755381661414341\n",
            "Epoch 7 - loss is 0.902918 : \n",
            "Roc 0.7701716780377911\n",
            "Epoch 8 - loss is 0.901547 : \n",
            "Roc 0.7555400134142898\n",
            "Epoch 9 - loss is 0.865499 : \n",
            "Roc 0.7854294297643193\n",
            "Epoch 10 - loss is 0.849746 : \n",
            "Roc 0.7901867089420042\n",
            "Epoch 11 - loss is 0.837023 : \n",
            "Roc 0.7998755648121999\n",
            "Epoch 12 - loss is 0.824479 : \n",
            "Roc 0.8123077512451542\n",
            "Epoch 13 - loss is 0.813709 : \n",
            "Roc 0.81258795104747\n",
            "Epoch 14 - loss is 0.799566 : \n",
            "Roc 0.8224083824869708\n",
            "Epoch 15 - loss is 0.790790 : \n",
            "Roc 0.8250832175361352\n",
            "Saving\n",
            "Time mean 5.911462277173996\n",
            "Time std 0.12104467315963109\n",
            "Roc 0.82430670104747\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.032505 : \n",
            "Roc 0.7103216228422111\n",
            "Epoch 1 - loss is 0.895936 : \n",
            "Roc 0.7673622802642136\n",
            "Epoch 2 - loss is 0.902184 : \n",
            "Roc 0.7387359906130402\n",
            "Epoch 3 - loss is 0.906418 : \n",
            "Roc 0.7262106275208526\n",
            "Epoch 4 - loss is 0.894133 : \n",
            "Roc 0.7485909287542988\n",
            "Epoch 5 - loss is 0.898066 : \n",
            "Roc 0.715394241279719\n",
            "Epoch 6 - loss is 0.903875 : \n",
            "Roc 0.6789212301209097\n",
            "Epoch 7 - loss is 0.906088 : \n",
            "Roc 0.767619002506157\n",
            "Epoch 8 - loss is 0.899411 : \n",
            "Roc 0.7673539099206453\n",
            "Epoch 9 - loss is 0.847792 : \n",
            "Roc 0.8150867452946303\n",
            "Epoch 10 - loss is 0.813122 : \n",
            "Roc 0.824965557938567\n",
            "Epoch 11 - loss is 0.798629 : \n",
            "Roc 0.8338652306200864\n",
            "Epoch 12 - loss is 0.789118 : \n",
            "Roc 0.8333234411091162\n",
            "Epoch 13 - loss is 0.784670 : \n",
            "Roc 0.8275169908462683\n",
            "Epoch 14 - loss is 0.775327 : \n",
            "Roc 0.8344332725722483\n",
            "Epoch 15 - loss is 0.777317 : \n",
            "Roc 0.8389912099978428\n",
            "Saving\n",
            "Time mean 5.913479954004288\n",
            "Time std 0.10003194425803605\n",
            "Roc 0.8389445072854329\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.032191 : \n",
            "Roc 0.78716521656411\n",
            "Epoch 1 - loss is 0.893049 : \n",
            "Roc 0.750596511420393\n",
            "Epoch 2 - loss is 0.893805 : \n",
            "Roc 0.7365461615325409\n",
            "Epoch 3 - loss is 0.899477 : \n",
            "Roc 0.7522062078068898\n",
            "Epoch 4 - loss is 0.896502 : \n",
            "Roc 0.7632530686434289\n",
            "Epoch 5 - loss is 0.899306 : \n",
            "Roc 0.7627778249483679\n",
            "Epoch 6 - loss is 0.899456 : \n",
            "Roc 0.7652913294952197\n",
            "Epoch 7 - loss is 0.907378 : \n",
            "Roc 0.7561588084394475\n",
            "Epoch 8 - loss is 0.897073 : \n",
            "Roc 0.7557122601866855\n",
            "Epoch 9 - loss is 0.861744 : \n",
            "Roc 0.7972847822173165\n",
            "Epoch 10 - loss is 0.829753 : \n",
            "Roc 0.8127650888890461\n",
            "Epoch 11 - loss is 0.821072 : \n",
            "Roc 0.8133786013762805\n",
            "Epoch 12 - loss is 0.809247 : \n",
            "Roc 0.8224455749662534\n",
            "Epoch 13 - loss is 0.802886 : \n",
            "Roc 0.8275621099090473\n",
            "Epoch 14 - loss is 0.799628 : \n",
            "Roc 0.8307904454397438\n",
            "Epoch 15 - loss is 0.790981 : \n",
            "Roc 0.8334244315354262\n",
            "Saving\n",
            "Time mean 5.907165765762329\n",
            "Time std 0.1869713547846142\n",
            "Roc 0.8328676220283511\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.023657 : \n",
            "Roc 0.706926353388152\n",
            "Epoch 1 - loss is 0.899736 : \n",
            "Roc 0.7503535827471547\n",
            "Epoch 2 - loss is 0.891137 : \n",
            "Roc 0.7506305290348143\n",
            "Epoch 3 - loss is 0.906366 : \n",
            "Roc 0.742874665344358\n",
            "Epoch 4 - loss is 0.917211 : \n",
            "Roc 0.7649244742563039\n",
            "Epoch 5 - loss is 0.907726 : \n",
            "Roc 0.7650353504596528\n",
            "Epoch 6 - loss is 0.902364 : \n",
            "Roc 0.7393470436887408\n",
            "Epoch 7 - loss is 0.901918 : \n",
            "Roc 0.7301792912167876\n",
            "Epoch 8 - loss is 0.900790 : \n",
            "Roc 0.7708616751469867\n",
            "Epoch 9 - loss is 0.851747 : \n",
            "Roc 0.8087929615981491\n",
            "Epoch 10 - loss is 0.824059 : \n",
            "Roc 0.81343247131237\n",
            "Epoch 11 - loss is 0.814419 : \n",
            "Roc 0.8216123998475282\n",
            "Epoch 12 - loss is 0.805508 : \n",
            "Roc 0.8250956710218058\n",
            "Epoch 13 - loss is 0.798460 : \n",
            "Roc 0.8156926851598405\n",
            "Epoch 14 - loss is 0.804768 : \n",
            "Roc 0.8246140677773043\n",
            "Epoch 15 - loss is 0.795762 : \n",
            "Roc 0.8256369618083743\n",
            "Saving\n",
            "Time mean 5.897482484579086\n",
            "Time std 0.07283749154537675\n",
            "Roc 0.8254027053012112\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.036494 : \n",
            "Roc 0.7674123125429793\n",
            "Epoch 1 - loss is 0.903139 : \n",
            "Roc 0.7447913334469871\n",
            "Epoch 2 - loss is 0.901344 : \n",
            "Roc 0.7616103154277439\n",
            "Epoch 3 - loss is 0.910958 : \n",
            "Roc 0.7056395615004607\n",
            "Epoch 4 - loss is 0.902867 : \n",
            "Roc 0.7462880205649617\n",
            "Epoch 5 - loss is 0.913901 : \n",
            "Roc 0.7435864749364528\n",
            "Epoch 6 - loss is 0.909318 : \n",
            "Roc 0.7741583708933768\n",
            "Epoch 7 - loss is 0.902769 : \n",
            "Roc 0.7717390763006821\n",
            "Epoch 8 - loss is 0.899803 : \n",
            "Roc 0.7626783104397445\n",
            "Epoch 9 - loss is 0.869422 : \n",
            "Roc 0.7898123370615627\n",
            "Epoch 10 - loss is 0.844931 : \n",
            "Roc 0.7985471781600718\n",
            "Epoch 11 - loss is 0.829379 : \n",
            "Roc 0.7944849469306756\n",
            "Epoch 12 - loss is 0.825898 : \n",
            "Roc 0.7991123506617144\n",
            "Epoch 13 - loss is 0.826380 : \n",
            "Roc 0.811504483261278\n",
            "Epoch 14 - loss is 0.822192 : \n",
            "Roc 0.8185812551599168\n",
            "Epoch 15 - loss is 0.809010 : \n",
            "Roc 0.8156981187733813\n",
            "Saving\n",
            "Time mean 5.910214424133301\n",
            "Time std 0.11272411585077918\n",
            "Roc 0.8156924680061219\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.022861 : \n",
            "Roc 0.7542530405638164\n",
            "Epoch 1 - loss is 0.900166 : \n",
            "Roc 0.7454395595488497\n",
            "Epoch 2 - loss is 0.908368 : \n",
            "Roc 0.744287468540651\n",
            "Epoch 3 - loss is 0.902844 : \n",
            "Roc 0.7528398521562571\n",
            "Epoch 4 - loss is 0.902070 : \n",
            "Roc 0.744025560594455\n",
            "Epoch 5 - loss is 0.905352 : \n",
            "Roc 0.7548792893125347\n",
            "Epoch 6 - loss is 0.901214 : \n",
            "Roc 0.7250333337386067\n",
            "Epoch 7 - loss is 0.911236 : \n",
            "Roc 0.7597803620712713\n",
            "Epoch 8 - loss is 0.903311 : \n",
            "Roc 0.7647136540667161\n",
            "Epoch 9 - loss is 0.863511 : \n",
            "Roc 0.7977491114380317\n",
            "Epoch 10 - loss is 0.842805 : \n",
            "Roc 0.8139153667521793\n",
            "Epoch 11 - loss is 0.831344 : \n",
            "Roc 0.8028848375056232\n",
            "Epoch 12 - loss is 0.822855 : \n",
            "Roc 0.8261377038018699\n",
            "Epoch 13 - loss is 0.810433 : \n",
            "Roc 0.8249365747101283\n",
            "Epoch 14 - loss is 0.800781 : \n",
            "Roc 0.8204273000279638\n",
            "Epoch 15 - loss is 0.803788 : \n",
            "Roc 0.8273987120410785\n",
            "Saving\n",
            "Time mean 5.918395057320595\n",
            "Time std 0.12415128380878916\n",
            "Roc 0.8273755101379145\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.028608 : \n",
            "Roc 0.6610429447852761\n",
            "Epoch 1 - loss is 0.897602 : \n",
            "Roc 0.7657142111416945\n",
            "Epoch 2 - loss is 0.898938 : \n",
            "Roc 0.7225598376358447\n",
            "Epoch 3 - loss is 0.897167 : \n",
            "Roc 0.7767419274188896\n",
            "Epoch 4 - loss is 0.898637 : \n",
            "Roc 0.7635670111266231\n",
            "Epoch 5 - loss is 0.895205 : \n",
            "Roc 0.7652634394527471\n",
            "Epoch 6 - loss is 0.898524 : \n",
            "Roc 0.7588328290525396\n",
            "Epoch 7 - loss is 0.898180 : \n",
            "Roc 0.7597211259833279\n",
            "Epoch 8 - loss is 0.898379 : \n",
            "Roc 0.7670696661071106\n",
            "Epoch 9 - loss is 0.860465 : \n",
            "Roc 0.7946440591352798\n",
            "Epoch 10 - loss is 0.838752 : \n",
            "Roc 0.8064739213761547\n",
            "Epoch 11 - loss is 0.817617 : \n",
            "Roc 0.8175784670465682\n",
            "Epoch 12 - loss is 0.802911 : \n",
            "Roc 0.8300311085451162\n",
            "Epoch 13 - loss is 0.791431 : \n",
            "Roc 0.8397930532098987\n",
            "Epoch 14 - loss is 0.779491 : \n",
            "Roc 0.8319782381479623\n",
            "Epoch 15 - loss is 0.778783 : \n",
            "Roc 0.8391579106015928\n",
            "Saving\n",
            "Time mean 5.9251928478479385\n",
            "Time std 0.11846442087259915\n",
            "Roc 0.8396225174686295\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.028862 : \n",
            "Roc 0.7759101250312188\n",
            "Epoch 1 - loss is 0.901175 : \n",
            "Roc 0.7749975610327173\n",
            "Epoch 2 - loss is 0.897053 : \n",
            "Roc 0.7789543464348152\n",
            "Epoch 3 - loss is 0.899853 : \n",
            "Roc 0.7519392716658342\n",
            "Epoch 4 - loss is 0.900912 : \n",
            "Roc 0.7517651294018481\n",
            "Epoch 5 - loss is 0.902390 : \n",
            "Roc 0.757485093031968\n",
            "Epoch 6 - loss is 0.900061 : \n",
            "Roc 0.7884737332979521\n",
            "Epoch 7 - loss is 0.910587 : \n",
            "Roc 0.7545223331356145\n",
            "Epoch 8 - loss is 0.910920 : \n",
            "Roc 0.7587473073801198\n",
            "Epoch 9 - loss is 0.862357 : \n",
            "Roc 0.8057625967782217\n",
            "Epoch 10 - loss is 0.840977 : \n",
            "Roc 0.8170969655344655\n",
            "Epoch 11 - loss is 0.817406 : \n",
            "Roc 0.8321349548888612\n",
            "Epoch 12 - loss is 0.810225 : \n",
            "Roc 0.8377855542894606\n",
            "Epoch 13 - loss is 0.802858 : \n",
            "Roc 0.8345391912774726\n",
            "Epoch 14 - loss is 0.787773 : \n",
            "Roc 0.832678844592907\n",
            "Epoch 15 - loss is 0.786914 : \n",
            "Roc 0.8369189404345655\n",
            "Saving\n",
            "Time mean 5.9247481524944305\n",
            "Time std 0.0936786900127909\n",
            "Roc 0.8369177697302698\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.008865 : \n",
            "Roc 0.7113750837427307\n",
            "Epoch 1 - loss is 0.893681 : \n",
            "Roc 0.6388701174709092\n",
            "Epoch 2 - loss is 0.902409 : \n",
            "Roc 0.7345580278311081\n",
            "Epoch 3 - loss is 0.910287 : \n",
            "Roc 0.7513029307145227\n",
            "Epoch 4 - loss is 0.907455 : \n",
            "Roc 0.7330150589103307\n",
            "Epoch 5 - loss is 0.901882 : \n",
            "Roc 0.722057096199735\n",
            "Epoch 6 - loss is 0.902223 : \n",
            "Roc 0.7376303295039641\n",
            "Epoch 7 - loss is 0.915072 : \n",
            "Roc 0.7377144365597549\n",
            "Epoch 8 - loss is 0.901463 : \n",
            "Roc 0.7503317440430762\n",
            "Epoch 9 - loss is 0.867613 : \n",
            "Roc 0.7850200753549256\n",
            "Epoch 10 - loss is 0.847051 : \n",
            "Roc 0.7821403455301056\n",
            "Epoch 11 - loss is 0.834242 : \n",
            "Roc 0.8016578666536758\n",
            "Epoch 12 - loss is 0.819270 : \n",
            "Roc 0.8142995318734514\n",
            "Epoch 13 - loss is 0.821446 : \n",
            "Roc 0.813733058361553\n",
            "Epoch 14 - loss is 0.808285 : \n",
            "Roc 0.8320516375685398\n",
            "Epoch 15 - loss is 0.797795 : \n",
            "Roc 0.8204422615471285\n",
            "Saving\n",
            "Time mean 5.913237169384956\n",
            "Time std 0.076120689572584\n",
            "Roc 0.820742569537174\n",
            "test\n",
            "Crossvalidation run 8\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.033244 : \n",
            "Roc 0.7265789469493572\n",
            "Epoch 1 - loss is 0.898213 : \n",
            "Roc 0.7724963413624305\n",
            "Epoch 2 - loss is 0.909196 : \n",
            "Roc 0.7794120223069402\n",
            "Epoch 3 - loss is 0.902108 : \n",
            "Roc 0.7626308589955507\n",
            "Epoch 4 - loss is 0.906907 : \n",
            "Roc 0.5859408590592485\n",
            "Epoch 5 - loss is 0.907983 : \n",
            "Roc 0.7712340317787905\n",
            "Epoch 6 - loss is 0.912138 : \n",
            "Roc 0.6947700990180996\n",
            "Epoch 7 - loss is 0.913839 : \n",
            "Roc 0.7953500866288941\n",
            "Epoch 8 - loss is 0.909003 : \n",
            "Roc 0.7785710133988146\n",
            "Epoch 9 - loss is 0.870272 : \n",
            "Roc 0.806434484510308\n",
            "Epoch 10 - loss is 0.843199 : \n",
            "Roc 0.8210574895774612\n",
            "Epoch 11 - loss is 0.832793 : \n",
            "Roc 0.844119745336531\n",
            "Epoch 12 - loss is 0.820990 : \n",
            "Roc 0.8430133956296998\n",
            "Epoch 13 - loss is 0.808137 : \n",
            "Roc 0.8476512860568886\n",
            "Epoch 14 - loss is 0.807279 : \n",
            "Roc 0.8486436765684766\n",
            "Epoch 15 - loss is 0.803151 : \n",
            "Roc 0.8499107634808254\n",
            "Saving\n",
            "Time mean 5.912389129400253\n",
            "Time std 0.10677934181995571\n",
            "Roc 0.8500061109996401\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.022762 : \n",
            "Roc 0.7684130582334215\n",
            "Epoch 1 - loss is 0.891540 : \n",
            "Roc 0.7548144423442275\n",
            "Epoch 2 - loss is 0.894880 : \n",
            "Roc 0.7613720604092321\n",
            "Epoch 3 - loss is 0.899813 : \n",
            "Roc 0.7430880705886883\n",
            "Epoch 4 - loss is 0.915757 : \n",
            "Roc 0.733181292681831\n",
            "Epoch 5 - loss is 0.906026 : \n",
            "Roc 0.7592029689545841\n",
            "Epoch 6 - loss is 0.906441 : \n",
            "Roc 0.7523621424995507\n",
            "Epoch 7 - loss is 0.906779 : \n",
            "Roc 0.7643681123398629\n",
            "Epoch 8 - loss is 0.900250 : \n",
            "Roc 0.7675719573951888\n",
            "Epoch 9 - loss is 0.863910 : \n",
            "Roc 0.7934800093386564\n",
            "Epoch 10 - loss is 0.846744 : \n",
            "Roc 0.7902477832725733\n",
            "Epoch 11 - loss is 0.834581 : \n",
            "Roc 0.8032901112936767\n",
            "Epoch 12 - loss is 0.824367 : \n",
            "Roc 0.8134286507406743\n",
            "Epoch 13 - loss is 0.810508 : \n",
            "Roc 0.8166506476085338\n",
            "Epoch 14 - loss is 0.796594 : \n",
            "Roc 0.8238502982914431\n",
            "Epoch 15 - loss is 0.799112 : \n",
            "Roc 0.8184900981361197\n",
            "Saving\n",
            "Time mean 5.896693095564842\n",
            "Time std 0.07982418082953101\n",
            "Roc 0.8184902987086339\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.027047 : \n",
            "Roc 0.7436111450363673\n",
            "Epoch 1 - loss is 0.895507 : \n",
            "Roc 0.7361909305805556\n",
            "Epoch 2 - loss is 0.910600 : \n",
            "Roc 0.737267375786955\n",
            "Epoch 3 - loss is 0.901671 : \n",
            "Roc 0.7524137027089857\n",
            "Epoch 4 - loss is 0.901601 : \n",
            "Roc 0.7625951793669509\n",
            "Epoch 5 - loss is 0.903162 : \n",
            "Roc 0.7576174882349115\n",
            "Epoch 6 - loss is 0.903035 : \n",
            "Roc 0.7480167041620012\n",
            "Epoch 7 - loss is 0.899225 : \n",
            "Roc 0.7602723139091903\n",
            "Epoch 8 - loss is 0.887963 : \n",
            "Roc 0.766677814442419\n",
            "Epoch 9 - loss is 0.849425 : \n",
            "Roc 0.8005827471239311\n",
            "Epoch 10 - loss is 0.821105 : \n",
            "Roc 0.8161456888735685\n",
            "Epoch 11 - loss is 0.818758 : \n",
            "Roc 0.8200453178010194\n",
            "Epoch 12 - loss is 0.813690 : \n",
            "Roc 0.8180371962849372\n",
            "Epoch 13 - loss is 0.815953 : \n",
            "Roc 0.8193913847097412\n",
            "Epoch 14 - loss is 0.809071 : \n",
            "Roc 0.8221260140005411\n",
            "Epoch 15 - loss is 0.801866 : \n",
            "Roc 0.8221802309986543\n",
            "Saving\n",
            "Time mean 5.919786095619202\n",
            "Time std 0.12939751920418552\n",
            "Roc 0.8220850183405641\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.038836 : \n",
            "Roc 0.7354130846499823\n",
            "Epoch 1 - loss is 0.891211 : \n",
            "Roc 0.7179746088575361\n",
            "Epoch 2 - loss is 0.901192 : \n",
            "Roc 0.7423237726409007\n",
            "Epoch 3 - loss is 0.912437 : \n",
            "Roc 0.7621917303666177\n",
            "Epoch 4 - loss is 0.897021 : \n",
            "Roc 0.7687683916165283\n",
            "Epoch 5 - loss is 0.900041 : \n",
            "Roc 0.734804584037944\n",
            "Epoch 6 - loss is 0.894608 : \n",
            "Roc 0.764852937823659\n",
            "Epoch 7 - loss is 0.896850 : \n",
            "Roc 0.7550089270341727\n",
            "Epoch 8 - loss is 0.899400 : \n",
            "Roc 0.7588122524035381\n",
            "Epoch 9 - loss is 0.857164 : \n",
            "Roc 0.7894725463234847\n",
            "Epoch 10 - loss is 0.830307 : \n",
            "Roc 0.7972426235024321\n",
            "Epoch 11 - loss is 0.822101 : \n",
            "Roc 0.8156486860239045\n",
            "Epoch 12 - loss is 0.812208 : \n",
            "Roc 0.8163129068954755\n",
            "Epoch 13 - loss is 0.808690 : \n",
            "Roc 0.8209828935855072\n",
            "Epoch 14 - loss is 0.801059 : \n",
            "Roc 0.8189094513352815\n",
            "Epoch 15 - loss is 0.800588 : \n",
            "Roc 0.8199441578831097\n",
            "Saving\n",
            "Time mean 5.879221424460411\n",
            "Time std 0.057396566973010765\n",
            "Roc 0.8201113186617075\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.032697 : \n",
            "Roc 0.7503925603728253\n",
            "Epoch 1 - loss is 0.901788 : \n",
            "Roc 0.7431193744980045\n",
            "Epoch 2 - loss is 0.897325 : \n",
            "Roc 0.7606173977755596\n",
            "Epoch 3 - loss is 0.909784 : \n",
            "Roc 0.7245751096892907\n",
            "Epoch 4 - loss is 0.920768 : \n",
            "Roc 0.7392797364604526\n",
            "Epoch 5 - loss is 0.897024 : \n",
            "Roc 0.7453111088577681\n",
            "Epoch 6 - loss is 0.910593 : \n",
            "Roc 0.7518094312568516\n",
            "Epoch 7 - loss is 0.900856 : \n",
            "Roc 0.7511781695599006\n",
            "Epoch 8 - loss is 0.904536 : \n",
            "Roc 0.7369232996523665\n",
            "Epoch 9 - loss is 0.874398 : \n",
            "Roc 0.7807970621615956\n",
            "Epoch 10 - loss is 0.847627 : \n",
            "Roc 0.767664532959148\n",
            "Epoch 11 - loss is 0.856685 : \n",
            "Roc 0.7959742071667644\n",
            "Epoch 12 - loss is 0.833378 : \n",
            "Roc 0.809207452990932\n",
            "Epoch 13 - loss is 0.821293 : \n",
            "Roc 0.8015491798970795\n",
            "Epoch 14 - loss is 0.818996 : \n",
            "Roc 0.8203430382736839\n",
            "Epoch 15 - loss is 0.808198 : \n",
            "Roc 0.8164237842839477\n",
            "Saving\n",
            "Time mean 5.9278048276901245\n",
            "Time std 0.1668994477929115\n",
            "Roc 0.8164265195559246\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.035873 : \n",
            "Roc 0.7495389357019305\n",
            "Epoch 1 - loss is 0.896430 : \n",
            "Roc 0.7733892918918195\n",
            "Epoch 2 - loss is 0.897578 : \n",
            "Roc 0.775012211403823\n",
            "Epoch 3 - loss is 0.904937 : \n",
            "Roc 0.7503244881266845\n",
            "Epoch 4 - loss is 0.903858 : \n",
            "Roc 0.7493716155256267\n",
            "Epoch 5 - loss is 0.900125 : \n",
            "Roc 0.7297580896958739\n",
            "Epoch 6 - loss is 0.908069 : \n",
            "Roc 0.770314412521334\n",
            "Epoch 7 - loss is 0.900714 : \n",
            "Roc 0.750724926396362\n",
            "Epoch 8 - loss is 0.895404 : \n",
            "Roc 0.7643619517558562\n",
            "Epoch 9 - loss is 0.850272 : \n",
            "Roc 0.8005174761949457\n",
            "Epoch 10 - loss is 0.828812 : \n",
            "Roc 0.8138248415390774\n",
            "Epoch 11 - loss is 0.816210 : \n",
            "Roc 0.8275707638879577\n",
            "Epoch 12 - loss is 0.800606 : \n",
            "Roc 0.8246333226703035\n",
            "Epoch 13 - loss is 0.796155 : \n",
            "Roc 0.819797894083551\n",
            "Epoch 14 - loss is 0.793261 : \n",
            "Roc 0.8391256251759878\n",
            "Epoch 15 - loss is 0.783852 : \n",
            "Roc 0.8368326587530385\n",
            "Saving\n",
            "Time mean 5.897989377379417\n",
            "Time std 0.06379965454603952\n",
            "Roc 0.8367813229691239\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.033106 : \n",
            "Roc 0.7375634759490489\n",
            "Epoch 1 - loss is 0.884423 : \n",
            "Roc 0.7769810777841272\n",
            "Epoch 2 - loss is 0.893977 : \n",
            "Roc 0.7496290735044397\n",
            "Epoch 3 - loss is 0.897006 : \n",
            "Roc 0.75822502401245\n",
            "Epoch 4 - loss is 0.895580 : \n",
            "Roc 0.76113610297187\n",
            "Epoch 5 - loss is 0.894168 : \n",
            "Roc 0.7668834879451423\n",
            "Epoch 6 - loss is 0.901299 : \n",
            "Roc 0.7585504585668722\n",
            "Epoch 7 - loss is 0.897228 : \n",
            "Roc 0.7562131454485768\n",
            "Epoch 8 - loss is 0.898713 : \n",
            "Roc 0.7640573137667327\n",
            "Epoch 9 - loss is 0.861571 : \n",
            "Roc 0.7839558333029377\n",
            "Epoch 10 - loss is 0.847732 : \n",
            "Roc 0.7959712782728868\n",
            "Epoch 11 - loss is 0.831835 : \n",
            "Roc 0.8101426765067052\n",
            "Epoch 12 - loss is 0.815350 : \n",
            "Roc 0.815597960664162\n",
            "Epoch 13 - loss is 0.806456 : \n",
            "Roc 0.8230052442380252\n",
            "Epoch 14 - loss is 0.808051 : \n",
            "Roc 0.8302865890973345\n",
            "Epoch 15 - loss is 0.797988 : \n",
            "Roc 0.8285410764872522\n",
            "Saving\n",
            "Time mean 5.921131506562233\n",
            "Time std 0.15342115954841784\n",
            "Roc 0.8272245457898171\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.026520 : \n",
            "Roc 0.7599225701011794\n",
            "Epoch 1 - loss is 0.894194 : \n",
            "Roc 0.7492254262706679\n",
            "Epoch 2 - loss is 0.894616 : \n",
            "Roc 0.7124299361069889\n",
            "Epoch 3 - loss is 0.904564 : \n",
            "Roc 0.7655843959600495\n",
            "Epoch 4 - loss is 0.902739 : \n",
            "Roc 0.6848098183058442\n",
            "Epoch 5 - loss is 0.906685 : \n",
            "Roc 0.7559636486092408\n",
            "Epoch 6 - loss is 0.909401 : \n",
            "Roc 0.7609689413007736\n",
            "Epoch 7 - loss is 0.892442 : \n",
            "Roc 0.77598197384229\n",
            "Epoch 8 - loss is 0.892378 : \n",
            "Roc 0.7414627139006842\n",
            "Epoch 9 - loss is 0.854274 : \n",
            "Roc 0.8138885956136401\n",
            "Epoch 10 - loss is 0.815742 : \n",
            "Roc 0.8213093534046116\n",
            "Epoch 11 - loss is 0.805627 : \n",
            "Roc 0.8339675601378886\n",
            "Epoch 12 - loss is 0.800636 : \n",
            "Roc 0.8342152196378676\n",
            "Epoch 13 - loss is 0.787363 : \n",
            "Roc 0.8434427928141852\n",
            "Epoch 14 - loss is 0.779308 : \n",
            "Roc 0.8335900854562646\n",
            "Epoch 15 - loss is 0.778383 : \n",
            "Roc 0.8391817542063847\n",
            "Saving\n",
            "Time mean 5.904953271150589\n",
            "Time std 0.1484893218804281\n",
            "Roc 0.8390432061818324\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.038432 : \n",
            "Roc 0.7120325963099401\n",
            "Epoch 1 - loss is 0.893066 : \n",
            "Roc 0.789526781811938\n",
            "Epoch 2 - loss is 0.896977 : \n",
            "Roc 0.727092146135115\n",
            "Epoch 3 - loss is 0.902704 : \n",
            "Roc 0.7297321818806194\n",
            "Epoch 4 - loss is 0.907185 : \n",
            "Roc 0.7574370941558443\n",
            "Epoch 5 - loss is 0.905245 : \n",
            "Roc 0.7746294720904096\n",
            "Epoch 6 - loss is 0.915281 : \n",
            "Roc 0.7607708697552448\n",
            "Epoch 7 - loss is 0.921790 : \n",
            "Roc 0.7491571904657842\n",
            "Epoch 8 - loss is 0.911990 : \n",
            "Roc 0.7450902613011988\n",
            "Epoch 9 - loss is 0.882719 : \n",
            "Roc 0.7784708455606894\n",
            "Epoch 10 - loss is 0.860954 : \n",
            "Roc 0.7876063389735264\n",
            "Epoch 11 - loss is 0.853920 : \n",
            "Roc 0.8016506930569431\n",
            "Epoch 12 - loss is 0.846834 : \n",
            "Roc 0.8042825338723776\n",
            "Epoch 13 - loss is 0.832741 : \n",
            "Roc 0.814544147258991\n",
            "Epoch 14 - loss is 0.828644 : \n",
            "Roc 0.8142263010427073\n",
            "Epoch 15 - loss is 0.827967 : \n",
            "Roc 0.814531464629121\n",
            "Saving\n",
            "Time mean 5.911435097455978\n",
            "Time std 0.07697736022856679\n",
            "Roc 0.814531464629121\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.021438 : \n",
            "Roc 0.6993435069561116\n",
            "Epoch 1 - loss is 0.897468 : \n",
            "Roc 0.7387865933353069\n",
            "Epoch 2 - loss is 0.898506 : \n",
            "Roc 0.737576409386514\n",
            "Epoch 3 - loss is 0.906610 : \n",
            "Roc 0.7328217584120588\n",
            "Epoch 4 - loss is 0.909876 : \n",
            "Roc 0.7566861986567021\n",
            "Epoch 5 - loss is 0.904168 : \n",
            "Roc 0.7585419667144245\n",
            "Epoch 6 - loss is 0.900115 : \n",
            "Roc 0.7524418110330969\n",
            "Epoch 7 - loss is 0.894979 : \n",
            "Roc 0.7492061876145283\n",
            "Epoch 8 - loss is 0.898029 : \n",
            "Roc 0.7529498884332396\n",
            "Epoch 9 - loss is 0.866796 : \n",
            "Roc 0.7823495722406744\n",
            "Epoch 10 - loss is 0.841488 : \n",
            "Roc 0.8147490049760566\n",
            "Epoch 11 - loss is 0.817741 : \n",
            "Roc 0.8171552162404896\n",
            "Epoch 12 - loss is 0.805198 : \n",
            "Roc 0.8230754994064623\n",
            "Epoch 13 - loss is 0.804499 : \n",
            "Roc 0.8309023479608828\n",
            "Epoch 14 - loss is 0.801531 : \n",
            "Roc 0.8351342444375882\n",
            "Epoch 15 - loss is 0.793342 : \n",
            "Roc 0.8291866889342179\n",
            "Saving\n",
            "Time mean 5.905711427330971\n",
            "Time std 0.11860772788323863\n",
            "Roc 0.8286108470621447\n",
            "test\n",
            "Crossvalidation run 9\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.047840 : \n",
            "Roc 0.7694904063277309\n",
            "Epoch 1 - loss is 0.897806 : \n",
            "Roc 0.7552701897873452\n",
            "Epoch 2 - loss is 0.901009 : \n",
            "Roc 0.7628298148307393\n",
            "Epoch 3 - loss is 0.904209 : \n",
            "Roc 0.7788160505505075\n",
            "Epoch 4 - loss is 0.918630 : \n",
            "Roc 0.7014488045531126\n",
            "Epoch 5 - loss is 0.905214 : \n",
            "Roc 0.7641513433848328\n",
            "Epoch 6 - loss is 0.902387 : \n",
            "Roc 0.7818029757980527\n",
            "Epoch 7 - loss is 0.902687 : \n",
            "Roc 0.7663181605373539\n",
            "Epoch 8 - loss is 0.902449 : \n",
            "Roc 0.7591053894637607\n",
            "Epoch 9 - loss is 0.872568 : \n",
            "Roc 0.8076709168967746\n",
            "Epoch 10 - loss is 0.839493 : \n",
            "Roc 0.8192648965071359\n",
            "Epoch 11 - loss is 0.817285 : \n",
            "Roc 0.8350805417809245\n",
            "Epoch 12 - loss is 0.808742 : \n",
            "Roc 0.8486137187363647\n",
            "Epoch 13 - loss is 0.798693 : \n",
            "Roc 0.8414930107680989\n",
            "Epoch 14 - loss is 0.799640 : \n",
            "Roc 0.8561291534892015\n",
            "Epoch 15 - loss is 0.797834 : \n",
            "Roc 0.8476810448336375\n",
            "Saving\n",
            "Time mean 5.888948678970337\n",
            "Time std 0.08702278050004619\n",
            "Roc 0.8479378262517397\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.029660 : \n",
            "Roc 0.7751535984313624\n",
            "Epoch 1 - loss is 0.892066 : \n",
            "Roc 0.7701750877705322\n",
            "Epoch 2 - loss is 0.898647 : \n",
            "Roc 0.7547472505519754\n",
            "Epoch 3 - loss is 0.897031 : \n",
            "Roc 0.7578892189866756\n",
            "Epoch 4 - loss is 0.899912 : \n",
            "Roc 0.7805079819837746\n",
            "Epoch 5 - loss is 0.896216 : \n",
            "Roc 0.7697678252804806\n",
            "Epoch 6 - loss is 0.898537 : \n",
            "Roc 0.7677512692228697\n",
            "Epoch 7 - loss is 0.898134 : \n",
            "Roc 0.7622108546635517\n",
            "Epoch 8 - loss is 0.896905 : \n",
            "Roc 0.7142435367513029\n",
            "Epoch 9 - loss is 0.854224 : \n",
            "Roc 0.8070987827655259\n",
            "Epoch 10 - loss is 0.829798 : \n",
            "Roc 0.8124071349259326\n",
            "Epoch 11 - loss is 0.808457 : \n",
            "Roc 0.8170827810903443\n",
            "Epoch 12 - loss is 0.805004 : \n",
            "Roc 0.8234746259723756\n",
            "Epoch 13 - loss is 0.794487 : \n",
            "Roc 0.830200524376781\n",
            "Epoch 14 - loss is 0.790691 : \n",
            "Roc 0.832775975745167\n",
            "Epoch 15 - loss is 0.769687 : \n",
            "Roc 0.8326838126748992\n",
            "Saving\n",
            "Time mean 5.921318560838699\n",
            "Time std 0.13810759426847327\n",
            "Roc 0.8328082679199507\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.013151 : \n",
            "Roc 0.6680148626864162\n",
            "Epoch 1 - loss is 0.895192 : \n",
            "Roc 0.7464621030792593\n",
            "Epoch 2 - loss is 0.897291 : \n",
            "Roc 0.7624157876854745\n",
            "Epoch 3 - loss is 0.898147 : \n",
            "Roc 0.7427473826506368\n",
            "Epoch 4 - loss is 0.898626 : \n",
            "Roc 0.7703184383069533\n",
            "Epoch 5 - loss is 0.898574 : \n",
            "Roc 0.7607028158977175\n",
            "Epoch 6 - loss is 0.890410 : \n",
            "Roc 0.7406247396157327\n",
            "Epoch 7 - loss is 0.900984 : \n",
            "Roc 0.7523501641918984\n",
            "Epoch 8 - loss is 0.896497 : \n",
            "Roc 0.7601701576706399\n",
            "Epoch 9 - loss is 0.862336 : \n",
            "Roc 0.7866570635997727\n",
            "Epoch 10 - loss is 0.845288 : \n",
            "Roc 0.7903086259814702\n",
            "Epoch 11 - loss is 0.828044 : \n",
            "Roc 0.8090525836397069\n",
            "Epoch 12 - loss is 0.815016 : \n",
            "Roc 0.8148780574106648\n",
            "Epoch 13 - loss is 0.804824 : \n",
            "Roc 0.8159059926713835\n",
            "Epoch 14 - loss is 0.792776 : \n",
            "Roc 0.8316032403882773\n",
            "Epoch 15 - loss is 0.789770 : \n",
            "Roc 0.8227263959164898\n",
            "Saving\n",
            "Time mean 5.926906779408455\n",
            "Time std 0.07992231751470046\n",
            "Roc 0.8213367286490608\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.018732 : \n",
            "Roc 0.7319163681161402\n",
            "Epoch 1 - loss is 0.887934 : \n",
            "Roc 0.7458404384034643\n",
            "Epoch 2 - loss is 0.897897 : \n",
            "Roc 0.7533798710513302\n",
            "Epoch 3 - loss is 0.895898 : \n",
            "Roc 0.768329606856717\n",
            "Epoch 4 - loss is 0.898910 : \n",
            "Roc 0.7389077374705085\n",
            "Epoch 5 - loss is 0.897477 : \n",
            "Roc 0.7757686054565366\n",
            "Epoch 6 - loss is 0.894897 : \n",
            "Roc 0.7621900597415524\n",
            "Epoch 7 - loss is 0.896850 : \n",
            "Roc 0.7463885016969619\n",
            "Epoch 8 - loss is 0.895286 : \n",
            "Roc 0.7204942062722733\n",
            "Epoch 9 - loss is 0.844451 : \n",
            "Roc 0.8058764138401656\n",
            "Epoch 10 - loss is 0.817308 : \n",
            "Roc 0.8222333073108912\n",
            "Epoch 11 - loss is 0.800619 : \n",
            "Roc 0.8247902481094421\n",
            "Epoch 12 - loss is 0.798235 : \n",
            "Roc 0.8206215472110782\n",
            "Epoch 13 - loss is 0.793314 : \n",
            "Roc 0.8246922708629623\n",
            "Epoch 14 - loss is 0.784699 : \n",
            "Roc 0.8288459361357381\n",
            "Epoch 15 - loss is 0.788901 : \n",
            "Roc 0.8243826155934965\n",
            "Saving\n",
            "Time mean 5.948053866624832\n",
            "Time std 0.14059007916570573\n",
            "Roc 0.8243816328728698\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.020985 : \n",
            "Roc 0.6559141171489634\n",
            "Epoch 1 - loss is 0.889750 : \n",
            "Roc 0.7547814996363065\n",
            "Epoch 2 - loss is 0.900393 : \n",
            "Roc 0.7650440447170077\n",
            "Epoch 3 - loss is 0.896065 : \n",
            "Roc 0.7507290965142671\n",
            "Epoch 4 - loss is 0.896369 : \n",
            "Roc 0.7540337935037682\n",
            "Epoch 5 - loss is 0.907261 : \n",
            "Roc 0.7221891710191682\n",
            "Epoch 6 - loss is 0.906526 : \n",
            "Roc 0.7443897129546975\n",
            "Epoch 7 - loss is 0.911563 : \n",
            "Roc 0.7245832178169365\n",
            "Epoch 8 - loss is 0.906521 : \n",
            "Roc 0.7569392384729289\n",
            "Epoch 9 - loss is 0.863339 : \n",
            "Roc 0.7883295127445114\n",
            "Epoch 10 - loss is 0.826641 : \n",
            "Roc 0.8050567754542945\n",
            "Epoch 11 - loss is 0.805836 : \n",
            "Roc 0.8217671380907918\n",
            "Epoch 12 - loss is 0.799678 : \n",
            "Roc 0.8227592603121218\n",
            "Epoch 13 - loss is 0.787838 : \n",
            "Roc 0.8354958588959153\n",
            "Epoch 14 - loss is 0.785462 : \n",
            "Roc 0.8316477219775782\n",
            "Epoch 15 - loss is 0.781639 : \n",
            "Roc 0.8311840933774962\n",
            "Saving\n",
            "Time mean 5.912220820784569\n",
            "Time std 0.07436141816824185\n",
            "Roc 0.8290037885470645\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.026223 : \n",
            "Roc 0.7626063828768344\n",
            "Epoch 1 - loss is 0.898662 : \n",
            "Roc 0.7635516891963076\n",
            "Epoch 2 - loss is 0.904730 : \n",
            "Roc 0.7213005192959334\n",
            "Epoch 3 - loss is 0.912817 : \n",
            "Roc 0.7714777043710122\n",
            "Epoch 4 - loss is 0.910319 : \n",
            "Roc 0.7689429042645095\n",
            "Epoch 5 - loss is 0.903720 : \n",
            "Roc 0.7864941873717802\n",
            "Epoch 6 - loss is 0.899810 : \n",
            "Roc 0.758668851630007\n",
            "Epoch 7 - loss is 0.902261 : \n",
            "Roc 0.773042583798963\n",
            "Epoch 8 - loss is 0.896923 : \n",
            "Roc 0.7644893334584802\n",
            "Epoch 9 - loss is 0.846396 : \n",
            "Roc 0.81747399210425\n",
            "Epoch 10 - loss is 0.821904 : \n",
            "Roc 0.8268039835035906\n",
            "Epoch 11 - loss is 0.800460 : \n",
            "Roc 0.82957898910647\n",
            "Epoch 12 - loss is 0.797096 : \n",
            "Roc 0.8379079327194748\n",
            "Epoch 13 - loss is 0.789916 : \n",
            "Roc 0.8336087523680547\n",
            "Epoch 14 - loss is 0.789276 : \n",
            "Roc 0.8361828205182233\n",
            "Epoch 15 - loss is 0.787691 : \n",
            "Roc 0.8368370644359864\n",
            "Saving\n",
            "Time mean 5.916812017560005\n",
            "Time std 0.1449779693255463\n",
            "Roc 0.8367903258864521\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.027165 : \n",
            "Roc 0.718097889741314\n",
            "Epoch 1 - loss is 0.896146 : \n",
            "Roc 0.7418141861907136\n",
            "Epoch 2 - loss is 0.890266 : \n",
            "Roc 0.7508918041556736\n",
            "Epoch 3 - loss is 0.902000 : \n",
            "Roc 0.7529765306163803\n",
            "Epoch 4 - loss is 0.898562 : \n",
            "Roc 0.7533506992992822\n",
            "Epoch 5 - loss is 0.904802 : \n",
            "Roc 0.7797640903435503\n",
            "Epoch 6 - loss is 0.901082 : \n",
            "Roc 0.7554539670188493\n",
            "Epoch 7 - loss is 0.893365 : \n",
            "Roc 0.7600818854940485\n",
            "Epoch 8 - loss is 0.901228 : \n",
            "Roc 0.7528055052341063\n",
            "Epoch 9 - loss is 0.858059 : \n",
            "Roc 0.8024012449999391\n",
            "Epoch 10 - loss is 0.843709 : \n",
            "Roc 0.8078396089922066\n",
            "Epoch 11 - loss is 0.825006 : \n",
            "Roc 0.8177326370736017\n",
            "Epoch 12 - loss is 0.819055 : \n",
            "Roc 0.8297025901024125\n",
            "Epoch 13 - loss is 0.810277 : \n",
            "Roc 0.8350730707972133\n",
            "Epoch 14 - loss is 0.790487 : \n",
            "Roc 0.8325412264384168\n",
            "Epoch 15 - loss is 0.788900 : \n",
            "Roc 0.8358200910244097\n",
            "Saving\n",
            "Time mean 5.9441667050123215\n",
            "Time std 0.1676044869773122\n",
            "Roc 0.8359675092301021\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.021185 : \n",
            "Roc 0.7428151270893572\n",
            "Epoch 1 - loss is 0.900400 : \n",
            "Roc 0.6990875258501963\n",
            "Epoch 2 - loss is 0.904690 : \n",
            "Roc 0.7548409386255801\n",
            "Epoch 3 - loss is 0.912643 : \n",
            "Roc 0.7481967170005491\n",
            "Epoch 4 - loss is 0.898362 : \n",
            "Roc 0.7534652705630978\n",
            "Epoch 5 - loss is 0.915397 : \n",
            "Roc 0.760206632800244\n",
            "Epoch 6 - loss is 0.897526 : \n",
            "Roc 0.7484104263472029\n",
            "Epoch 7 - loss is 0.916010 : \n",
            "Roc 0.7498058168975997\n",
            "Epoch 8 - loss is 0.911347 : \n",
            "Roc 0.7667680396004007\n",
            "Epoch 9 - loss is 0.862117 : \n",
            "Roc 0.7916160194485318\n",
            "Epoch 10 - loss is 0.845918 : \n",
            "Roc 0.8000074965078441\n",
            "Epoch 11 - loss is 0.835964 : \n",
            "Roc 0.8149786310277712\n",
            "Epoch 12 - loss is 0.823051 : \n",
            "Roc 0.819362321923596\n",
            "Epoch 13 - loss is 0.813850 : \n",
            "Roc 0.8206199494240836\n",
            "Epoch 14 - loss is 0.807939 : \n",
            "Roc 0.8293665293876844\n",
            "Epoch 15 - loss is 0.793923 : \n",
            "Roc 0.8304600971877106\n",
            "Saving\n",
            "Time mean 5.901072919368744\n",
            "Time std 0.10827492063418778\n",
            "Roc 0.8304035790133889\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.026334 : \n",
            "Roc 0.7696128090659341\n",
            "Epoch 1 - loss is 0.896779 : \n",
            "Roc 0.7356558480581917\n",
            "Epoch 2 - loss is 0.906753 : \n",
            "Roc 0.7696142724463038\n",
            "Epoch 3 - loss is 0.906056 : \n",
            "Roc 0.7610675457355145\n",
            "Epoch 4 - loss is 0.900775 : \n",
            "Roc 0.7722364549512986\n",
            "Epoch 5 - loss is 0.922729 : \n",
            "Roc 0.7852542964847653\n",
            "Epoch 6 - loss is 0.918978 : \n",
            "Roc 0.7655530992445054\n",
            "Epoch 7 - loss is 0.900886 : \n",
            "Roc 0.7721950900661838\n",
            "Epoch 8 - loss is 0.897186 : \n",
            "Roc 0.7622524936001498\n",
            "Epoch 9 - loss is 0.869351 : \n",
            "Roc 0.7991524100899101\n",
            "Epoch 10 - loss is 0.843700 : \n",
            "Roc 0.8147381914960041\n",
            "Epoch 11 - loss is 0.829902 : \n",
            "Roc 0.8117287985452047\n",
            "Epoch 12 - loss is 0.823976 : \n",
            "Roc 0.8210500437062938\n",
            "Epoch 13 - loss is 0.814232 : \n",
            "Roc 0.8244067455981519\n",
            "Epoch 14 - loss is 0.814086 : \n",
            "Roc 0.8219817292082917\n",
            "Epoch 15 - loss is 0.813125 : \n",
            "Roc 0.8320749562937062\n",
            "Saving\n",
            "Time mean 5.880382388830185\n",
            "Time std 0.15385033407842885\n",
            "Roc 0.8320749562937062\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.028334 : \n",
            "Roc 0.7323795718118118\n",
            "Epoch 1 - loss is 0.893179 : \n",
            "Roc 0.7574387902574051\n",
            "Epoch 2 - loss is 0.905537 : \n",
            "Roc 0.7463846457156824\n",
            "Epoch 3 - loss is 0.900790 : \n",
            "Roc 0.7545794626391877\n",
            "Epoch 4 - loss is 0.898348 : \n",
            "Roc 0.7309074901496898\n",
            "Epoch 5 - loss is 0.896700 : \n",
            "Roc 0.7549381667409891\n",
            "Epoch 6 - loss is 0.897134 : \n",
            "Roc 0.7556919033634912\n",
            "Epoch 7 - loss is 0.898990 : \n",
            "Roc 0.7615067404310527\n",
            "Epoch 8 - loss is 0.896168 : \n",
            "Roc 0.7441996318442637\n",
            "Epoch 9 - loss is 0.854149 : \n",
            "Roc 0.8039932947523443\n",
            "Epoch 10 - loss is 0.819986 : \n",
            "Roc 0.8159940600432776\n",
            "Epoch 11 - loss is 0.807001 : \n",
            "Roc 0.8342026130646572\n",
            "Epoch 12 - loss is 0.789473 : \n",
            "Roc 0.8413929335916503\n",
            "Epoch 13 - loss is 0.784967 : \n",
            "Roc 0.8317769364296388\n",
            "Epoch 14 - loss is 0.776366 : \n",
            "Roc 0.8418848765937142\n",
            "Epoch 15 - loss is 0.774306 : \n",
            "Roc 0.8408458213574461\n",
            "Saving\n",
            "Time mean 5.824834272265434\n",
            "Time std 0.08883198575959798\n",
            "Roc 0.8409779152359098\n",
            "test\n",
            "Crossvalidation run 10\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dilated run\n",
            "using cuda\n",
            "Epoch 0 - loss is 1.019152 : \n",
            "Roc 0.7775895709321843\n",
            "Epoch 1 - loss is 0.896079 : \n",
            "Roc 0.7780771570435342\n",
            "Epoch 2 - loss is 0.909096 : \n",
            "Roc 0.7120476068768056\n",
            "Epoch 3 - loss is 0.924563 : \n",
            "Roc 0.7410428072538957\n",
            "Epoch 4 - loss is 0.919316 : \n",
            "Roc 0.7625261558746812\n",
            "Epoch 5 - loss is 0.912619 : \n",
            "Roc 0.7828608555240253\n",
            "Epoch 6 - loss is 0.912694 : \n",
            "Roc 0.7385191833634306\n",
            "Epoch 7 - loss is 0.914912 : \n",
            "Roc 0.7523977213734502\n",
            "Epoch 8 - loss is 0.901901 : \n",
            "Roc 0.7946936020421489\n",
            "Epoch 9 - loss is 0.857945 : \n",
            "Roc 0.8107337817652549\n",
            "Epoch 10 - loss is 0.829475 : \n",
            "Roc 0.8331651315517082\n",
            "Epoch 11 - loss is 0.817916 : \n",
            "Roc 0.8452713801384152\n",
            "Epoch 12 - loss is 0.805395 : \n",
            "Roc 0.8411026632015108\n",
            "Epoch 13 - loss is 0.805313 : \n",
            "Roc 0.8440150422156615\n",
            "Epoch 14 - loss is 0.800478 : \n",
            "Roc 0.8548989913466652\n",
            "Epoch 15 - loss is 0.799125 : \n",
            "Roc 0.8558642108012217\n",
            "Saving\n",
            "Time mean 5.846056491136551\n",
            "Time std 0.2606985567875076\n",
            "Roc 0.8557334314278161\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.036130 : \n",
            "Roc 0.7752376383148059\n",
            "Epoch 1 - loss is 0.894905 : \n",
            "Roc 0.7813535957035762\n",
            "Epoch 2 - loss is 0.900823 : \n",
            "Roc 0.7811271493350621\n",
            "Epoch 3 - loss is 0.901207 : \n",
            "Roc 0.7486804334291801\n",
            "Epoch 4 - loss is 0.899078 : \n",
            "Roc 0.7520623868771021\n",
            "Epoch 5 - loss is 0.905331 : \n",
            "Roc 0.7218121405740544\n",
            "Epoch 6 - loss is 0.918881 : \n",
            "Roc 0.7725608978267566\n",
            "Epoch 7 - loss is 0.915819 : \n",
            "Roc 0.7664565736438088\n",
            "Epoch 8 - loss is 0.896023 : \n",
            "Roc 0.7685091324677159\n",
            "Epoch 9 - loss is 0.864268 : \n",
            "Roc 0.7860463908179508\n",
            "Epoch 10 - loss is 0.838722 : \n",
            "Roc 0.7986160697093785\n",
            "Epoch 11 - loss is 0.828935 : \n",
            "Roc 0.8123897854034556\n",
            "Epoch 12 - loss is 0.818831 : \n",
            "Roc 0.8108756634938769\n",
            "Epoch 13 - loss is 0.805291 : \n",
            "Roc 0.8122916051577623\n",
            "Epoch 14 - loss is 0.804933 : \n",
            "Roc 0.8283908588675515\n",
            "Epoch 15 - loss is 0.798422 : \n",
            "Roc 0.8241380195630408\n",
            "Saving\n",
            "Time mean 5.726733505725861\n",
            "Time std 0.12263952452584063\n",
            "Roc 0.8272902171959641\n",
            "test\n",
            "Fold:  3\n",
            "len(train_idx 3396\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.045992 : \n",
            "Roc 0.7345798296787271\n",
            "Epoch 1 - loss is 0.893556 : \n",
            "Roc 0.7615685757517995\n",
            "Epoch 2 - loss is 0.895459 : \n",
            "Roc 0.7118438839611662\n",
            "Epoch 3 - loss is 0.908851 : \n",
            "Roc 0.7526859291100385\n",
            "Epoch 4 - loss is 0.901080 : \n",
            "Roc 0.7640421073937907\n",
            "Epoch 5 - loss is 0.900205 : \n",
            "Roc 0.7357282788633226\n",
            "Epoch 6 - loss is 0.899375 : \n",
            "Roc 0.751702223505675\n",
            "Epoch 7 - loss is 0.895421 : \n",
            "Roc 0.7441294405148218\n",
            "Epoch 8 - loss is 0.902620 : \n",
            "Roc 0.7727288119020199\n",
            "Epoch 9 - loss is 0.864857 : \n",
            "Roc 0.7844979905468384\n",
            "Epoch 10 - loss is 0.823730 : \n",
            "Roc 0.8162704830867697\n",
            "Epoch 11 - loss is 0.802533 : \n",
            "Roc 0.8210622194270347\n",
            "Epoch 12 - loss is 0.790062 : \n",
            "Roc 0.8314789217627792\n",
            "Epoch 13 - loss is 0.783641 : \n",
            "Roc 0.8264329362366251\n",
            "Epoch 14 - loss is 0.779054 : \n",
            "Roc 0.832820269319609\n",
            "Epoch 15 - loss is 0.779339 : \n",
            "Roc 0.8348089868574191\n",
            "Saving\n",
            "Time mean 5.753635570406914\n",
            "Time std 0.1873126513404212\n",
            "Roc 0.8342301966231751\n",
            "test\n",
            "Fold:  4\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.037046 : \n",
            "Roc 0.7459666197319296\n",
            "Epoch 1 - loss is 0.901996 : \n",
            "Roc 0.7376072049931641\n",
            "Epoch 2 - loss is 0.901996 : \n",
            "Roc 0.7568707895334751\n",
            "Epoch 3 - loss is 0.902601 : \n",
            "Roc 0.7395071970527815\n",
            "Epoch 4 - loss is 0.905974 : \n",
            "Roc 0.7345387581084278\n",
            "Epoch 5 - loss is 0.894109 : \n",
            "Roc 0.7483247561477037\n",
            "Epoch 6 - loss is 0.900618 : \n",
            "Roc 0.7619299335916709\n",
            "Epoch 7 - loss is 0.897258 : \n",
            "Roc 0.7302858380523578\n",
            "Epoch 8 - loss is 0.903170 : \n",
            "Roc 0.7476789121518517\n",
            "Epoch 9 - loss is 0.857187 : \n",
            "Roc 0.7827991853639092\n",
            "Epoch 10 - loss is 0.840145 : \n",
            "Roc 0.8009383409631763\n",
            "Epoch 11 - loss is 0.828350 : \n",
            "Roc 0.8145357549141927\n",
            "Epoch 12 - loss is 0.812416 : \n",
            "Roc 0.8242295077198601\n",
            "Epoch 13 - loss is 0.797290 : \n",
            "Roc 0.8281618643075084\n",
            "Epoch 14 - loss is 0.789952 : \n",
            "Roc 0.8284312280312803\n",
            "Epoch 15 - loss is 0.785954 : \n",
            "Roc 0.8282658361498105\n",
            "Saving\n",
            "Time mean 5.714733004570007\n",
            "Time std 0.09423230075599262\n",
            "Roc 0.8282652465174346\n",
            "test\n",
            "Fold:  5\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.052728 : \n",
            "Roc 0.7567276456478599\n",
            "Epoch 1 - loss is 0.891883 : \n",
            "Roc 0.7152123714006996\n",
            "Epoch 2 - loss is 0.891920 : \n",
            "Roc 0.7322106214127642\n",
            "Epoch 3 - loss is 0.909581 : \n",
            "Roc 0.747444620999738\n",
            "Epoch 4 - loss is 0.918065 : \n",
            "Roc 0.7448419120254474\n",
            "Epoch 5 - loss is 0.913253 : \n",
            "Roc 0.7054413937695585\n",
            "Epoch 6 - loss is 0.913495 : \n",
            "Roc 0.7522287093755942\n",
            "Epoch 7 - loss is 0.906647 : \n",
            "Roc 0.7605454991978815\n",
            "Epoch 8 - loss is 0.913961 : \n",
            "Roc 0.7529790530918244\n",
            "Epoch 9 - loss is 0.872997 : \n",
            "Roc 0.7866344256251219\n",
            "Epoch 10 - loss is 0.846340 : \n",
            "Roc 0.8027132334998137\n",
            "Epoch 11 - loss is 0.836016 : \n",
            "Roc 0.8070250967065176\n",
            "Epoch 12 - loss is 0.826711 : \n",
            "Roc 0.816071422640354\n",
            "Epoch 13 - loss is 0.820056 : \n",
            "Roc 0.8176207588386895\n",
            "Epoch 14 - loss is 0.813321 : \n",
            "Roc 0.8273484605596251\n",
            "Epoch 15 - loss is 0.809123 : \n",
            "Roc 0.8305102395883338\n",
            "Saving\n",
            "Time mean 5.81936377286911\n",
            "Time std 0.21591040706194126\n",
            "Roc 0.8313695057422151\n",
            "test\n",
            "Fold:  6\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.026935 : \n",
            "Roc 0.7272460842098407\n",
            "Epoch 1 - loss is 0.897549 : \n",
            "Roc 0.7662171273797871\n",
            "Epoch 2 - loss is 0.898234 : \n",
            "Roc 0.733918395258719\n",
            "Epoch 3 - loss is 0.908220 : \n",
            "Roc 0.7655009165736046\n",
            "Epoch 4 - loss is 0.907246 : \n",
            "Roc 0.701076231723599\n",
            "Epoch 5 - loss is 0.904021 : \n",
            "Roc 0.7833269802108215\n",
            "Epoch 6 - loss is 0.902294 : \n",
            "Roc 0.778256134913505\n",
            "Epoch 7 - loss is 0.900216 : \n",
            "Roc 0.7779942841052536\n",
            "Epoch 8 - loss is 0.897557 : \n",
            "Roc 0.7668668698388862\n",
            "Epoch 9 - loss is 0.857189 : \n",
            "Roc 0.8134246905965486\n",
            "Epoch 10 - loss is 0.827187 : \n",
            "Roc 0.8243207107324352\n",
            "Epoch 11 - loss is 0.808223 : \n",
            "Roc 0.826860395400467\n",
            "Epoch 12 - loss is 0.804666 : \n",
            "Roc 0.8338972288254257\n",
            "Epoch 13 - loss is 0.797776 : \n",
            "Roc 0.8367330520081295\n",
            "Epoch 14 - loss is 0.788624 : \n",
            "Roc 0.841688774894503\n",
            "Epoch 15 - loss is 0.785062 : \n",
            "Roc 0.8362476606781304\n",
            "Saving\n",
            "Time mean 5.775492414832115\n",
            "Time std 0.1657325859148956\n",
            "Roc 0.836249767743888\n",
            "test\n",
            "Fold:  7\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.018407 : \n",
            "Roc 0.7295209262929234\n",
            "Epoch 1 - loss is 0.890444 : \n",
            "Roc 0.7446472500172241\n",
            "Epoch 2 - loss is 0.904621 : \n",
            "Roc 0.7621538458420974\n",
            "Epoch 3 - loss is 0.908286 : \n",
            "Roc 0.7398676579654464\n",
            "Epoch 4 - loss is 0.907262 : \n",
            "Roc 0.754083636275213\n",
            "Epoch 5 - loss is 0.900287 : \n",
            "Roc 0.7424874466558864\n",
            "Epoch 6 - loss is 0.905119 : \n",
            "Roc 0.7378241680749917\n",
            "Epoch 7 - loss is 0.923672 : \n",
            "Roc 0.7521185667910857\n",
            "Epoch 8 - loss is 0.905637 : \n",
            "Roc 0.7507502624145379\n",
            "Epoch 9 - loss is 0.868130 : \n",
            "Roc 0.7847789436143094\n",
            "Epoch 10 - loss is 0.841129 : \n",
            "Roc 0.8065064620846454\n",
            "Epoch 11 - loss is 0.822538 : \n",
            "Roc 0.829862571784054\n",
            "Epoch 12 - loss is 0.805205 : \n",
            "Roc 0.8370902179155166\n",
            "Epoch 13 - loss is 0.789958 : \n",
            "Roc 0.833037686375113\n",
            "Epoch 14 - loss is 0.784250 : \n",
            "Roc 0.8212899244975623\n",
            "Epoch 15 - loss is 0.790153 : \n",
            "Roc 0.843744199524209\n",
            "Saving\n",
            "Time mean 5.768724948167801\n",
            "Time std 0.16590144856825545\n",
            "Roc 0.8435424746805432\n",
            "test\n",
            "Fold:  8\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.032700 : \n",
            "Roc 0.7166547677593645\n",
            "Epoch 1 - loss is 0.893493 : \n",
            "Roc 0.7673372443386647\n",
            "Epoch 2 - loss is 0.902520 : \n",
            "Roc 0.7414508411592035\n",
            "Epoch 3 - loss is 0.901894 : \n",
            "Roc 0.7631789392873136\n",
            "Epoch 4 - loss is 0.902319 : \n",
            "Roc 0.7271029765061015\n",
            "Epoch 5 - loss is 0.901646 : \n",
            "Roc 0.7464787018717329\n",
            "Epoch 6 - loss is 0.902724 : \n",
            "Roc 0.7522695579376046\n",
            "Epoch 7 - loss is 0.895184 : \n",
            "Roc 0.762791750544478\n",
            "Epoch 8 - loss is 0.900264 : \n",
            "Roc 0.7691200198755579\n",
            "Epoch 9 - loss is 0.858851 : \n",
            "Roc 0.7932991032057187\n",
            "Epoch 10 - loss is 0.829101 : \n",
            "Roc 0.8227352598442688\n",
            "Epoch 11 - loss is 0.805521 : \n",
            "Roc 0.8239095819107303\n",
            "Epoch 12 - loss is 0.792810 : \n",
            "Roc 0.8398024729056189\n",
            "Epoch 13 - loss is 0.779188 : \n",
            "Roc 0.8309238209582734\n",
            "Epoch 14 - loss is 0.780303 : \n",
            "Roc 0.830954827456686\n",
            "Epoch 15 - loss is 0.779038 : \n",
            "Roc 0.8365018507739653\n",
            "Saving\n",
            "Time mean 5.749999687075615\n",
            "Time std 0.1020708952446492\n",
            "Roc 0.8364792827529689\n",
            "test\n",
            "Fold:  9\n",
            "len(train_idx 3397\n",
            "dilated run\n",
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 1.015402 : \n",
            "Roc 0.7636230956543457\n",
            "Epoch 1 - loss is 0.895447 : \n",
            "Roc 0.7324811906843156\n",
            "Epoch 2 - loss is 0.903720 : \n",
            "Roc 0.7329179609453047\n",
            "Epoch 3 - loss is 0.898712 : \n",
            "Roc 0.7744862559315684\n",
            "Epoch 4 - loss is 0.903456 : \n",
            "Roc 0.7426294408716283\n",
            "Epoch 5 - loss is 0.896975 : \n",
            "Roc 0.7795944290084914\n",
            "Epoch 6 - loss is 0.907243 : \n",
            "Roc 0.778564892139111\n",
            "Epoch 7 - loss is 0.903784 : \n",
            "Roc 0.759654993444056\n",
            "Epoch 8 - loss is 0.898989 : \n",
            "Roc 0.7562080497627373\n",
            "Epoch 9 - loss is 0.853875 : \n",
            "Roc 0.8104518528346654\n",
            "Epoch 10 - loss is 0.822596 : \n",
            "Roc 0.8261287540584416\n",
            "Epoch 11 - loss is 0.795949 : \n",
            "Roc 0.835519656125125\n",
            "Epoch 12 - loss is 0.780285 : \n",
            "Roc 0.8382016421078922\n",
            "Epoch 13 - loss is 0.780496 : \n",
            "Roc 0.836621386426074\n",
            "Epoch 14 - loss is 0.780186 : \n",
            "Roc 0.844041700487013\n",
            "Epoch 15 - loss is 0.771392 : \n",
            "Roc 0.8429987980769231\n",
            "Saving\n",
            "Time mean 5.775209829211235\n",
            "Time std 0.1660576055173103\n",
            "Roc 0.8424791029283217\n",
            "test\n",
            "Fold:  10\n",
            "len(train_idx 3397\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n",
            "Epoch 0 - loss is 1.018523 : \n",
            "Roc 0.7584442234899349\n",
            "Epoch 1 - loss is 0.897060 : \n",
            "Roc 0.764902250529937\n",
            "Epoch 2 - loss is 0.897926 : \n",
            "Roc 0.7555342026338833\n",
            "Epoch 3 - loss is 0.893705 : \n",
            "Roc 0.7455609586705422\n",
            "Epoch 4 - loss is 0.899996 : \n",
            "Roc 0.7341999482866519\n",
            "Epoch 5 - loss is 0.903629 : \n",
            "Roc 0.7506068615535406\n",
            "Epoch 6 - loss is 0.903595 : \n",
            "Roc 0.7292351337656103\n",
            "Epoch 7 - loss is 0.894371 : \n",
            "Roc 0.7559941891184622\n",
            "Epoch 8 - loss is 0.901050 : \n",
            "Roc 0.7396903278134955\n",
            "Epoch 9 - loss is 0.866566 : \n",
            "Roc 0.7652675124838604\n",
            "Epoch 10 - loss is 0.836982 : \n",
            "Roc 0.7801257317209761\n",
            "Epoch 11 - loss is 0.824665 : \n",
            "Roc 0.8096774952419139\n",
            "Epoch 12 - loss is 0.801993 : \n",
            "Roc 0.8216444303433275\n",
            "Epoch 13 - loss is 0.789189 : \n",
            "Roc 0.8191249660136712\n",
            "Epoch 14 - loss is 0.786123 : \n",
            "Roc 0.8277369872436244\n",
            "Epoch 15 - loss is 0.784657 : \n",
            "Roc 0.8324942405403505\n",
            "Saving\n",
            "Time mean 5.7351017743349075\n",
            "Time std 0.15857142740559585\n",
            "Roc 0.832207985090567\n",
            "test\n",
            "04:42:11.93\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaC4UC3DdEPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_cv_results():\n",
        "    \"\"\"\n",
        "    Plots PR curves, output computed metrics\n",
        "    :return:void\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
        "\n",
        "    # Plot ROC per loop type\n",
        "    fig = None\n",
        "    cols = [(\"#D6083B\", \"#EB99A9\"),\n",
        "            (\"#0072CF\", \"#68ACE5\"),\n",
        "            (\"#EA7125\", \"#F3BD48\"),\n",
        "            (\"#55A51C\", \"#AAB300\"),\n",
        "            (\"#8F2BBC\", \"#AF95A3\"),\n",
        "            (\"#00B1C1\", \"#91B9A4\")]\n",
        "\n",
        "\n",
        "    # Plot PR curves\n",
        "    print(\"Plotting PR curves\")\n",
        "    labels, probs, labels1, probs1 = open_crossval_results(\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq\", NUM_ITERATIONS)\n",
        "\n",
        "    #labels, probs = initial_open_crossval_results(\"parapred-cv-ab-seq\", NUM_ITERATIONS)\n",
        "    #selflabels, selfprobs, selflabels1, selfprobs1 = open_crossval_results(\"self-cv-ab-seq\", NUM_ITERATIONS)\n",
        "    #_,_,aglabels, agprobs = open_crossval_results(\"ag-cv-ab-seq\", NUM_ITERATIONS)\n",
        "\n",
        "\n",
        "    fig1 = plot_pr_curve(labels1, probs1, colours=(\"#0072CF\", \"#68ACE5\"),label=\"Parapred\")\n",
        "\n",
        "    #fig1 = plot_abip_pr(fig1)\n",
        "    #fig1 = plot_pr_curve(selflabels1, selfprobs1, colours=(\"#228B18\", \"#006400\"), label=\"Fast-Parapred\", plot_fig=fig1)\n",
        "    #fig1 = plot_pr_curve(aglabels, agprobs, colours=(\"#FF8C00\", \"#FFA500\"), label=\"AG-Fast-Parapred\", plot_fig=fig1)\n",
        "    #fig1.savefig(\"pr1.pdf\")\n",
        "\n",
        "    print(\"Printing PDB for visualisation\")\n",
        "    if visualisation_flag:\n",
        "        print_probabilities()\n",
        "\n",
        "    # Computing overall classifier metrics\n",
        "    print(\"Computing classifier metrics\")\n",
        "    initial_compute_classifier_metrics(labels, probs, threshold=0.4913739)\n",
        "\n",
        "#run_cv()\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZLaRr6DshgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "process_cv_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkQCelqUskZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def open_crossval_results(folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq\", num_results=NUM_ITERATIONS,\n",
        "                          loop_filter=None, flatten_by_lengths=True):\n",
        "    class_probabilities = []\n",
        "    labels = []\n",
        "    class_probabilities1 = []\n",
        "    labels1 = []\n",
        "    for r in range(num_results):\n",
        "        result_filename = \"{}/run-{}.p\".format(folder, r)\n",
        "        with open(result_filename, \"rb\") as f:\n",
        "            lbl_mat, prob_mat, mask_mat, all_lbls, all_probs = pickle.load(f)\n",
        "            lbl_mat = lbl_mat.data.cpu().numpy()\n",
        "            prob_mat = prob_mat.data.cpu().numpy()\n",
        "            mask_mat = mask_mat.data.cpu().numpy()\n",
        "        # Get entries corresponding to the given loop\n",
        "        if loop_filter is not None:\n",
        "            lbl_mat = lbl_mat[loop_filter::6]\n",
        "            prob_mat = prob_mat[loop_filter::6]\n",
        "            mask_mat = mask_mat[loop_filter::6]\n",
        "        \"\"\"\"\"\n",
        "        if not flatten_by_lengths:\n",
        "            class_probabilities.append(prob_mat)\n",
        "            labels.append(lbl_mat)\n",
        "            continue\n",
        "        \"\"\"\n",
        "        # Discard sequence padding\n",
        "        seq_lens = np.sum(np.squeeze(mask_mat), axis=1)\n",
        "        seq_lens = seq_lens.astype(int)\n",
        "        p = flatten_with_lengths(prob_mat, seq_lens)\n",
        "        l = flatten_with_lengths(lbl_mat, seq_lens)\n",
        "        class_probabilities.append(p)\n",
        "        labels.append(l)\n",
        "        #class_probabilities1 = np.concatenate((class_probabilities1, all_probs))\n",
        "        #labels1 = np.concatenate((labels1,all_lbls))\n",
        "        class_probabilities1.append(all_probs)\n",
        "        labels1.append(all_lbls)\n",
        "    return labels, class_probabilities, labels1, class_probabilities1"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX-wvJl9sp_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pr_curve(labels_test, probs_test, colours=(\"#0072CF\", \"#68ACE5\"),\n",
        "                  label=\"This method\", plot_fig=None):\n",
        "    if plot_fig is None:\n",
        "        plot_fig = plt.figure(figsize=(4.5, 3.5), dpi=300)\n",
        "    ax = plot_fig.gca()\n",
        "\n",
        "    num_runs = len(labels_test)\n",
        "    precs = np.zeros((num_runs, 10000))\n",
        "    recalls = np.linspace(0.0, 1.0, num=10000)\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        l = labels_test[i]\n",
        "        p = probs_test[i]\n",
        "\n",
        "        #print(\"run i\", i)\n",
        "        #print(\"labels\", l)\n",
        "        #print(\"probs\", p)\n",
        "\n",
        "        prec, rec, _ = metrics.precision_recall_curve(l.flatten(), p.flatten())\n",
        "\n",
        "        # Maximum interpolation\n",
        "        for j in range(len(prec)):\n",
        "            prec[j] = prec[:(j+1)].max()\n",
        "\n",
        "        prec = list(reversed(prec))\n",
        "        rec = list(reversed(rec))\n",
        "\n",
        "        for j, recall in enumerate(recalls):  # Inefficient, but good enough\n",
        "            for p, r in zip(prec, rec):\n",
        "                if r >= recall:\n",
        "                    precs[i, j] = p\n",
        "                    break\n",
        "\n",
        "    avg_prec = np.average(precs, axis=0)\n",
        "    err_prec = np.std(precs, axis=0)\n",
        "\n",
        "    ax.plot(recalls, avg_prec, c=colours[0], label=label)\n",
        "\n",
        "    btm_err = avg_prec - 2 * err_prec\n",
        "    btm_err[btm_err < 0.0] = 0.0\n",
        "    top_err = avg_prec + 2 * err_prec\n",
        "    top_err[top_err > 1.0] = 1.0\n",
        "\n",
        "    ax.fill_between(recalls, btm_err, top_err, facecolor=colours[1])\n",
        "\n",
        "    ax.set_ylabel(\"Precision\")\n",
        "    ax.set_xlabel(\"Recall\")\n",
        "    ax.legend()\n",
        "\n",
        "    return plot_fig"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02YAhPZes3Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}