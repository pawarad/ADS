{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_parapred_AG_CDR_DIST.ipynb",
      "provenance": [],
      "mount_file_id": "1ZrTHucvgLPDJNuPYSQ0tZt9C9ykcYSNu",
      "authorship_tag": "ABX9TyMspGYjqXldNBzk7pqZoH6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawarad/ADS/blob/master/Fast_parapred_AG_CDR_DIST_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFcIty02kTtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "51a68a7c-356d-4357-bc15-aeffcbbc7657"
      },
      "source": [
        "!pip install Biopython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/66/134dbd5f885fc71493c61b6cf04c9ea08082da28da5ed07709b02857cbd0/biopython-1.77-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Biopython) (1.18.5)\n",
            "Installing collected packages: Biopython\n",
            "Successfully installed Biopython-1.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1jbODeOkrAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Bio.PDB import *\n",
        "from Bio.PDB.Model import Model\n",
        "from Bio.PDB.Structure import Structure\n",
        "import numpy as np\n",
        "import Bio.PDB\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwoqHc7QktPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residue_in_contact_with(res, ab_search, dist):\n",
        "    return any(len(ab_search.search(a.coord, dist)) > 0\n",
        "               for a in res.get_unpacked_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QauC7HVTkwKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aaMap(_aa):\n",
        "    aa_df = pd.DataFrame([['Alanine','Ala','A'],\n",
        "                        ['Arginine','Arg','R'],\n",
        "                        ['Asparagine','Asn','N'],\n",
        "                        ['Aspartate','Asp','D'],\n",
        "                        ['Cysteine','Cys','C'],\n",
        "                        ['Glutamate','Glu','E'],\n",
        "                        ['Glutamine','Gln','Q'],\n",
        "                        ['Glycine','Gly','G'],\n",
        "                        ['Histidine','His','H'],\n",
        "                        ['Histidine_D','HID','H'],\n",
        "                        ['Histidine_E','HIE','H'],\n",
        "                        ['Histidine_P','HIP','H'],\n",
        "                        ['Isoleucine','Ile','I'],\n",
        "                        ['Leucine','Leu','L'],\n",
        "                        ['Lysine','Lys','K'],\n",
        "                        ['Methionine','Met','M'],\n",
        "                        ['Phenylalanine','Phe','F'],\n",
        "                        ['Proline','Pro','P'],\n",
        "                        ['Serine','Ser','S'],\n",
        "                        ['Threonine','Thr','T'],\n",
        "                        ['Tryptophan','Trp','W'],\n",
        "                        ['Tyrosine','Tyr','Y'],\n",
        "                        ['Valine','Val','V']],columns=['Full','3','1'])\n",
        "    aa_df['3'] = aa_df['3'].str.upper()\n",
        "    _aa = _aa.upper()\n",
        "    if len(_aa)==1:\n",
        "        toret = aa_df.loc[aa_df['1']==_aa,'3'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        toret = aa_df.loc[aa_df['3']==_aa,'1'].values\n",
        "        if len(toret)>0:\n",
        "            toret = toret[0]\n",
        "        else:\n",
        "            return None\n",
        "    return toret\n",
        "\n",
        "def maxi_len(all_variables):\n",
        "    max_length = 0\n",
        "    for i in range(len(all_variables)):\n",
        "        length = len(all_variables[i])\n",
        "        max_length = max(max_length,length)\n",
        "    #     max_length = length\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def ext_epitope(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in epitope:\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in ext_epi:\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "\n",
        "def ext_epitope_sorted(H_chain,L_chain,A_chain,pdb):\n",
        "    p = PDBParser()\n",
        "    structure = p.get_structure('X', \"/content/drive/My Drive/Peritia_Fast-Parapred/cleaned_pdb_shweta/\"+pdb+'.pdb')\n",
        "    print(pdb)\n",
        "    ## construct a Model with residues of Heavy and Light chain of the pdb\n",
        "    model = structure[0]\n",
        "    model1 = Model(0)\n",
        "    model1.add(model[H_chain]) ## heavy chain\n",
        "    model1.add(model[L_chain]) ## Light chain\n",
        "    \n",
        "#     print(list(model1))\n",
        "\n",
        "    ## construct another Model with residues of Antigen chain of the pdb\n",
        "    model2 = Model(1)\n",
        "    for i in A_chain:\n",
        "        model2.add(model[i])\n",
        "#     print(list(model2))\n",
        "    \n",
        "    ## Search for nearest atoms within the Antibody(Heavy/Light chain)\n",
        "    ab_search = NeighborSearch(Selection.unfold_entities(model1, 'A'))\n",
        "\n",
        "    epitope = []\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, ab_search,5) == True:\n",
        "                epitope.append(j)\n",
        "\n",
        "    epi_residues =[]\n",
        "    epi_res_number = []\n",
        "    for i in sorted(epitope):\n",
        "        tags = i.id\n",
        "    #     print(tags)\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            epi_residues.append(res_abbre)\n",
        "            epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"epi_res\",epi_residues) \n",
        "#     print(\"epi_res_number\",epi_res_number )\n",
        "\n",
        "    # print(\"tags\",sorted(epitope))\n",
        "\n",
        "    epi_search = NeighborSearch(Selection.unfold_entities(epitope, 'A'))\n",
        "\n",
        "    ext_epi =[]\n",
        "    for r in model2:\n",
        "        for j in r:\n",
        "            if residue_in_contact_with(j, epi_search,10) == True:\n",
        "                ext_epi.append(j)\n",
        "\n",
        "    ext_epi_residues = []\n",
        "    ext_epi_res_number = []\n",
        "    for i in sorted(ext_epi):\n",
        "        tags = i.id\n",
        "    #     print(\"tags\",sorted(tags[1]))\n",
        "        if tags[0] == \" \":\n",
        "            res_name = i.get_resname() \n",
        "            res_abbre = aaMap(res_name)\n",
        "            ext_epi_residues.append(res_abbre)\n",
        "    #         print(i,res_name)\n",
        "            ext_epi_res_number.append(str(tags[1])+str(tags[2]))\n",
        "#     print(\"ext_epi_res_number\",ext_epi_res_number)\n",
        "#     print(\"ext_epi_res\",ext_epi_residues)\n",
        "\n",
        "\n",
        "    return epi_residues, epi_res_number, ext_epi_res_number, ext_epi_residues\n",
        "\n",
        "\n",
        "aa_s = \"CSTPAGNDEQHRKMILVFYWX\"\n",
        "def one_to_number(res_str):\n",
        "    return [aa_s.index(r) for r in res_str]\n",
        "\n",
        "def aa_features():\n",
        "    # Meiler's features\n",
        "    prop1 = [[1.77, 0.13, 2.43,  1.54,  6.35, 0.17, 0.41],\n",
        "             [1.31, 0.06, 1.60, -0.04,  5.70, 0.20, 0.28],\n",
        "             [3.03, 0.11, 2.60,  0.26,  5.60, 0.21, 0.36],\n",
        "             [2.67, 0.00, 2.72,  0.72,  6.80, 0.13, 0.34],\n",
        "             [1.28, 0.05, 1.00,  0.31,  6.11, 0.42, 0.23],\n",
        "             [0.00, 0.00, 0.00,  0.00,  6.07, 0.13, 0.15],\n",
        "             [1.60, 0.13, 2.95, -0.60,  6.52, 0.21, 0.22],\n",
        "             [1.60, 0.11, 2.78, -0.77,  2.95, 0.25, 0.20],\n",
        "             [1.56, 0.15, 3.78, -0.64,  3.09, 0.42, 0.21],\n",
        "             [1.56, 0.18, 3.95, -0.22,  5.65, 0.36, 0.25],\n",
        "             [2.99, 0.23, 4.66,  0.13,  7.69, 0.27, 0.30],\n",
        "             [2.34, 0.29, 6.13, -1.01, 10.74, 0.36, 0.25],\n",
        "             [1.89, 0.22, 4.77, -0.99,  9.99, 0.32, 0.27],\n",
        "             [2.35, 0.22, 4.43,  1.23,  5.71, 0.38, 0.32],\n",
        "             [4.19, 0.19, 4.00,  1.80,  6.04, 0.30, 0.45],\n",
        "             [2.59, 0.19, 4.00,  1.70,  6.04, 0.39, 0.31],\n",
        "             [3.67, 0.14, 3.00,  1.22,  6.02, 0.27, 0.49],\n",
        "             [2.94, 0.29, 5.89,  1.79,  5.67, 0.30, 0.38],\n",
        "             [2.94, 0.30, 6.47,  0.96,  5.66, 0.25, 0.41],\n",
        "             [3.21, 0.41, 8.08,  2.25,  5.94, 0.32, 0.42],\n",
        "             [0.00, 0.00, 0.00,  0.00,  0.00, 0.00, 0.00]]\n",
        "    return np.array(prop1)\n",
        "\n",
        "NUM_FEATURES = len(aa_s) + 7\n",
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" Converts a class vector to binary class matrix. \"\"\"\n",
        "    new_y = torch.LongTensor(y)\n",
        "    n = new_y.size()[0]\n",
        "    categorical = torch.zeros(n, num_classes)\n",
        "    arangedTensor = torch.arange(0, n)\n",
        "    intaranged = arangedTensor.long()\n",
        "    categorical[intaranged, new_y] = 1\n",
        "    return categorical\n",
        "\n",
        "def seq_to_one_hot(res_seq_one):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[new_ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    return torch.cat((onehot, feats), axis=1)\n",
        "\n",
        "def cdrseq_to_one_hot(res_seq_one,i):\n",
        "#     from keras.utils.np_utils import to_categorical\n",
        "    ints = one_to_number(res_seq_one)\n",
        "    new_ints = torch.LongTensor(ints)\n",
        "    feats = torch.Tensor(aa_features()[ints])\n",
        "    onehot = to_categorical(ints, num_classes=len(aa_s))\n",
        "    if i%6 == 0:\n",
        "        ext_onehot = [1, 0, 0, 0, 0, 0]\n",
        "    if i%6 == 1:\n",
        "        ext_onehot = [0, 1, 0, 0, 0, 0]\n",
        "    if i%6 == 2:\n",
        "        ext_onehot = [0, 0, 1, 0, 0, 0]\n",
        "    if i%6 == 3:\n",
        "        ext_onehot = [0, 0, 0, 1, 0, 0]\n",
        "    if i%6 == 4:\n",
        "        ext_onehot = [0, 0, 0, 0, 1, 0]\n",
        "    if i%6 == 5:\n",
        "        ext_onehot = [0, 0, 0, 0, 0, 1]\n",
        "    \n",
        "    chain_encoding = torch.Tensor(ext_onehot)\n",
        "    chain_encoding = chain_encoding.expand(onehot.shape[0], 6)\n",
        "    concatenated = torch.cat((onehot, feats,chain_encoding), 1)\n",
        "\n",
        "    return concatenated\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vww8c73nkzan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch import index_select"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpknZbYk20b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNYj3FuLk6Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AG(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Declares the building blocks of the neural network.\n",
        "        \"\"\"\n",
        "        super(AG, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(NUM_FEATURES, 64, 3, padding=1)  # antibody first a trous convolutional layer\n",
        "\n",
        "        self.agconv1 = nn.Conv1d(AG_NUM_FEATURES, 64, 3, padding=1)   # antigen first a trous convolutional layer\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)  #antibody second a trous convolutional layer\n",
        "\n",
        "        self.agconv2 = nn.Conv1d(64, 128, 3, padding=2, dilation=2)  #antigen second a trous convolutional layer\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)  #antibody third a trous convolutional layer\n",
        "\n",
        "        self.agconv3 = nn.Conv1d(128, 256, 3, padding=4, dilation=4)\n",
        "\n",
        "        #self.agconv4 = nn.Conv1d(256, 32, 1)\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # batch normalisation after the first convolutional layer for antibody\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.agbn1 = nn.BatchNorm1d(64)  # batch normalisation after the first convolutional layer for antigen\n",
        "        self.agbn2 = nn.BatchNorm1d(128)\n",
        "        self.agbn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.elu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, 1, 1)  # dense prediction layer\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.aconv1 = nn.Conv1d(256, 1, 1)  # attentional mechanism\n",
        "        self.aconv2 = nn.Conv1d(32, 1, 1)\n",
        "\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool1d(2)\n",
        "        self.maxpool2 = nn.MaxPool1d(4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, ab_input, ab_unpacked_masks, ag_input, ag_unpacked_masks, dist):\n",
        "        \"\"\"\n",
        "        Forward propagation step\n",
        "        :param ab_input: antibody amino acid sequences\n",
        "        :param ab_unpacked_masks: antibody amino acid masks\n",
        "        :param ag_input: antigen amino acid sequences\n",
        "        :param ag_unpacked_masks: antigen amino acid masks\n",
        "        :param dist: maskin\n",
        "        :return: antibody binding probabilities, attentional coefficients\n",
        "        \"\"\"\n",
        "        x=ab_input\n",
        "        agx = ag_input\n",
        "\n",
        "        ab_unpacked_masks = torch.transpose(ab_unpacked_masks, 1, 2)\n",
        "        ag_unpacked_masks = torch.transpose(ag_unpacked_masks, 1, 2)\n",
        "\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        agx = torch.transpose(agx, 1, 2)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn1(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        agx = self.agconv1(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn1(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn2(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        agx = self.conv2(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn2(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.bn3(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        agx = self.conv3(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.agbn3(agx)\n",
        "        agx = self.elu(agx)\n",
        "        agx = torch.mul(agx, ag_unpacked_masks)\n",
        "        agx = self.dropout(agx)\n",
        "\n",
        "        # MaxPool\n",
        "        #agx = self.maxpool1(agx)\n",
        "        #ag_unpacked_masks = self.maxpool1(ag_unpacked_masks)\n",
        "\n",
        "        old = x\n",
        "\n",
        "        oldag = agx\n",
        "\n",
        "        heads_no = 1\n",
        "        #bias_mat = 1e9 * (ag_unpacked_masks - 1.0)\n",
        "        dist_mat = 1e9 * (dist - 1.0)\n",
        "        #dist_mat = self.maxpool1(dist_mat)\n",
        "\n",
        "        for i in range(heads_no):\n",
        "            #agconvi = nn.Conv1d(256, 128, 1)\n",
        "            aconvi1 = nn.Conv1d(256, 1, 1)\n",
        "            aconvi2 = nn.Conv1d(256, 1, 1)\n",
        "            if use_cuda:\n",
        "                aconvi1.cuda()\n",
        "                aconvi2.cuda()\n",
        "                #agconvi.cuda()\n",
        "            #agx = agconvi(oldag)\n",
        "            w_1 = aconvi1(x)\n",
        "            w_2 = aconvi2(agx)\n",
        "            w = self.lrelu(w_2 + torch.transpose(w_1, 1, 2))\n",
        "            w = self.softmax(w + dist_mat)\n",
        "            w = self.dropout(w)\n",
        "            temp_loop_x = torch.bmm(w, torch.transpose(agx, 1, 2))\n",
        "            if i==0:\n",
        "                loop_x = temp_loop_x\n",
        "            else:\n",
        "                loop_x = torch.cat((loop_x, temp_loop_x), dim=2)\n",
        "\n",
        "        x = torch.transpose(loop_x, 1, 2)\n",
        "        #x = x + old\n",
        "        x = torch.cat((x, old), dim=1)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "\n",
        "        x = self.bn4(x)\n",
        "        x = self.elu(x)\n",
        "        x = torch.mul(x, ab_unpacked_masks)\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        #print(\"x after fc\", x, file=track_f)\n",
        "\n",
        "        return x, w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu6EIzC3lRNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "# np.set_printoptions(threshold=np.nan)\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.set_printoptions(threshold=50000)\n",
        "import torch.optim as optim\n",
        "from torch import squeeze\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch import index_select\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJobQo_0lWvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off):\n",
        "    \"\"\"\n",
        "    For every CDR get the corresponding seq and check if dist is less than cutoff\n",
        "    and substitute 1 orelse 0 \n",
        "    \n",
        "    E.g. - CDR - TCRASGNIHNYLAWY\n",
        "           Seq.no - ['22','23','24','25','26','27','28','29','30','31','32','33','34','35','36']\n",
        "           \n",
        "           Output - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    \"\"\"\n",
        "    pdb_name = cdr_loop['PDB_Name'].to_numpy()\n",
        "    seq_no = cdr_loop['seq_no'].to_numpy()\n",
        "    chain_type = cdr_loop['Type'].to_numpy()\n",
        "\n",
        "    all_var = []\n",
        "    for i in range(len(pdb_name)):\n",
        "        if chain_type[i] == 'H1' or chain_type[i] == 'H2' or chain_type[i] == 'H3':\n",
        "            dist = pd.read_pickle(dist_matx_path_heavy+pdb_name[i]+\"heavy\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "\n",
        "        elif chain_type[i] == 'L1' or chain_type[i] == 'L2' or chain_type[i] == 'L3':\n",
        "            dist = pd.read_pickle(dist_matx_path_light+pdb_name[i]+\"light\"+\".pkl\")\n",
        "            var = []\n",
        "            for j in range(len(seq_no[i])):\n",
        "                first_index = dist.index.get_level_values(0)[0] ##get the first index for slicing\n",
        "                row_slc = dist.loc[(first_index, seq_no[i][j])] ##slice multi index for specific row\n",
        "\n",
        "                if np.any(row_slc < cut_off) == True:\n",
        "                    var.append(1)\n",
        "                else:\n",
        "                    var.append(0)\n",
        "            all_var.append(var)\n",
        "    \n",
        "    return all_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkHtfn_VlbVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def permute_training_ag_data(cdrs, masks, lengths, lbls, ag, ag_masks, ag_lengths, dist):\n",
        "    index = torch.randperm(cdrs.shape[0])\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    lbls = torch.index_select(lbls, 0, index)\n",
        "    masks = torch.index_select(masks, 0, index)\n",
        "    lengths = [lengths[i] for i in index]\n",
        "\n",
        "    ag = torch.index_select(ag, 0, index)\n",
        "    ag_masks = torch.index_select(ag_masks, 0, index)\n",
        "    ag_lengths = [ag_lengths[i] for i in index]\n",
        "\n",
        "    dist= torch.index_select(dist, 0, index)\n",
        "\n",
        "    return cdrs, masks, lengths, lbls, ag, ag_masks, ag_lengths, dist\n",
        "\n",
        "\n",
        "def flatten_with_lengths(matrix, lengths):\n",
        "    seqs = []\n",
        "    for i, example in enumerate(matrix):\n",
        "        seq = example[:lengths[i]]\n",
        "        seqs.append(seq)\n",
        "    return np.concatenate(seqs)\n",
        "\n",
        "def sort_ag_batch(cdrs, masks, lengths, lbls, ag, ag_masks, dist):\n",
        "    order = np.argsort(lengths)\n",
        "    order = order.tolist()\n",
        "    order.reverse()\n",
        "\n",
        "    lengths.sort(reverse=True)\n",
        "\n",
        "    index = Variable(torch.LongTensor(order))\n",
        "    if use_cuda:\n",
        "        index = index.cuda()\n",
        "\n",
        "    cdrs = torch.index_select(cdrs, 0, index)\n",
        "    lbls = torch.index_select(lbls, 0, index)\n",
        "    masks = torch.index_select(masks, 0, index)\n",
        "\n",
        "    ag = torch.index_select(ag, 0, index)\n",
        "    ag_masks = torch.index_select(ag_masks, 0, index)\n",
        "\n",
        "    dist= torch.index_select(dist, 0, index)\n",
        "\n",
        "    return cdrs, masks, lengths, lbls, ag, ag_masks, dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_NFtqm6lkBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def antigen_run(cdrs_train, lbls_train, masks_train, lengths_train,\n",
        "               ag_train, ag_masks_train, ag_lengths_train, dist_mat_train,\n",
        "               weights_template, weights_template_number,\n",
        "               cdrs_test, lbls_test, masks_test, lengths_test,\n",
        "               ag_test, ag_masks_test, ag_lengths_test, dist_test):\n",
        "    \"\"\"\n",
        "    :param cdrs_train: antibody amino acids used for training\n",
        "    :param lbls_train: ground truth values for antibody amino acids used for training\n",
        "    :param masks_train: amino acids' masks\n",
        "    :param lengths_train: amino acids' lengths\n",
        "    :param ag_train: antigen amino acids used for training\n",
        "    :param ag_masks_train: antigen amino acids' masks\n",
        "    :param ag_lengths_train: antigen amino acids' lengths\n",
        "    :param dist_mat_train:\n",
        "    :param weights_template: template for printing weights\n",
        "    :param weights_template_number: which file to print weights to\n",
        "    :param cdrs_test: antibody amino acids used for testing\n",
        "    :param lbls_test:\n",
        "    :param masks_test:\n",
        "    :param lengths_test:\n",
        "    :param ag_test:\n",
        "    :param ag_masks_test:\n",
        "    :param ag_lengths_test:\n",
        "    :param dist_test:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"dilated run\", file=print_file)\n",
        "    print(\"dilated run\")\n",
        "    model = AG()\n",
        "\n",
        "    ignored_params = list(map(id, [model.conv1.weight, model.conv2.weight, model.conv3.weight,\n",
        "                                   model.agconv1.weight, model.agconv2.weight, model.agconv3.weight,\n",
        "                                   model.aconv1.weight, model.aconv2.weight]))\n",
        "    base_params = filter(lambda p: id(p) not in ignored_params,\n",
        "                         model.parameters())\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': base_params},\n",
        "        {'params': model.conv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.conv2.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.conv3.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.agconv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.agconv2.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.agconv3.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.aconv1.weight, 'weight_decay': 0.01},\n",
        "        {'params': model.aconv2.weight, 'weight_decay': 0.01}\n",
        "    ], lr=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "\n",
        "    total_input = cdrs_train\n",
        "    total_lbls = lbls_train\n",
        "    total_masks = masks_train\n",
        "    total_lengths = lengths_train\n",
        "    total_dist_train = dist_mat_train\n",
        "\n",
        "    total_ag_input = ag_train\n",
        "    total_ag_masks = ag_masks_train\n",
        "    total_ag_lengths = ag_lengths_train\n",
        "\n",
        "    if use_cuda:\n",
        "        print(\"using cuda\")\n",
        "        model.cuda()\n",
        "        total_input = total_input.cuda()\n",
        "        total_lbls = total_lbls.cuda()\n",
        "        total_masks = total_masks.cuda()\n",
        "        cdrs_test = cdrs_test.cuda()\n",
        "        lbls_test = lbls_test.cuda()\n",
        "        masks_test = masks_test.cuda()\n",
        "\n",
        "        total_ag_input = total_ag_input.cuda()\n",
        "        total_ag_masks = total_ag_masks.cuda()\n",
        "        ag_test = ag_test.cuda()\n",
        "        ag_masks_test = ag_masks_test.cuda()\n",
        "\n",
        "        total_dist_train = total_dist_train.cuda()\n",
        "        dist_test = dist_test.cuda()\n",
        "\n",
        "    times = []\n",
        "\n",
        "    for epoch in range(epochs):  # training iterations\n",
        "        model.train(True)\n",
        "        scheduler.step()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        batches_done=0\n",
        "\n",
        "        total_input, total_masks, total_lengths, total_lbls,\\\n",
        "        total_ag_input, total_ag_masks, total_ag_lengths, total_dist_train = \\\n",
        "            permute_training_ag_data(total_input, total_masks, total_lengths, total_lbls,\n",
        "                                     total_ag_input, total_ag_masks, total_ag_lengths, total_dist_train)\n",
        "\n",
        "        total_time = 0\n",
        "\n",
        "        for j in range(0, cdrs_train.shape[0], batch_size):\n",
        "            batches_done +=1\n",
        "            interval = [x for x in range(j, min(cdrs_train.shape[0], j + batch_size))]\n",
        "            interval = torch.LongTensor(interval)\n",
        "            if use_cuda:\n",
        "                interval = interval.cuda()\n",
        "\n",
        "            input = Variable(index_select(total_input, 0, interval), requires_grad=True)\n",
        "            lbls = Variable(index_select(total_lbls, 0, interval))\n",
        "            masks = Variable(index_select(total_masks, 0, interval))\n",
        "            lengths = total_lengths[j:j + batch_size]\n",
        "\n",
        "            ag_input = Variable(index_select(total_ag_input, 0, interval), requires_grad=True)\n",
        "            ag_masks = Variable(index_select(total_ag_masks, 0, interval))\n",
        "\n",
        "            dist = Variable(index_select(total_dist_train, 0, interval))\n",
        "\n",
        "            input, masks, lengths, lbls, ag, ag_masks, dist = \\\n",
        "                sort_ag_batch(input, masks, list(lengths), lbls, ag_input, ag_masks, dist)\n",
        "\n",
        "            output, _ = model(input, masks, ag_input, ag_masks, dist)\n",
        "\n",
        "            loss_weights = (lbls * 1.5 + 1) * masks\n",
        "            max_val = (-output).clamp(min=0)\n",
        "            loss = loss_weights * \\\n",
        "                   (output - output * lbls + max_val + ((-max_val).exp() + (-output - max_val).exp()).log())\n",
        "            masks_added = masks.sum()\n",
        "            loss = loss.sum() / masks_added\n",
        "            #print(\"Epoch %d - Batch %d has loss %d \" % (epoch, j, loss.data), file=monitoring_file)\n",
        "            epoch_loss +=loss\n",
        "            model.zero_grad()\n",
        "\n",
        "            start_time =time.time()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_time += time.time() - start_time\n",
        "            # print(\"Total time\", total_time)\n",
        "\n",
        "        print(\"Epoch %d - loss is %f : \" % (epoch, epoch_loss.data/batches_done))\n",
        "        # print(\"--- %s seconds ---\" % (total_time))\n",
        "        times.append(total_time)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        cdrs_test2, masks_test2, lengths_test2, lbls_test2, ag_test2, ag_masks_test2, dist_test2 = \\\n",
        "            sort_ag_batch(cdrs_test, masks_test, list(lengths_test), lbls_test, ag_test, ag_masks_test, dist_test)\n",
        "\n",
        "\n",
        "        probs_test2, _= model(cdrs_test2, masks_test2, ag_test2, ag_masks_test2, dist_test2)\n",
        "\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        probs_test2 = sigmoid(probs_test2)\n",
        "\n",
        "        probs_test2 = probs_test2.data.cpu().numpy().astype('float32')\n",
        "        lbls_test2 = lbls_test2.data.cpu().numpy().astype('int32')\n",
        "\n",
        "        probs_test2 = flatten_with_lengths(probs_test2, lengths_test2)\n",
        "        lbls_test2 = flatten_with_lengths(lbls_test2, lengths_test2)\n",
        "\n",
        "        print(\"Roc\", roc_auc_score(lbls_test2, probs_test2))\n",
        "\n",
        "    print(\"Saving\")\n",
        "\n",
        "    torch.save(model.state_dict(), weights_template.format(weights_template_number))\n",
        "\n",
        "    times_mean = np.mean(times)\n",
        "    times_std = 2 * np.std(times)\n",
        "\n",
        "    print(\"Time mean\", times_mean)\n",
        "    print(\"Time std\", times_std)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    cdrs_test, masks_test, lengths_test, lbls_test, ag_test, ag_masks_test, dist_test = \\\n",
        "        sort_ag_batch(cdrs_test, masks_test, list(lengths_test), lbls_test, ag_test, ag_masks_test, dist_test)\n",
        "\n",
        "    probs_test, _ = model(cdrs_test, masks_test, ag_test, ag_masks_test, dist_test)\n",
        "\n",
        "\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    probs_test = sigmoid(probs_test)\n",
        "\n",
        "    #print(\"probs\", probs_test, file=track_f)\n",
        "\n",
        "    probs_test1 = probs_test.data.cpu().numpy().astype('float32')\n",
        "    lbls_test1 = lbls_test.data.cpu().numpy().astype('int32')\n",
        "\n",
        "    probs_test1 = flatten_with_lengths(probs_test1, list(lengths_test))\n",
        "    lbls_test1 = flatten_with_lengths(lbls_test1, list(lengths_test))\n",
        "\n",
        "    print(\"Roc\", roc_auc_score(lbls_test1, probs_test1))\n",
        "\n",
        "    return probs_test, lbls_test, probs_test1, lbls_test1  # get them in kfold, append, concatenate do roc on them"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P47ohksJl33K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def kfold_cv_eval(cdrs, lbls, masks, lengths, ag, ag_masks, ag_lengths, dist_mat, output_file=\"crossval-data.p\",\n",
        "                  weights_template=\"weights-fold-{}.h5\", seed=0):\n",
        "    \"\"\"\n",
        "    Performs 10-fold cross-vallidation\n",
        "    :param dataset: contains antibody amino acids, ground truth values, antigen atoms\n",
        "    :param output_file: where to print weights\n",
        "    :param weights_template:\n",
        "    :param seed: cv\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # cdrs, lbls, masks, lengths, ag, ag_masks, ag_lengths, dist_mat = \\\n",
        "    #     dataset[\"cdrs\"], dataset[\"lbls\"], dataset[\"masks\"], dataset[\"lengths\"],\\\n",
        "    #     dataset[\"ag\"], dataset[\"ag_masks\"], dataset[\"ag_lengths\"], dataset[\"dist_mat\"]\n",
        "\n",
        "\n",
        "    print(\"cdrs\", cdrs.shape)\n",
        "    print(\"ag\", ag.shape)\n",
        "    #print(\"lbls\", lbls, file=data_file)\n",
        "    #print(\"masks\", masks, file=data_file)\n",
        "    #print(\"lengths\", lengths, file=data_file)\n",
        "\n",
        "    kf = KFold(n_splits=NUM_SPLIT, random_state=seed, shuffle=True)\n",
        "\n",
        "    all_lbls2 = []\n",
        "    all_probs2 = []\n",
        "    all_masks = []\n",
        "\n",
        "    all_probs1 = []\n",
        "    all_lbls1 = []\n",
        "\n",
        "    for i, (train_idx, test_idx) in enumerate(kf.split(cdrs)):\n",
        "        print(\"Fold: \", i + 1)\n",
        "        #print(train_idx, )\n",
        "        #print(test_idx)\n",
        "\n",
        "        lengths_train = [lengths[i] for i in train_idx]\n",
        "        lengths_test = [lengths[i] for i in test_idx]\n",
        "\n",
        "        ag_lengths_train = [ag_lengths[i] for i in train_idx]\n",
        "        ag_lengths_test = [ag_lengths[i] for i in test_idx]\n",
        "\n",
        "        #print(\"train_idx\", train_idx)\n",
        "\n",
        "        print(\"len(train_idx\",len(train_idx))\n",
        "\n",
        "        train_idx = torch.from_numpy(train_idx)\n",
        "        test_idx = torch.from_numpy(test_idx)\n",
        "\n",
        "        cdrs_train = index_select(cdrs, 0, train_idx)\n",
        "        lbls_train = index_select(lbls, 0, train_idx)\n",
        "        mask_train = index_select(masks, 0, train_idx)\n",
        "        ag_train = index_select(ag, 0, train_idx)\n",
        "        ag_masks_train = index_select(ag_masks, 0, train_idx)\n",
        "        dist_mat_train = index_select(dist_mat, 0, train_idx)\n",
        "\n",
        "        cdrs_test = Variable(index_select(cdrs, 0, test_idx))\n",
        "        lbls_test = Variable(index_select(lbls, 0, test_idx))\n",
        "        mask_test = Variable(index_select(masks, 0, test_idx))\n",
        "        ag_test = Variable(index_select(ag, 0, test_idx))\n",
        "        ag_masks_test = Variable(index_select(ag_masks, 0, test_idx))\n",
        "        dist_mat_test = Variable(index_select(dist_mat, 0, test_idx))\n",
        "\n",
        "        code = 4\n",
        "        if code ==1:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                simple_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                    cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "        if code == 2:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                attention_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                              cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 3:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                atrous_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                     cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 4:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                antigen_run(cdrs_train, lbls_train, mask_train, lengths_train,\n",
        "                            ag_train, ag_masks_train, ag_lengths_train, dist_mat_train, weights_template, i,\n",
        "                           cdrs_test, lbls_test, mask_test, lengths_test,\n",
        "                            ag_test, ag_masks_test, ag_lengths_test, dist_mat_test)\n",
        "\n",
        "        if code == 5:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                atrous_self_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                           cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 6:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                rnn_run(cdrs_train, lbls_train, mask_train, lengths_train, weights_template, i,\n",
        "                                    cdrs_test, lbls_test, mask_test, lengths_test)\n",
        "\n",
        "        if code == 7:\n",
        "            probs_test1, lbls_test1, probs_test2, lbls_test2 = \\\n",
        "                xself_run(cdrs_train, lbls_train, mask_train, lengths_train,\n",
        "                            ag_train, ag_masks_train, ag_lengths_train, dist_mat_train, weights_template, i,\n",
        "                            cdrs_test, lbls_test, mask_test, lengths_test,\n",
        "                            ag_test, ag_masks_test, ag_lengths_test, dist_mat_test)\n",
        "\n",
        "        # print(\"test\", file=track_f)\n",
        "        print(\"test\")\n",
        "\n",
        "        lbls_test2 = np.squeeze(lbls_test2)\n",
        "        all_lbls2 = np.concatenate((all_lbls2, lbls_test2))\n",
        "        all_lbls1.append(lbls_test1)\n",
        "\n",
        "        probs_test_pad = torch.zeros(probs_test1.data.shape[0], MAX_CDR_LENGTH, probs_test1.data.shape[2])\n",
        "        probs_test_pad[:probs_test1.data.shape[0], :probs_test1.data.shape[1], :] = probs_test1.data\n",
        "        probs_test_pad = Variable(probs_test_pad)\n",
        "\n",
        "        probs_test2 = np.squeeze(probs_test2)\n",
        "        #print(probs_test)\n",
        "        all_probs2 = np.concatenate((all_probs2, probs_test2))\n",
        "        #print(all_probs)\n",
        "        #print(type(all_probs))\n",
        "\n",
        "        all_probs1.append(probs_test_pad)\n",
        "\n",
        "        all_masks.append(mask_test)\n",
        "\n",
        "    lbl_mat1 = torch.cat(all_lbls1)\n",
        "    prob_mat1 = torch.cat(all_probs1)\n",
        "    #print(\"end\", all_probs)\n",
        "    mask_mat = torch.cat(all_masks)\n",
        "\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump((lbl_mat1, prob_mat1, mask_mat, all_lbls2, all_probs2), f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09x22JfCmdT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGHKK47_miyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysjvjCdzmjTI",
        "colab_type": "text"
      },
      "source": [
        "**Building the input Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhN1t2GVmk3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_files = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/summary_file_cleaned.pkl\")\n",
        "summary_files = summary_files.drop(summary_files[summary_files.pdb.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "\n",
        "pdb_names = summary_files['pdb'].to_numpy()\n",
        "Hchain_id = summary_files['Hchain'].to_numpy()\n",
        "Lchain_id = summary_files['Lchain'].to_numpy()\n",
        "Achain_id = summary_files['antigen_chain_list'].to_numpy()\n",
        " \n",
        "epi_res = []\n",
        "epi_res_num = []\n",
        "ext_epi_res_num = []\n",
        "ext_epi_res = []\n",
        "A_pdb = []\n",
        "# for i in range(len(pdb_names)):\n",
        "#     epi_residues,epi_res_number,ext_epi_res_number, ext_epi_residues = ext_epitope_sorted(Hchain_id[i],Lchain_id[i],Achain_id[i],pdb_names[i])\n",
        "#     A_pdb.append(pdb_names[i])\n",
        "#     epi_res.append(epi_residues)\n",
        "#     epi_res_num.append(epi_res_number)\n",
        "#     ext_epi_res_num.append(ext_epi_res_number)\n",
        "#     ext_epi_res.append(ext_epi_residues)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcq6c9kXmnQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorted_Antigen_dataframe = pd.DataFrame(list(zip(A_pdb,epi_res,epi_res_num, ext_epi_res_num, ext_epi_res)),\n",
        "#                columns =['pdb','sorted_epi_residues','sorted_epi_res_number', 'sorted_ext_epi_res_number', 'sorted_ext_epi_residues'])\n",
        "# Sorted_Antigen_dataframe.to_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-Ilg1RRmqJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9403720a-f5a3-4d69-89cb-713a1887ae13"
      },
      "source": [
        "Sorted_epi = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/Sorted_Antigen_epitope_dataframe.pkl\")\n",
        "Sorted_epi.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pdb</th>\n",
              "      <th>sorted_epi_residues</th>\n",
              "      <th>sorted_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_res_number</th>\n",
              "      <th>sorted_ext_epi_residues</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4ydk</td>\n",
              "      <td>[Q, L, T, G, G, S, T, E, N, A, K, T, Q, P, S, ...</td>\n",
              "      <td>[105 , 122 , 123 , 124 , 198 , 199 , 257 , 275...</td>\n",
              "      <td>[52 , 54 , 65 , 66 , 69 , 93 , 94 , 95 , 96 , ...</td>\n",
              "      <td>[L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4ydj</td>\n",
              "      <td>[W, K, E, E, N, N, A, K, T, I, R, P, S, G, G, ...</td>\n",
              "      <td>[96 , 97 , 102 , 275 , 279 , 280 , 281 , 282 ,...</td>\n",
              "      <td>[47 , 48 , 49 , 50 , 51 , 52 , 69 , 92 , 93 , ...</td>\n",
              "      <td>[D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1u8i</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[E, L, D, K, W, A, N]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1u8l</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[1 , 2 , 3 , 4 , 5 , 6 , 7 ]</td>\n",
              "      <td>[D, L, D, R, W, A, S]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5y9j</td>\n",
              "      <td>[K, G, S, Y, T, Y, A, M, G, H, L, Q, K, V, G, ...</td>\n",
              "      <td>[160 , 161 , 162 , 163 , 205 , 206 , 207 , 208...</td>\n",
              "      <td>[156 , 157 , 158 , 159 , 160 , 161 , 162 , 163...</td>\n",
              "      <td>[P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pdb  ...                            sorted_ext_epi_residues\n",
              "0  4ydk  ...  [L, C, V, H, W, F, N, M, W, K, N, N, M, V, E, ...\n",
              "1  4ydj  ...  [D, A, D, T, T, L, W, N, F, N, M, W, K, N, N, ...\n",
              "2  1u8i  ...                              [E, L, D, K, W, A, N]\n",
              "3  1u8l  ...                              [D, L, D, R, W, A, S]\n",
              "4  5y9j  ...  [P, T, I, Q, K, G, S, Y, T, F, V, W, E, N, K, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TldP6Wq6msX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epi_res = Sorted_epi['sorted_epi_residues'].to_numpy()\n",
        "epi_res_num = Sorted_epi['sorted_epi_res_number'].to_numpy()\n",
        "ext_epi_res_num = Sorted_epi['sorted_ext_epi_res_number'].to_numpy()\n",
        "ext_epi_res = Sorted_epi['sorted_ext_epi_residues'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLIF3f_jmwmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de1946fa-04dd-4c36-9718-8f0846ef05a4"
      },
      "source": [
        "##Joining all the residue abbrevations for making a sequence and repeating each seq 6 times/pdb\n",
        "\n",
        "Anti_str = []\n",
        "for i in range(len(ext_epi_res)):\n",
        "#     print(i)\n",
        "    str1 = ''.join(ext_epi_res[i])\n",
        "#     print(repeat(str1, 6))\n",
        "#     print(str1,i,len(A_mat[i]))\n",
        "    Anti_str.append(str1)\n",
        "\n",
        "import itertools\n",
        "Anti_seq = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in Anti_str))\n",
        "\n",
        "MAX_EXT_AG_LENGTH = maxi_len(Anti_seq)\n",
        "MAX_EXT_AG_LENGTH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUEc8rD0my5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "022b1a9a-e7e6-4cd4-afc4-f323c4865314"
      },
      "source": [
        "## Generating Labels for Extended Epitope Antigen Matrix\n",
        "\n",
        "labels = []\n",
        "for i in range(len(ext_epi_res_num)):\n",
        "    label = []\n",
        "    for j in ext_epi_res_num[i]:\n",
        "#     print(i)\n",
        "        if j in epi_res_num[i]:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    labels.append(label)\n",
        "\n",
        "import itertools\n",
        "Anti_repeat_labels = list(itertools.chain.from_iterable(itertools.repeat(x, 6) for x in labels))\n",
        "\n",
        "# Antigen_lbls = np.stack(labels)\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "label_mats = []\n",
        "for i in range(len(Anti_repeat_labels)):\n",
        "    label_mat = torch.tensor(Anti_repeat_labels[i])\n",
        "    label_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, 1))\n",
        "    label_mat_pad[:label_mat.shape[0], 0] = label_mat\n",
        "    label_mats.append(label_mat_pad)\n",
        "\n",
        "ext_epi_lbls = torch.stack(label_mats)\n",
        "ext_epi_lbls.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGEwRKXsm19I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25a56929-24da-4f28-ff1b-f5682cfcb490"
      },
      "source": [
        "NUM_FEATURES = 28\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "# cdr_mats = []\n",
        "ext_epi_mats = []\n",
        "\n",
        "ext_epi_lengths = []\n",
        "for i in range(len(Anti_seq)):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "#     print(i)\n",
        "    ext_epi_mat = seq_to_one_hot(Anti_seq[i])\n",
        "    ext_epi_mat_pad = torch.zeros((MAX_EXT_AG_LENGTH, NUM_FEATURES))\n",
        "    ext_epi_mat_pad[:ext_epi_mat.shape[0], :] = ext_epi_mat\n",
        "    ext_epi_mats.append(ext_epi_mat_pad)\n",
        "    ext_epi_lengths.append(ext_epi_mat.shape[0])\n",
        "\n",
        "ext_epi = torch.stack(ext_epi_mats)\n",
        "ext_epi.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9q2Kn3cm9-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbecc3e2-d709-4e59-e3c7-c0eca8c32403"
      },
      "source": [
        "ext_epi_masks = []\n",
        "for i in range(len(Anti_seq)):\n",
        "    ext_epi_mask = torch.zeros((MAX_EXT_AG_LENGTH, 1), dtype=int)\n",
        "    ext_epi_mask[:len(Anti_seq[i]), 0] = 1\n",
        "    ext_epi_masks.append(ext_epi_mask)\n",
        "ext_epi_masks = torch.stack(ext_epi_masks)\n",
        "ext_epi_masks.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 288, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUJySbK8nDuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38f10386-fd7f-4083-b8fa-d9d118410a9d"
      },
      "source": [
        "NUM_FEATURES = 28+6\n",
        "Max_len_CDR = 38\n",
        "cdr_mats = []\n",
        "\n",
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\"5e08\",\"5gkr\",\n",
        "                                                                        \"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "cdrs_seq = torch.stack(cdr_mats)\n",
        "cdrs_seq.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 34])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfcLXpkVnHYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea3d175b-8c83-4e96-e371-8d029f734bbe"
      },
      "source": [
        "all_cdrs_lengths = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "#     on_hot = seq_to_one_hot(df['CDR'][i])\n",
        "    cdr_mat = cdrseq_to_one_hot(cdr_loop['CDR'][i],i)\n",
        "    cdr_mat_pad = torch.zeros((Max_len_CDR, NUM_FEATURES))\n",
        "    cdr_mat_pad[:cdr_mat.shape[0], :] = cdr_mat\n",
        "    cdr_mats.append(cdr_mat_pad)\n",
        "    all_cdrs_lengths.append(cdr_mat.shape[0])\n",
        "max(all_cdrs_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoSFf2ScnPoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "134a5de4-156b-4a17-976b-1cbc8566bf7f"
      },
      "source": [
        "Max_len_CDR = 38\n",
        "\n",
        "cdr_masks = []\n",
        "for i in range(len(cdr_loop['CDR'])):\n",
        "    cdr_mask = torch.zeros((Max_len_CDR, 1), dtype=int)\n",
        "    cdr_mask[:len(cdr_loop['CDR'][i]), 0] = 1\n",
        "    cdr_masks.append(cdr_mask)\n",
        "cdr_mask = torch.stack(cdr_masks)\n",
        "cdr_mask.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8ewun6unUsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cdr_loop = pd.read_pickle(\"/content/drive/My Drive/Peritia_Fast-Parapred/CDRs_seqno_pdb.pkl\")\n",
        "cdr_loop = cdr_loop.drop(cdr_loop[cdr_loop.PDB_Name.isin([\"4ydl\",\"4ydv\",\"5ig7\",\"5ies\",\"4hii\",\"5w0d\",\"5i9q\",\"4hkx\",\n",
        "                                                          \"5te4\",\"5te6\",\"4xvs\",\"4xvt\",\"3gbm\",\"2xqb\",\"5if0\",\"4s1q\",\n",
        "                                                          \"4xmp\",\"4xny\",\"3mlt\",\"6mvl\",\"4xh2\",\"5ifj\",\"5te7\",\"5occ\",\n",
        "                                                          \"4dqo\",\"4lsq\",\"4lsp\",\"4lsr\",\"4yaq\",\"3h3p\",\"3idi\",\n",
        "                                                          \"3lrs\",\"3qnz\",\"5x08\",\"5alc\",\"2p8m\",\"6pbw\",\"5csz\",\n",
        "                                                          \"6bkb\",\"5ado\",\"4xaw\",\"5wnb\",\"5dt1\",\"4ob5\",\"4xbe\",\n",
        "                                                          \"5fgb\",\"4y5y\",\"3u4e\",\"4xcf\",\"4rnr\",\"3drq\",\"6uyf\",\n",
        "                                                          \"4m8q\",\"3d0l\",\"2jb6\",\"3u2s\",\"3lhp\",\"3mug\",\"1bvk\",\n",
        "                                                          \"5e08\",\"5gkr\",\"6mwn\",\"6u8k\",\"6u8d\",\"6df1\"])].index)\n",
        "cdr_loop.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dist_matx_path_light = \"/content/drive/My Drive/Peritia_Fast-Parapred/Light_chain_dist_clean/\"\n",
        "dist_matx_path_heavy = \"/content/drive/My Drive/Peritia_Fast-Parapred/Heavy_chain_dist_clean/\"\n",
        "cut_off = 5\n",
        "# all_variables = get_variables(dist_matx_path_heavy,dist_matx_path_light,cdr_loop,cut_off)\n",
        "with open('/content/drive/My Drive/Peritia_Fast-Parapred/all_variables.pkl', 'rb') as f:\n",
        "    all_variables = pickle.load(f)\n",
        "# all_variables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcOKtCD5nXeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81620533-2871-4678-9c09-02143d68b26b"
      },
      "source": [
        "cont_mats = []\n",
        "cdr_lengths = []\n",
        "Max_len_CDR = 38\n",
        "\"\"\"\n",
        "Pad the variables to Max CDR len with 0\n",
        "Here max len of CDR is 38\n",
        "\n",
        "E.g - Input = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    Output = [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[1.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.]\n",
        "\n",
        "\"\"\"\n",
        "for i in range(len(all_variables)):\n",
        "    cont_mat = torch.tensor(all_variables[i])\n",
        "    cont_mat_pad = torch.zeros((Max_len_CDR, 1))\n",
        "    cont_mat_pad[:cont_mat.shape[0], 0] = cont_mat\n",
        "    cont_mats.append(cont_mat_pad)\n",
        "    cdr_lengths.append(cont_mat.shape[0])\n",
        "\n",
        "\n",
        "cdr_lbls = torch.stack(cont_mats)\n",
        "\n",
        "cdr_lbls.shape\n",
        "# cdr_lengths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3774, 38, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy430Dsn2ziQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/Peritia_Fast-Parapred/Fast_parapred_dist_mat/dist_mat_15_prob.pkl', 'rb') as f:\n",
        "    dist_mat = pickle.load(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOadrGq1pWeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "bd7ac4a6-c7d7-41c9-c3de-991dff7bb732"
      },
      "source": [
        "# dist_mat = dist_mat1[1:3775]\n",
        "dist_mat[0][0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3661,\n",
              "        0.3661, 0.3661, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.3661, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLb9KQrBnbGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def structure_ids_to_selection_mask(idx, num_structures):\n",
        "    mask = np.zeros((num_structures * 6, ), dtype=np.bool)\n",
        "    offset = idx * 6\n",
        "    for i in range(6):\n",
        "        mask[offset + i] = True\n",
        "    return mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tufF4Mhgndg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_CDR_LENGTH = 38\n",
        "\n",
        "AG_NUM_FEATURES = 28\n",
        "\n",
        "NUM_FEATURES = 34\n",
        "\n",
        "MAX_EXT_AG_LENGTH = 288\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAhhIiPfngvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_ITERATIONS = 10\n",
        "NUM_SPLIT = 10\n",
        "output_file=\"crossval-data.p\",\n",
        "# weights_template=\"weights-fold-{}.h5\"\n",
        "import os \n",
        "def run_cv(output_folder=\"cv-ab-seq_dist_5\",\n",
        "           num_iters=NUM_ITERATIONS):\n",
        "#     cache_file = dataset.split(\"/\")[-1] + \".p\"\n",
        "#     dataset = open_dataset(dataset_cache=cache_file)\n",
        "\n",
        "    dir =  output_folder + \"/weights\"\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(output_folder + \"/weights\")\n",
        "    for i in range(num_iters):\n",
        "        #i=0\n",
        "        print(\"Crossvalidation run\", i+1)\n",
        "        output_file = \"{}/run-{}.p\".format(output_folder, i)\n",
        "        weights_template = output_folder + \"/weights/run-\" + \\\n",
        "                           str(i) + \"-fold-{}.pth.tar\"\n",
        "        kfold_cv_eval(cdrs_seq, cdr_lbls, cdr_mask, all_cdrs_lengths, ext_epi, ext_epi_masks, ext_epi_lengths, dist_mat, output_file,\n",
        "                  weights_template, seed=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaSEI7nIn92Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24267571-c8fb-4901-eabf-60d310a60f85"
      },
      "source": [
        "import time\n",
        "import os\n",
        "start_time = time.time()\n",
        "run_cv(output_folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq_dist_15_prob\",\n",
        "           num_iters=NUM_ITERATIONS)\n",
        "end_time = time.time() - start_time\n",
        "hours, rem = divmod(end_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crossvalidation run 1\n",
            "cdrs torch.Size([3774, 38, 34])\n",
            "ag torch.Size([3774, 288, 28])\n",
            "Fold:  1\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.981304 : \n",
            "Roc 0.8485151863317439\n",
            "Epoch 1 - loss is 0.784752 : \n",
            "Roc 0.8128924376478981\n",
            "Epoch 2 - loss is 0.784499 : \n",
            "Roc 0.7999300519454875\n",
            "Epoch 3 - loss is 0.765576 : \n",
            "Roc 0.8341886742275856\n",
            "Epoch 4 - loss is 0.754090 : \n",
            "Roc 0.8034056581088784\n",
            "Epoch 5 - loss is 0.753699 : \n",
            "Roc 0.8465390642168524\n",
            "Epoch 6 - loss is 0.721854 : \n",
            "Roc 0.8494273575320958\n",
            "Epoch 7 - loss is 0.706524 : \n",
            "Roc 0.8659545262004631\n",
            "Epoch 8 - loss is 0.659782 : \n",
            "Roc 0.7650883965055433\n",
            "Epoch 9 - loss is 0.631518 : \n",
            "Roc 0.889973939671893\n",
            "Epoch 10 - loss is 0.588740 : \n",
            "Roc 0.8957013596277505\n",
            "Epoch 11 - loss is 0.577988 : \n",
            "Roc 0.8970317461454919\n",
            "Epoch 12 - loss is 0.571496 : \n",
            "Roc 0.9013266641824558\n",
            "Epoch 13 - loss is 0.564836 : \n",
            "Roc 0.9034675046101223\n",
            "Epoch 14 - loss is 0.566284 : \n",
            "Roc 0.9059229520388046\n",
            "Epoch 15 - loss is 0.562751 : \n",
            "Roc 0.9082842462808497\n",
            "Epoch 16 - loss is 0.560097 : \n",
            "Roc 0.9028892487809849\n",
            "Epoch 17 - loss is 0.544432 : \n",
            "Roc 0.9082912132185501\n",
            "Epoch 18 - loss is 0.553758 : \n",
            "Roc 0.9062445259775213\n",
            "Epoch 19 - loss is 0.543942 : \n",
            "Roc 0.9092717599360475\n",
            "Saving\n",
            "Time mean 31.663421189785005\n",
            "Time std 26.42887176904647\n",
            "Roc 0.9069008115089033\n",
            "test\n",
            "Fold:  2\n",
            "len(train_idx 3396\n",
            "dilated run\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - loss is 0.949994 : \n",
            "Roc 0.774391021732433\n",
            "Epoch 1 - loss is 0.787392 : \n",
            "Roc 0.8122189979076276\n",
            "Epoch 2 - loss is 0.765270 : \n",
            "Roc 0.7950666382121127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFq_T-5P6oD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def open_crossval_results(folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq_dist_15_prob\", num_results=NUM_ITERATIONS,\n",
        "                          loop_filter=None, flatten_by_lengths=True):\n",
        "    class_probabilities = []\n",
        "    labels = []\n",
        "    class_probabilities1 = []\n",
        "    labels1 = []\n",
        "    for r in range(num_results):\n",
        "        result_filename = \"{}/run-{}.p\".format(folder, r)\n",
        "        with open(result_filename, \"rb\") as f:\n",
        "            lbl_mat, prob_mat, mask_mat, all_lbls, all_probs = pickle.load(f)\n",
        "            lbl_mat = lbl_mat.data.cpu().numpy()\n",
        "            prob_mat = prob_mat.data.cpu().numpy()\n",
        "            mask_mat = mask_mat.data.cpu().numpy()\n",
        "        # Get entries corresponding to the given loop\n",
        "        if loop_filter is not None:\n",
        "            lbl_mat = lbl_mat[loop_filter::6]\n",
        "            prob_mat = prob_mat[loop_filter::6]\n",
        "            mask_mat = mask_mat[loop_filter::6]\n",
        "        \"\"\"\"\"\n",
        "        if not flatten_by_lengths:\n",
        "            class_probabilities.append(prob_mat)\n",
        "            labels.append(lbl_mat)\n",
        "            continue\n",
        "        \"\"\"\n",
        "        # Discard sequence padding\n",
        "        seq_lens = np.sum(np.squeeze(mask_mat), axis=1)\n",
        "        seq_lens = seq_lens.astype(int)\n",
        "        p = flatten_with_lengths(prob_mat, seq_lens)\n",
        "        l = flatten_with_lengths(lbl_mat, seq_lens)\n",
        "        class_probabilities.append(p)\n",
        "        labels.append(l)\n",
        "        #class_probabilities1 = np.concatenate((class_probabilities1, all_probs))\n",
        "        #labels1 = np.concatenate((labels1,all_lbls))\n",
        "        class_probabilities1.append(all_probs)\n",
        "        labels1.append(all_lbls)\n",
        "    return labels, class_probabilities, labels1, class_probabilities1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkk1g3M8GhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_cv_results():\n",
        "    \"\"\"\n",
        "    Plots PR curves, output computed metrics\n",
        "    :return:void\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
        "\n",
        "    # Plot ROC per loop type\n",
        "    fig = None\n",
        "    cols = [(\"#D6083B\", \"#EB99A9\"),\n",
        "            (\"#0072CF\", \"#68ACE5\"),\n",
        "            (\"#EA7125\", \"#F3BD48\"),\n",
        "            (\"#55A51C\", \"#AAB300\"),\n",
        "            (\"#8F2BBC\", \"#AF95A3\"),\n",
        "            (\"#00B1C1\", \"#91B9A4\")]\n",
        "\n",
        "\n",
        "    # Plot PR curves\n",
        "    print(\"Plotting PR curves\")\n",
        "    labels, probs, labels1, probs1 = open_crossval_results(\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq_dist_15_prob\", NUM_ITERATIONS)\n",
        "\n",
        "    #labels, probs = initial_open_crossval_results(\"parapred-cv-ab-seq\", NUM_ITERATIONS)\n",
        "    #selflabels, selfprobs, selflabels1, selfprobs1 = open_crossval_results(\"self-cv-ab-seq\", NUM_ITERATIONS)\n",
        "    #_,_,aglabels, agprobs = open_crossval_results(\"ag-cv-ab-seq\", NUM_ITERATIONS)\n",
        "\n",
        "\n",
        "    fig1 = plot_pr_curve(labels1, probs1, colours=(\"#0072CF\", \"#68ACE5\"),label=\"Parapred\")\n",
        "\n",
        "    #fig1 = plot_abip_pr(fig1)\n",
        "    #fig1 = plot_pr_curve(selflabels1, selfprobs1, colours=(\"#228B18\", \"#006400\"), label=\"Fast-Parapred\", plot_fig=fig1)\n",
        "    #fig1 = plot_pr_curve(aglabels, agprobs, colours=(\"#FF8C00\", \"#FFA500\"), label=\"AG-Fast-Parapred\", plot_fig=fig1)\n",
        "    #fig1.savefig(\"pr1.pdf\")\n",
        "\n",
        "    print(\"Printing PDB for visualisation\")\n",
        "    if visualisation_flag:\n",
        "        print_probabilities()\n",
        "\n",
        "    # Computing overall classifier metrics\n",
        "    print(\"Computing classifier metrics\")\n",
        "    initial_compute_classifier_metrics(labels, probs, threshold=0.4913739)\n",
        "\n",
        "#run_cv()\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6od2c_vB8JRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, class_probabilities, labels1, class_probabilities1 = open_crossval_results(folder=\"/content/drive/My Drive/Peritia_Fast-Parapred/cv-ab-seq_dist_15_prob\", num_results=NUM_ITERATIONS,\n",
        "                          loop_filter=None, flatten_by_lengths=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRX7yJkN8KDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "# process_cv_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOyRs5he8Mmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def helper_compute_metrics(matrices, aucs, mcorrs):\n",
        "    matrices = np.stack(matrices)\n",
        "    mean_conf = np.mean(matrices, axis=0)\n",
        "    errs_conf = 2 * np.std(matrices, axis=0)\n",
        "\n",
        "    tps = matrices[:, 1, 1]\n",
        "    fns = matrices[:, 1, 0]\n",
        "    fps = matrices[:, 0, 1]\n",
        "\n",
        "    tpsf = tps.astype(float)\n",
        "    fnsf = fns.astype(float)\n",
        "    fpsf = fps.astype(float)\n",
        "\n",
        "    recalls = tpsf / (tpsf + fnsf)\n",
        "    precisions = tpsf / (tpsf + fpsf)\n",
        "\n",
        "    rec = np.mean(recalls)\n",
        "    rec_err = 2 * np.std(recalls)\n",
        "    prec = np.mean(precisions)\n",
        "    prec_err = 2 * np.std(precisions)\n",
        "\n",
        "    fscores = 2 * precisions * recalls / (precisions + recalls)\n",
        "    fsc = np.mean(fscores)\n",
        "    fsc_err = 2 * np.std(fscores)\n",
        "\n",
        "    auc_scores = np.array(aucs)\n",
        "    auc = np.mean(auc_scores)\n",
        "    auc_err = 2 * np.std(auc_scores)\n",
        "\n",
        "    mcorr_scores = np.array(mcorrs)\n",
        "    mcorr = np.mean(mcorr_scores)\n",
        "    mcorr_err = 2 * np.std(mcorr_scores)\n",
        "\n",
        "    print(\"Mean confusion matrix and error\")\n",
        "    print(mean_conf)\n",
        "    print(errs_conf)\n",
        "\n",
        "    print(\"Recall = {} +/- {}\".format(rec, rec_err))\n",
        "    print(\"Precision = {} +/- {}\".format(prec, prec_err))\n",
        "    print(\"F-score = {} +/- {}\".format(fsc, fsc_err))\n",
        "    print(\"ROC AUC = {} +/- {}\".format(auc, auc_err))\n",
        "    #print(\"ROC AUC - original = {} +/- {}\".format(auc2, auc_err2))\n",
        "    # print(\"ROC AUC - concatenated and iterated = {} +/- {}\".format(auc3, auc_err3))\n",
        "    print(\"MCC = {} +/- {}\".format(mcorr, mcorr_err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZYirxr48Pqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_classifier_metrics(labels, probs, labels1, probs1, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Computes metric: precision, recall, mcc, f1\n",
        "    :param labels: ground truth\n",
        "    :param probs: predicted values\n",
        "    :param labels1:\n",
        "    :param probs1:\n",
        "    :param threshold: binding/non-binding threshold\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    matrices = []\n",
        "    matrices1 = []\n",
        "\n",
        "    aucs1 = []\n",
        "    aucs2 = []\n",
        "\n",
        "    mcorrs = []\n",
        "    mcorrs1 = []\n",
        "\n",
        "    #print(\"labels\", labels)\n",
        "    #print(\"probs\", probs)\n",
        "\n",
        "    for l, p in zip(labels, probs):\n",
        "        #print(\"l\", l)\n",
        "        #print(\"p\", p)\n",
        "        aucs2.append(roc_auc_score(l, p))\n",
        "        l_pred = (p > threshold).astype(int)\n",
        "        matrices.append(confusion_matrix(l, l_pred))\n",
        "        mcorrs.append(matthews_corrcoef(l, l_pred))\n",
        "\n",
        "    for l1, p1 in zip(labels1, probs1):\n",
        "        #print(\"in for\")\n",
        "        #print(\"l1\", l1)\n",
        "        #print(\"p1\", p1)\n",
        "        aucs1.append(roc_auc_score(l1, p1))\n",
        "        l_pred1 = (p1 > threshold).astype(int)\n",
        "        matrices1.append(confusion_matrix(l1, l_pred1))\n",
        "        mcorrs1.append(matthews_corrcoef(l1, l_pred1))\n",
        "\n",
        "    print(\"Metrics with the original version\")\n",
        "    helper_compute_metrics(matrices=matrices,  aucs=aucs2, mcorrs =mcorrs)\n",
        "    print(\"Metrics with probabilities concatenated\")\n",
        "    helper_compute_metrics(matrices=matrices1, aucs=aucs1, mcorrs=mcorrs1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po7_lLFl8SWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compute_classifier_metrics(labels, class_probabilities, labels1, class_probabilities1, threshold=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYViyQSRHpYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_probabilities "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKBaozy_HqAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}